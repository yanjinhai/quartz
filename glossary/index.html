<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="adjugate (or classical adjoint) The matrix adj $A$ formed from a square matrix $A$ by replacing the $(i, j)$-entry of $A$ by the $(i, j)$-cofactor, for all $i$ and $j$, and then transposing the resulting matrix."><title>ðŸª´ Quartz 3.3</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://yanjinhai.github.io/quartz//icon.png><link href=https://yanjinhai.github.io/quartz/styles.706bb6073ba85d26809f9096dae23a6b.min.css rel=stylesheet><link href=https://yanjinhai.github.io/quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://yanjinhai.github.io/quartz/js/darkmode.f8a7799ddfa4bf9eced5584f1780e8b3.min.js></script>
<script src=https://yanjinhai.github.io/quartz/js/util.5e39932758f9ecaf45fd54506ed61416.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://yanjinhai.github.io/quartz/js/popover.6da9b273c092cc16fc1aa904d71a2163.min.js></script>
<script src=https://yanjinhai.github.io/quartz/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://yanjinhai.github.io/quartz/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://yanjinhai.github.io/quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://yanjinhai.github.io/quartz/",fetchData=Promise.all([fetch("https://yanjinhai.github.io/quartz/indices/linkIndex.e6bc758255cec383b3768a102dd43d89.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://yanjinhai.github.io/quartz/indices/contentIndex.fc95a4ba222a6b35d0f0397d139797d8.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://yanjinhai.github.io/quartz",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://yanjinhai.github.io/quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/yanjinhai.github.io\/quartz\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://yanjinhai.github.io/quartz/js/full-text-search.51f0b1753e9b30839d053f8a98cc20d1.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://yanjinhai.github.io/quartz/>ðŸª´ Quartz 3.3</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Unknown
<a href=https://github.com/yanjinhai/quartz/tree/hugo/content/glossary.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents></nav></details></aside><a href=#adjugate-or-classical-adjoint><h1 id=adjugate-or-classical-adjoint><span class=hanchor arialabel=Anchor># </span>adjugate (or classical adjoint)</h1></a><p>The matrix adj $A$ formed from a square matrix $A$ by replacing the $(i, j)$-entry of $A$ by the $(i, j)$-cofactor, for all $i$ and $j$, and then transposing the resulting matrix.</p><a href=#affine-combination><h1 id=affine-combination><span class=hanchor arialabel=Anchor># </span>affine combination</h1></a><p>A linear combination of vectors (points in $\mathbb{R}^{n}$) in which the sum of the weights involved is 1.</p><a href=#affine-dependence-relation><h1 id=affine-dependence-relation><span class=hanchor arialabel=Anchor># </span>affine dependence relation</h1></a><p>An equation of the form $c_{1} \mathbf{v}_{1}+$ $\cdots+c_{p} \mathbf{v}_{p}=\mathbf{0}$, where the weights $c_{1}, \ldots, c_{p}$ are not all zero, and $c_{1}+\cdots+c_{p}=0$.</p><a href=#affine-hull-or-affine-span-of-a-set-s><h1 id=affine-hull-or-affine-span-of-a-set-s><span class=hanchor arialabel=Anchor># </span>affine hull (or affine span) of a set $S$</h1></a><p>The set of all affine combinations of points in $S$, denoted by aff $S$.</p><a href=#affinely-dependent-set><h1 id=affinely-dependent-set><span class=hanchor arialabel=Anchor># </span>affinely dependent set</h1></a><p>A set $\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$ in $\mathbb{R}^{n}$ such that there are real numbers $c_{1}, \ldots, c_{p}$, not all zero, such that $c_{1}+\cdots+$ $c_{p}=0$ and $c_{1} \mathbf{v}_{1}+\cdots+c_{p} \mathbf{v}_{p}=\mathbf{0}$.</p><a href=#affinely-independent-set><h1 id=affinely-independent-set><span class=hanchor arialabel=Anchor># </span>affinely independent set</h1></a><p>A set $\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$ in $\mathbb{R}^{n}$ that is not affinely dependent.</p><a href=#affine-set-or-affine-subset><h1 id=affine-set-or-affine-subset><span class=hanchor arialabel=Anchor># </span>affine set (or affine subset)</h1></a><p>A set $S$ of points such that if $\mathbf{p}$ and $\mathbf{q}$ are in $S$, then $(1-t) \mathbf{p}+t \mathbf{q} \in S$ for each real number $t$.</p><a href=#affine-transformation><h1 id=affine-transformation><span class=hanchor arialabel=Anchor># </span>affine transformation</h1></a><p>A mapping $T: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ of the form $T(\mathbf{x})=A \mathbf{x}+\mathbf{b}$, with $A$ an $m \times n$ matrix and $\mathbf{b}$ in $\mathbb{R}^{m}$.</p><a href=#algebraic-multiplicity><h1 id=algebraic-multiplicity><span class=hanchor arialabel=Anchor># </span>algebraic multiplicity</h1></a><p>The multiplicity of an eigenvalue as a root of the characteristic equation.</p><a href=#angle-between-nonzero-vectors-mathbfu-and-mathbfv-in-mathbbr2-or-mathbbr3><h1 id=angle-between-nonzero-vectors-mathbfu-and-mathbfv-in-mathbbr2-or-mathbbr3><span class=hanchor arialabel=Anchor># </span>angle (between nonzero vectors $\mathbf{u}$ and $\mathbf{v}$ in $\mathbb{R}^{2}$ or $\mathbb{R}^{3}$)</h1></a><p>The angle $\vartheta$ between the two directed line segments from the origin to the points $\mathbf{u}$ and $\mathbf{v}$. Related to the scalar product by</p><p>$$
\mathbf{u} \cdot \mathbf{v}=|\mathbf{u}||\mathbf{v}| \cos \vartheta
$$</p><a href=#associative-law-of-multiplication><h1 id=associative-law-of-multiplication><span class=hanchor arialabel=Anchor># </span>associative law of multiplication</h1></a><p>$\quad A(B C)=(A B) C$, for all $A$, $B, C$.</p><a href=#attractor-of-a-dynamical-system-in-mathbbr2><h1 id=attractor-of-a-dynamical-system-in-mathbbr2><span class=hanchor arialabel=Anchor># </span>attractor (of a dynamical system in $\mathbb{R}^{2}$)</h1></a><p>The origin when all trajectories tend toward $\mathbf{0}$.</p><a href=#augmented-matrix><h1 id=augmented-matrix><span class=hanchor arialabel=Anchor># </span>augmented matrix</h1></a><p>A matrix made up of a coefficient matrix for a linear system and one or more columns to the right. Each extra column contains the constants from the right side of a system with the given coefficient matrix.</p><a href=#auxiliary-equation><h1 id=auxiliary-equation><span class=hanchor arialabel=Anchor># </span>auxiliary equation</h1></a><p>A polynomial equation in a variable $r$, created from the coefficients of a homogeneous difference equation.</p><a href=#back-substitution-with-matrix-notation><h1 id=back-substitution-with-matrix-notation><span class=hanchor arialabel=Anchor># </span>back-substitution (with matrix notation)</h1></a><p>The backward phase of row reduction of an augmented matrix that transforms an echelon matrix into a reduced echelon matrix; used to find the solution(s) of a system of linear equations.</p><a href=#backward-phase-of-row-reduction><h1 id=backward-phase-of-row-reduction><span class=hanchor arialabel=Anchor># </span>backward phase (of row reduction)</h1></a><p>The last part of the algorithm that reduces a matrix in echelon form to a reduced echelon form.</p><a href=#band-matrix><h1 id=band-matrix><span class=hanchor arialabel=Anchor># </span>band matrix</h1></a><p>A matrix whose nonzero entries lie within a band along the main diagonal.</p><a href=#barycentric-coordinates-of-a-point-mathbfp-with-respect-to-an-affinely-independent-set-sleftmathbfv_1-ldots-mathbfv_kright><h1 id=barycentric-coordinates-of-a-point-mathbfp-with-respect-to-an-affinely-independent-set-sleftmathbfv_1-ldots-mathbfv_kright><span class=hanchor arialabel=Anchor># </span>barycentric coordinates (of a point $\mathbf{p}$ with respect to an affinely independent set $S=\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{k}\right}$)</h1></a><p>The (unique) set of weights $c_{1}, \ldots, c_{k}$ such that $\mathbf{p}=c_{1} \mathbf{v}_{1}+\cdots+c_{k} \mathbf{v}_{k}$ and $c_{1}+$ $\cdots+c_{k}=1$. (Sometimes also called the affine coordinates of $\mathbf{p}$ with respect to $S$.)</p><a href=#basic-variable><h1 id=basic-variable><span class=hanchor arialabel=Anchor># </span>basic variable</h1></a><p>A variable in a linear system that corresponds to a pivot column in the coefficient matrix.</p><a href=#basis-for-a-nontrivial-subspace-h-of-a-vector-space-v><h1 id=basis-for-a-nontrivial-subspace-h-of-a-vector-space-v><span class=hanchor arialabel=Anchor># </span>basis (for a nontrivial subspace $H$ of a vector space $V$)</h1></a><p>An indexed set $\mathcal{B}=\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$ in $V$ such that: (i) $\mathcal{B}$ is a linearly independent set and (ii) the subspace spanned by $\mathcal{B}$ coincides with $H$, that is, $H=\operatorname{Span}\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$.</p><a href=#mathcalb-coordinates-of-mathbfx><h1 id=mathcalb-coordinates-of-mathbfx><span class=hanchor arialabel=Anchor># </span>$\mathcal{B}$-coordinates of $\mathbf{x}$</h1></a><p>See coordinates of $\mathbf{x}$ relative to the basis $\mathcal{B}$.</p><a href=#best-approximation><h1 id=best-approximation><span class=hanchor arialabel=Anchor># </span>best approximation</h1></a><p>The closest point in a given subspace to a given vector.</p><a href=#bidiagonal-matrix><h1 id=bidiagonal-matrix><span class=hanchor arialabel=Anchor># </span>bidiagonal matrix</h1></a><p>A matrix whose nonzero entries lie on the main diagonal and on one diagonal adjacent to the main diagonal.</p><a href=#block-diagonal-matrix><h1 id=block-diagonal-matrix><span class=hanchor arialabel=Anchor># </span>block diagonal (matrix)</h1></a><p>A partitioned matrix $A=\left[A_{i j}\right]$ such that each block $A_{i j}$ is a zero matrix for $i \neq j$.</p><a href=#block-matrix><h1 id=block-matrix><span class=hanchor arialabel=Anchor># </span>block matrix</h1></a><p>See partitioned matrix.</p><a href=#block-matrix-multiplication><h1 id=block-matrix-multiplication><span class=hanchor arialabel=Anchor># </span>block matrix multiplication</h1></a><p>The row-column multiplication of partitioned matrices as if the block entries were scalars. block upper triangular (matrix): A partitioned matrix $A=\left[A_{i j}\right]$ such that each block $A_{i j}$ is a zero matrix for $i>j$.</p><a href=#boundary-point-of-a-set-s-in-mathbbrn><h1 id=boundary-point-of-a-set-s-in-mathbbrn><span class=hanchor arialabel=Anchor># </span>boundary point of a set $S$ in $\mathbb{R}^{n}$</h1></a><p>A point $\mathbf{p}$ such that every open ball in $\mathbb{R}^{n}$ centered at $\mathbf{p}$ intersects both $S$ and the complement of $S$.</p><a href=#bounded-set-in-mathbbrn><h1 id=bounded-set-in-mathbbrn><span class=hanchor arialabel=Anchor># </span>bounded set in $\mathbb{R}^{n}$</h1></a><p>A set that is contained in an open ball $B(\mathbf{0}, \delta)$ for some $\delta>0$.</p><a href=#mathcalb-matrix-for-t><h1 id=mathcalb-matrix-for-t><span class=hanchor arialabel=Anchor># </span>$\mathcal{B}$-matrix (for $T$)</h1></a><p>A matrix $[T]_{\mathcal{B}}$ for a linear transformation $T: V \rightarrow V$ relative to a basis $\mathcal{B}$ for $V$, with the property that $[T(\mathbf{x})]_{\mathcal{B}}=[T]_{\mathcal{B}}[\mathbf{x}]_{\mathcal{B}}$ for all $\mathbf{x}$ in $V$.</p><a href=#cauchy-schwarz-inequality><h1 id=cauchy-schwarz-inequality><span class=hanchor arialabel=Anchor># </span>Cauchy-Schwarz inequality</h1></a><p>$|\langle\mathbf{u}, \mathbf{v}\rangle| \leq|u| \cdot|v|$ for all u, $\mathbf{v}$. change of basis: See change-of-coordinates matrix.</p><a href=#change-of-coordinates-matrix-from-a-basis-mathcalb-to-a-basis-mathcalc><h1 id=change-of-coordinates-matrix-from-a-basis-mathcalb-to-a-basis-mathcalc><span class=hanchor arialabel=Anchor># </span>change-of-coordinates matrix (from a basis $\mathcal{B}$ to a basis $\mathcal{C}$)</h1></a><p>A matrix $\underset{\mathcal{C} \leftarrow \mathcal{B}}{P}$ that transforms $\mathcal{B}$-coordinate vectors into $\mathcal{C}$ coordinate vectors: $[\mathbf{x}]_{\mathcal{C}}={ }_{\mathcal{C} \leftarrow \mathcal{B}}^{P}[\mathbf{x}]_{\mathcal{B}}$. If $\mathcal{C}$ is the standard basis for $\mathbb{R}^{n}$, then ${ }_{\mathcal{C} \leftarrow \mathcal{B}}$ is sometimes written as $P_{\mathcal{B}}$.</p><a href=#characteristic-equation-of-a><h1 id=characteristic-equation-of-a><span class=hanchor arialabel=Anchor># </span>characteristic equation (of $A$)</h1></a><p>$\quad \operatorname{det}(A-\lambda I)=0$.</p><a href=#characteristic-polynomial-of-a><h1 id=characteristic-polynomial-of-a><span class=hanchor arialabel=Anchor># </span>characteristic polynomial (of $A$)</h1></a><p>$\operatorname{det}(A-\lambda I)$ or, in some texts, $\operatorname{det}(\lambda I-A)$.</p><a href=#cholesky-factorization><h1 id=cholesky-factorization><span class=hanchor arialabel=Anchor># </span>Cholesky factorization</h1></a><p>A factorization $A=R^{T} R$, where $R$ is an invertible upper triangular matrix whose diagonal entries are all positive.</p><a href=#closed-ball-in-mathbbrn><h1 id=closed-ball-in-mathbbrn><span class=hanchor arialabel=Anchor># </span>closed ball (in $\mathbb{R}^{n}$)</h1></a><p>A set ${\mathbf{x}:|\mathbf{x}-\mathbf{p}|&lt;\delta}$ in $\mathbb{R}^{n}$, where $\mathbf{p}$ is in $\mathbb{R}^{n}$ and $\delta>0$.</p><a href=#closed-set-in-mathbbrn><h1 id=closed-set-in-mathbbrn><span class=hanchor arialabel=Anchor># </span>closed set (in $\mathbb{R}^{n}$)</h1></a><p>A set that contains all of its boundary points. codomain (of a transformation $T: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$): The set $\mathbb{R}^{m}$ that contains the range of $T$. In general, if $T$ maps a vector space $V$ into a vector space $W$, then $W$ is called the codomain of $T$.</p><a href=#coefficient-matrix><h1 id=coefficient-matrix><span class=hanchor arialabel=Anchor># </span>coefficient matrix</h1></a><p>A matrix whose entries are the coefficients of a system of linear equations.</p><a href=#cofactor><h1 id=cofactor><span class=hanchor arialabel=Anchor># </span>cofactor</h1></a><p>A number $C_{i j}=(-1)^{i+j} \operatorname{det} A_{i j}$, called the $(i, j)$ cofactor of $A$, where $A_{i j}$ is the submatrix formed by deleting the $i$ th row and the $j$ th column of $A$.</p><a href=#cofactor-expansion><h1 id=cofactor-expansion><span class=hanchor arialabel=Anchor># </span>cofactor expansion</h1></a><p>A formula for $\operatorname{det} A$ using cofactors associated with one row or one column, such as for row 1:</p><p>$$
\operatorname{det} A=a_{11} C_{11}+\cdots+a_{1 n} C_{1 n}
$$</p><a href=#column-row-expansion><h1 id=column-row-expansion><span class=hanchor arialabel=Anchor># </span>column-row expansion</h1></a><p>The expression of a product $A B$ as a sum of outer products: $\operatorname{col}_{1}(A) \operatorname{row}_{1}(B)+\cdots+$ $\operatorname{col}_{n}(A) \operatorname{row}_{n}(B)$, where $n$ is the number of columns of $A$.</p><a href=#column-space-of-an-m-times-n-matrix-a><h1 id=column-space-of-an-m-times-n-matrix-a><span class=hanchor arialabel=Anchor># </span>column space (of an $m \times n$ matrix $A$)</h1></a><p>The set $\operatorname{Col} A$ of all linear combinations of the columns of $A$. If $A=\left[\mathbf{a}_{1} \cdots \mathbf{a}_{n}\right]$, then $\operatorname{Col} A=\operatorname{Span}\left{\mathbf{a}_{1}, \ldots, \mathbf{a}_{n}\right}$. Equivalently,</p><p>$$
\operatorname{Col} A=\left{\mathbf{y}: \mathbf{y}=A \mathbf{x} \text { for some } \mathbf{x} \text { in } \mathbb{R}^{n}\right}
$$</p><a href=#column-sum><h1 id=column-sum><span class=hanchor arialabel=Anchor># </span>column sum</h1></a><p>The sum of the entries in a column of a matrix. column vector: A matrix with only one column, or a single column of a matrix that has several columns.</p><a href=#commuting-matrices><h1 id=commuting-matrices><span class=hanchor arialabel=Anchor># </span>commuting matrices</h1></a><p>Two matrices $A$ and $B$ such that $A B=B A$.</p><a href=#compact-set-in-mathbbrn><h1 id=compact-set-in-mathbbrn><span class=hanchor arialabel=Anchor># </span>compact set (in $\mathbb{R}^{n}$)</h1></a><p>A set in $\mathbb{R}^{n}$ that is both closed and bounded.</p><a href=#companion-matrix><h1 id=companion-matrix><span class=hanchor arialabel=Anchor># </span>companion matrix</h1></a><p>A special form of matrix whose characteristic polynomial is $(-1)^{n} p(\lambda)$ when $p(\lambda)$ is a specified polynomial whose leading term is $\lambda^{n}$.</p><a href=#complex-eigenvalue><h1 id=complex-eigenvalue><span class=hanchor arialabel=Anchor># </span>complex eigenvalue</h1></a><p>A nonreal root of the characteristic equation of an $n \times n$ matrix.</p><a href=#complex-eigenvector><h1 id=complex-eigenvector><span class=hanchor arialabel=Anchor># </span>complex eigenvector</h1></a><p>A nonzero vector $\mathbf{x}$ in $\mathbb{C}^{n}$ such that $A \mathbf{x}=\lambda \mathbf{x}$, where $A$ is an $n \times n$ matrix and $\lambda$ is a complex eigenvalue.</p><a href=#component-of-mathbfy-orthogonal-to-mathbfu-for-mathbfu-neq-mathbf0><h1 id=component-of-mathbfy-orthogonal-to-mathbfu-for-mathbfu-neq-mathbf0><span class=hanchor arialabel=Anchor># </span>component of $\mathbf{y}$ orthogonal to $\mathbf{u}$ (for $\mathbf{u} \neq \mathbf{0}$)</h1></a><p>The vector $\mathbf{y}-\frac{\mathbf{y} \cdot \mathbf{u}}{\mathbf{u} \cdot \mathbf{u}} \mathbf{u}$.</p><a href=#composition-of-linear-transformations><h1 id=composition-of-linear-transformations><span class=hanchor arialabel=Anchor># </span>composition of linear transformations</h1></a><p>A mapping produced by applying two or more linear transformations in succession. If the transformations are matrix transformations, say left-multiplication by $B$ followed by left-multiplication by $A$, then the composition is the mapping $\mathbf{x} \mapsto A(B \mathbf{x})$.</p><a href=#condition-number-of-a><h1 id=condition-number-of-a><span class=hanchor arialabel=Anchor># </span>condition number (of $A$)</h1></a><p>The quotient $\sigma_{1} / \sigma_{n}$, where $\sigma_{1}$ is the largest singular value of $A$ and $\sigma_{n}$ is the smallest singular value. The condition number is $+\infty$ when $\sigma_{n}$ is zero.</p><a href=#conformable-for-block-multiplication><h1 id=conformable-for-block-multiplication><span class=hanchor arialabel=Anchor># </span>conformable for block multiplication</h1></a><p>Two partitioned matrices $A$ and $B$ such that the block product $A B$ is defined: The column partition of $A$ must match the row partition of $B$.</p><a href=#consistent-linear-system><h1 id=consistent-linear-system><span class=hanchor arialabel=Anchor># </span>consistent linear system</h1></a><p>A linear system with at least one solution.</p><a href=#constrained-optimization><h1 id=constrained-optimization><span class=hanchor arialabel=Anchor># </span>constrained optimization</h1></a><p>The problem of maximizing a quantity such as $\mathbf{x}^{T} A \mathbf{x}$ or $|A \mathbf{x}|$ when $\mathbf{x}$ is subject to one or more constraints, such as $\mathbf{x}^{T} \mathbf{x}=1$ or $\mathbf{x}^{T} \mathbf{v}=0$.</p><a href=#consumption-matrix><h1 id=consumption-matrix><span class=hanchor arialabel=Anchor># </span>consumption matrix</h1></a><p>A matrix in the Leontief input-output model whose columns are the unit consumption vectors for the various sectors of an economy.</p><a href=#contraction><h1 id=contraction><span class=hanchor arialabel=Anchor># </span>contraction</h1></a><p>A mapping $\mathbf{x} \mapsto r \mathbf{x}$ for some scalar $r$, with $0 \leq r \leq 1$</p><a href=#controllable-pair-of-matrices><h1 id=controllable-pair-of-matrices><span class=hanchor arialabel=Anchor># </span>controllable (pair of matrices)</h1></a><p>A matrix pair $(A, B)$ where $A$ is $n \times n, B$ has $n$ rows, and</p><p>$$
\operatorname{rank}\left[\begin{array}{lllll}
B & A B & A^{2} B & \cdots & A^{n-1} B
\end{array}\right]=n
$$</p><p>Related to a state-space model of a control system and the difference equation $\mathbf{x}_{k+1}=A \mathbf{x}_{k}+B \mathbf{u}_{k}(k=0,1, \ldots)$.</p><a href=#convergent-sequence-of-vectors><h1 id=convergent-sequence-of-vectors><span class=hanchor arialabel=Anchor># </span>convergent (sequence of vectors)</h1></a><p>A sequence $\left{\mathbf{x}_{k}\right}$ such that the entries in $\mathbf{x}_{k}$ can be made as close as desired to the entries in some fixed vector for all $k$ sufficiently large.</p><a href=#convex-combination-of-points-mathbfv_1-ldots-mathbfv_k-in-mathbbrn><h1 id=convex-combination-of-points-mathbfv_1-ldots-mathbfv_k-in-mathbbrn><span class=hanchor arialabel=Anchor># </span>convex combination (of points $\mathbf{v}_{1}, \ldots, \mathbf{v}_{k}$ in $\mathbb{R}^{n}$)</h1></a><p>A linear combination of vectors (points) in which the weights in the combination are nonnegative and the sum of the weights is 1.</p><a href=#convex-hull-of-a-set-s><h1 id=convex-hull-of-a-set-s><span class=hanchor arialabel=Anchor># </span>convex hull (of a set $S$)</h1></a><p>The set of all convex combinations of points in $S$, denoted by: conv $S$. convex set: A set $S$ with the property that for each $\mathbf{p}$ and $\mathbf{q}$ in $S$, the line segment $\overline{\mathbf{p q}}$ is contained in $S$.</p><a href=#coordinate-mapping-determined-by-an-ordered-basis-mathcalb-in-a-vector-space-v><h1 id=coordinate-mapping-determined-by-an-ordered-basis-mathcalb-in-a-vector-space-v><span class=hanchor arialabel=Anchor># </span>coordinate mapping (determined by an ordered basis $\mathcal{B}$ in a vector space $V$)</h1></a><p>A mapping that associates to each $\mathbf{x}$ in $V$ its coordinate vector $[\mathbf{x}]_{\mathcal{B}}$.</p><a href=#coordinates-of-x-relative-to-the-basis-mathcalbleftmathbfb_1-ldots-mathbfb_boldsymbolnright><h1 id=coordinates-of-x-relative-to-the-basis-mathcalbleftmathbfb_1-ldots-mathbfb_boldsymbolnright><span class=hanchor arialabel=Anchor># </span>coordinates of $x$ relative to the basis $\mathcal{B}=\left{\mathbf{b}_{1}, \ldots, \mathbf{b}_{\boldsymbol{n}}\right}$</h1></a><p>The weights $c_{1}, \ldots, c_{n}$ in the equation $\mathbf{x}=c_{1} \mathbf{b}_{1}+\cdots+c_{n} \mathbf{b}_{n}$.</p><a href=#coordinate-vector-of-mathbfx-relative-to-mathcalb><h1 id=coordinate-vector-of-mathbfx-relative-to-mathcalb><span class=hanchor arialabel=Anchor># </span>coordinate vector of $\mathbf{x}$ relative to $\mathcal{B}$</h1></a><p>The vector $[\mathbf{x}]_{\mathcal{B}}$ whose entries are the coordinates of $\mathbf{x}$ relative to the basis $\mathcal{B}$.</p><a href=#covariance-of-variables-x_i-and-x_j-for-i-neq-j><h1 id=covariance-of-variables-x_i-and-x_j-for-i-neq-j><span class=hanchor arialabel=Anchor># </span>covariance (of variables $x_{i}$ and $x_{j}$, for $i \neq j$)</h1></a><p>The entry $s_{i j}$ in the covariance matrix $S$ for a matrix of observations, where $x_{i}$ and $x_{j}$ vary over the $i$ th and $j$ th coordinates, respectively, of the observation vectors.</p><a href=#covariance-matrix-or-sample-covariance-matrix><h1 id=covariance-matrix-or-sample-covariance-matrix><span class=hanchor arialabel=Anchor># </span>covariance matrix (or sample covariance matrix)</h1></a><p>The $p \times p$ matrix $S$ defined by $S=(N-1)^{-1} B B^{T}$, where $B$ is a $p \times N$ matrix of observations in mean-deviation form.</p><a href=#cramers-rule><h1 id=cramers-rule><span class=hanchor arialabel=Anchor># </span>Cramer&rsquo;s rule</h1></a><p>A formula for each entry in the solution $\mathbf{x}$ of the equation $A \mathbf{x}=\mathbf{b}$ when $A$ is an invertible matrix.</p><a href=#cross-product-term><h1 id=cross-product-term><span class=hanchor arialabel=Anchor># </span>cross-product term</h1></a><p>A term $c x_{i} x_{j}$ in a quadratic form, with $i \neq j$.</p><a href=#cube><h1 id=cube><span class=hanchor arialabel=Anchor># </span>cube</h1></a><p>A three-dimensional solid object bounded by six square faces, with three faces meeting at each vertex.</p><a href=#decoupled-system><h1 id=decoupled-system><span class=hanchor arialabel=Anchor># </span>decoupled system</h1></a><p>A difference equation $\mathbf{y}_{k+1}=A \mathbf{y}_{k}$, or a differential equation $\mathbf{y}^{\prime}(t)=A \mathbf{y}(t)$, in which $A$ is a diagonal matrix. The discrete evolution of each entry in $\mathbf{y}_{k}$ (as a function of $k$), or the continuous evolution of each entry in the vector-valued function $\mathbf{y}(t)$, is unaffected by what happens to the other entries as $k \rightarrow \infty$ or $t \rightarrow \infty$.</p><a href=#design-matrix><h1 id=design-matrix><span class=hanchor arialabel=Anchor># </span>design matrix</h1></a><p>The matrix $X$ in the linear model $\mathbf{y}=\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\epsilon}$, where the columns of $X$ are determined in some way by the observed values of some independent variables.</p><a href=#determinant-of-a-square-matrix-a><h1 id=determinant-of-a-square-matrix-a><span class=hanchor arialabel=Anchor># </span>determinant (of a square matrix $A$)</h1></a><p>The number $\operatorname{det} A$ defined inductively by a cofactor expansion along the first row of $A$. Also, $(-1)^{r}$ times the product of the diagonal entries in any echelon form $U$ obtained from $A$ by row replacements and $r$ row interchanges (but no scaling operations).</p><a href=#diagonal-entries-in-a-matrix><h1 id=diagonal-entries-in-a-matrix><span class=hanchor arialabel=Anchor># </span>diagonal entries (in a matrix)</h1></a><p>Entries having equal row and column indices.</p><a href=#diagonalizable-matrix><h1 id=diagonalizable-matrix><span class=hanchor arialabel=Anchor># </span>diagonalizable (matrix)</h1></a><p>A matrix that can be written in factored form as $P D P^{-1}$, where $D$ is a diagonal matrix and $P$ is an invertible matrix.</p><a href=#diagonal-matrix><h1 id=diagonal-matrix><span class=hanchor arialabel=Anchor># </span>diagonal matrix</h1></a><p>A square matrix whose entries not on the main diagonal are all zero.</p><a href=#difference-equation-or-linear-recurrence-relation><h1 id=difference-equation-or-linear-recurrence-relation><span class=hanchor arialabel=Anchor># </span>difference equation (or linear recurrence relation)</h1></a><p>An equation of the form $\mathbf{x}_{k+1}=A \mathbf{x}_{k}(k=0,1,2, \ldots)$ whose solution is a sequence of vectors, $\mathbf{x}_{0}, \mathbf{x}_{1}, \ldots$</p><a href=#dilation><h1 id=dilation><span class=hanchor arialabel=Anchor># </span>dilation</h1></a><p>A mapping $\mathbf{x} \mapsto r \mathbf{x}$ for some scalar $r$, with $1 &lt; r$.</p><a href=#dimension><h1 id=dimension><span class=hanchor arialabel=Anchor># </span>dimension</h1></a><p>of a flat $S$ : The dimension of the corresponding parallel subspace.</p><p>of a set $S$ : The dimension of the smallest flat containing $S$.</p><p>of a subspace $S$ : The number of vectors in a basis for $S$, written as $\operatorname{dim} S$.</p><p>of a vector space $V$ : The number of vectors in a basis for $V$, written as $\operatorname{dim} V$. The dimension of the zero space is 0.</p><a href=#discrete-linear-dynamical-system><h1 id=discrete-linear-dynamical-system><span class=hanchor arialabel=Anchor># </span>discrete linear dynamical system</h1></a><p>A difference equation of the form $\mathbf{x}_{k+1}=A \mathbf{x}_{k}$ that describes the changes in a system (usually a physical system) as time passes. The physical system is measured at discrete times, when $k=0,1,2, \ldots$, and the state of the system at time $k$ is a vector $\mathbf{x}_{k}$ whose entries provide certain facts of interest about the system.</p><a href=#distance-between-mathbfu-and-mathbfv><h1 id=distance-between-mathbfu-and-mathbfv><span class=hanchor arialabel=Anchor># </span>distance between $\mathbf{u}$ and $\mathbf{v}$</h1></a><p>The length of the vector $\mathbf{u}-\mathbf{v}$, denoted by dist $(\mathbf{u}, \mathbf{v})$.</p><a href=#distance-to-a-subspace><h1 id=distance-to-a-subspace><span class=hanchor arialabel=Anchor># </span>distance to a subspace</h1></a><p>The distance from a given point (vector) $\mathbf{v}$ to the nearest point in the subspace.</p><a href=#distributive-laws><h1 id=distributive-laws><span class=hanchor arialabel=Anchor># </span>distributive laws</h1></a><p>(left) $A(B+C)=A B+A C$, and (right) $(B+C) A=B A+C A$, for all $A, B, C$.</p><a href=#domain-of-a-transformation-t><h1 id=domain-of-a-transformation-t><span class=hanchor arialabel=Anchor># </span>domain (of a transformation $T$)</h1></a><p>The set of all vectors $\mathbf{x}$ for which $T(\mathbf{x})$ is defined.</p><a href=#dot-product><h1 id=dot-product><span class=hanchor arialabel=Anchor># </span>dot product</h1></a><p>See inner product.</p><a href=#dynamical-system><h1 id=dynamical-system><span class=hanchor arialabel=Anchor># </span>dynamical system</h1></a><p>See discrete linear dynamical system.</p><a href=#echelon-form-or-row-echelon-form-of-a-matrix><h1 id=echelon-form-or-row-echelon-form-of-a-matrix><span class=hanchor arialabel=Anchor># </span>echelon form (or row echelon form, of a matrix)</h1></a><p>An echelon matrix that is row equivalent to the given matrix.</p><a href=#echelon-matrix-or-row-echelon-matrix><h1 id=echelon-matrix-or-row-echelon-matrix><span class=hanchor arialabel=Anchor># </span>echelon matrix (or row echelon matrix)</h1></a><p>A rectangular matrix that has three properties: (1) All nonzero rows are above any row of all zeros. (2) Each leading entry of a row is in a column to the right of the leading entry of the row above it. (3) All entries in a column below a leading entry are zero.</p><a href=#eigenfunctions-of-a-differential-equation-mathbfxprimeta-mathbfxt><h1 id=eigenfunctions-of-a-differential-equation-mathbfxprimeta-mathbfxt><span class=hanchor arialabel=Anchor># </span>eigenfunctions (of a differential equation $\mathbf{x}^{\prime}(t)=A \mathbf{x}(t)$)</h1></a><p>$\quad \mathrm{A}$ function $\mathbf{x}(t)=\mathbf{v} e^{\lambda t}$, where $\mathbf{v}$ is an eigenvector of $A$ and $\lambda$ is the corresponding eigenvalue.</p><a href=#eigenspace-of-a-corresponding-to-lambda><h1 id=eigenspace-of-a-corresponding-to-lambda><span class=hanchor arialabel=Anchor># </span>eigenspace (of $A$ corresponding to $\lambda$)</h1></a><p>The set of all solutions of $A \mathbf{x}=\lambda \mathbf{x}$, where $\lambda$ is an eigenvalue of $A$. Consists of the zero vector and all eigenvectors corresponding to $\lambda$.</p><a href=#eigenvalue-of-a><h1 id=eigenvalue-of-a><span class=hanchor arialabel=Anchor># </span>eigenvalue (of $A$)</h1></a><p>A scalar $\lambda$ such that the equation $A \mathbf{x}=\lambda \mathbf{x}$ has a solution for some nonzero vector $\mathbf{x}$.</p><a href=#eigenvector-of-a><h1 id=eigenvector-of-a><span class=hanchor arialabel=Anchor># </span>eigenvector (of $A$)</h1></a><p>A nonzero vector $\mathbf{x}$ such that $A \mathbf{x}=\lambda \mathbf{x}$ for some scalar $\lambda$.</p><a href=#eigenvector-basis><h1 id=eigenvector-basis><span class=hanchor arialabel=Anchor># </span>eigenvector basis</h1></a><p>A basis consisting entirely of eigenvectors of a given matrix.</p><a href=#eigenvector-decomposition-of-mathbfx><h1 id=eigenvector-decomposition-of-mathbfx><span class=hanchor arialabel=Anchor># </span>eigenvector decomposition (of $\mathbf{x}$)</h1></a><p>An equation, $\mathbf{x}=c_{1} \mathbf{v}_{1}+$ $\cdots+c_{n} \mathbf{v}_{n}$, expressing $\mathbf{x}$ as a linear combination of eigenvectors of a matrix.</p><a href=#elementary-matrix><h1 id=elementary-matrix><span class=hanchor arialabel=Anchor># </span>elementary matrix</h1></a><p>An invertible matrix that results by performing one elementary row operation on an identity matrix.</p><a href=#elementary-row-operations><h1 id=elementary-row-operations><span class=hanchor arialabel=Anchor># </span>elementary row operations</h1></a><p>(1) (Replacement) Replace one row by the sum of itself and a multiple of another row. (2) Interchange two rows. (3) (Scaling) Multiply all entries in a row by a nonzero constant.</p><a href=#equal-vectors><h1 id=equal-vectors><span class=hanchor arialabel=Anchor># </span>equal vectors</h1></a><p>Vectors in $\mathbb{R}^{n}$ whose corresponding entries are the same. equilibrium prices: A set of prices for the total output of the various sectors in an economy, such that the income of each sector exactly balances its expenses.</p><a href=#equilibrium-vector><h1 id=equilibrium-vector><span class=hanchor arialabel=Anchor># </span>equilibrium vector</h1></a><p>See steady-state vector.</p><a href=#equivalent-linear-systems><h1 id=equivalent-linear-systems><span class=hanchor arialabel=Anchor># </span>equivalent (linear) systems</h1></a><p>Linear systems with the same solution set.</p><a href=#exchange-model><h1 id=exchange-model><span class=hanchor arialabel=Anchor># </span>exchange model</h1></a><p>See Leontief exchange model.</p><a href=#existence-question><h1 id=existence-question><span class=hanchor arialabel=Anchor># </span>existence question</h1></a><p>Asks, &ldquo;Does a solution to the system exist?&rdquo; That is, &ldquo;Is the system consistent?&rdquo; Also, &ldquo;Does a solution of $A \mathbf{x}=\mathbf{b}$ exist for all possible $\mathbf{b}$ ?&rdquo;</p><a href=#expansion-by-cofactors><h1 id=expansion-by-cofactors><span class=hanchor arialabel=Anchor># </span>expansion by cofactors</h1></a><p>See cofactor expansion.</p><a href=#explicit-description-of-a-subspace-w-of-mathbbrn><h1 id=explicit-description-of-a-subspace-w-of-mathbbrn><span class=hanchor arialabel=Anchor># </span>explicit description (of a subspace $W$ of $\mathbb{R}^{n}$)</h1></a><p>A parametric representation of $W$ as the set of all linear combinations of a set of specified vectors.</p><a href=#extreme-point-of-a-convex-set-s><h1 id=extreme-point-of-a-convex-set-s><span class=hanchor arialabel=Anchor># </span>extreme point (of a convex set $S$)</h1></a><p>A point $\mathbf{p}$ in $S$ such that $\mathbf{p}$ is not in the interior of any line segment that lies in $S$. (That is, if $\mathbf{x}, \mathbf{y}$ are in $S$ and $\mathbf{p}$ is on the line segment $\overline{\mathbf{x y}}$, then $\mathbf{p}=\mathbf{x}$ or $\mathbf{p}=\mathbf{y}$.</p><a href=#factorization-of-a><h1 id=factorization-of-a><span class=hanchor arialabel=Anchor># </span>factorization (of $A$)</h1></a><p>An equation that expresses $A$ as a product of two or more matrices.</p><a href=#final-demand-vector-or-bill-of-final-demands><h1 id=final-demand-vector-or-bill-of-final-demands><span class=hanchor arialabel=Anchor># </span>final demand vector (or bill of final demands)</h1></a><p>The vector d in the Leontief input-output model that lists the dollar values of the goods and services demanded from the various sectors by the nonproductive part of the economy. The vector $\mathbf{d}$ can represent consumer demand, government consumption, surplus production, exports, or other external demand.</p><a href=#finite-dimensional-vector-space><h1 id=finite-dimensional-vector-space><span class=hanchor arialabel=Anchor># </span>finite-dimensional (vector space)</h1></a><p>A vector space that is spanned by a finite set of vectors.</p><a href=#flat-leftright-in-mathbbrn><h1 id=flat-leftright-in-mathbbrn><span class=hanchor arialabel=Anchor># </span>flat $\left(\right.$ in $\mathbb{R}^{n}$)</h1></a><p>A translate of a subspace of $\mathbb{R}^{n}$.</p><a href=#flexibility-matrix><h1 id=flexibility-matrix><span class=hanchor arialabel=Anchor># </span>flexibility matrix</h1></a><p>A matrix whose $j$ th column gives the deflections of an elastic beam at specified points when a unit force is applied at the $j$ th point on the beam.</p><a href=#floating-point-arithmetic><h1 id=floating-point-arithmetic><span class=hanchor arialabel=Anchor># </span>floating point arithmetic</h1></a><p>Arithmetic with numbers represented as decimals $\pm . d_{1} \cdots d_{p} \times 10^{r}$, where $r$ is an integer and the number $p$ of digits to the right of the decimal point is usually between 8 and 16 .</p><a href=#flop><h1 id=flop><span class=hanchor arialabel=Anchor># </span>flop</h1></a><p>One arithmetic operation $(+,-, *, /)$ on two real floating point numbers.</p><a href=#forward-phase-of-row-reduction><h1 id=forward-phase-of-row-reduction><span class=hanchor arialabel=Anchor># </span>forward phase (of row reduction)</h1></a><p>The first part of the algorithm that reduces a matrix to echelon form.</p><a href=#fourier-approximation-of-order-n><h1 id=fourier-approximation-of-order-n><span class=hanchor arialabel=Anchor># </span>Fourier approximation (of order $n$)</h1></a><p>The closest point in the subspace of $n$ th-order trigonometric polynomials to a given function in $C[0,2 \pi]$.</p><a href=#fourier-coefficients><h1 id=fourier-coefficients><span class=hanchor arialabel=Anchor># </span>Fourier coefficients</h1></a><p>The weights used to make a trigonometric polynomial as a Fourier approximation to a function.</p><a href=#fourier-series><h1 id=fourier-series><span class=hanchor arialabel=Anchor># </span>Fourier series</h1></a><p>An infinite series that converges to a function in the inner product space $C[0,2 \pi]$, with the inner product given by a definite integral.</p><a href=#free-variable><h1 id=free-variable><span class=hanchor arialabel=Anchor># </span>free variable</h1></a><p>Any variable in a linear system that is not a basic variable. full rank (matrix): An $m \times n$ matrix whose rank is the smaller of $m$ and $n$.</p><a href=#fundamental-set-of-solutions><h1 id=fundamental-set-of-solutions><span class=hanchor arialabel=Anchor># </span>fundamental set of solutions</h1></a><p>A basis for the set of all solutions of a homogeneous linear difference or differential equation.</p><a href=#fundamental-subspaces-determined-by-a><h1 id=fundamental-subspaces-determined-by-a><span class=hanchor arialabel=Anchor># </span>fundamental subspaces (determined by $A$)</h1></a><p>The null space and column space of $A$, and the null space and column space of $A^{T}$, with $\operatorname{Col} A^{T}$ commonly called the row space of $A$.</p><a href=#gaussian-elimination><h1 id=gaussian-elimination><span class=hanchor arialabel=Anchor># </span>Gaussian elimination</h1></a><p>See row reduction algorithm.</p><a href=#general-least-squares-problem><h1 id=general-least-squares-problem><span class=hanchor arialabel=Anchor># </span>general least-squares problem</h1></a><p>Given an $m \times n$ matrix $A$ and a vector $\mathbf{b}$ in $\mathbb{R}^{m}$, find $\hat{\mathbf{x}}$ in $\mathbb{R}^{n}$ such that $|\mathbf{b}-A \hat{\mathbf{x}}| \leq|\mathbf{b}-A \mathbf{x}|$ for all $\mathbf{x}$ in $\mathbb{R}^{n}$.</p><a href=#general-solution-of-a-linear-system><h1 id=general-solution-of-a-linear-system><span class=hanchor arialabel=Anchor># </span>general solution (of a linear system)</h1></a><p>A parametric description of a solution set that expresses the basic variables in terms of the free variables (the parameters), if any. After Section 1.5, the parametric description is written in vector form.</p><a href=#givens-rotation><h1 id=givens-rotation><span class=hanchor arialabel=Anchor># </span>Givens rotation</h1></a><p>A linear transformation from $\mathbb{R}^{n}$ to $\mathbb{R}^{n}$ used in computer programs to create zero entries in a vector (usually a column of a matrix).</p><a href=#gram-matrix-of-a><h1 id=gram-matrix-of-a><span class=hanchor arialabel=Anchor># </span>Gram matrix (of $A$)</h1></a><p>The matrix $A^{T} A$.</p><a href=#gram-schmidt-process><h1 id=gram-schmidt-process><span class=hanchor arialabel=Anchor># </span>Gram-Schmidt process</h1></a><p>An algorithm for producing an orthogonal or orthonormal basis for a subspace that is spanned by a given set of vectors.</p><a href=#homogeneous-coordinates><h1 id=homogeneous-coordinates><span class=hanchor arialabel=Anchor># </span>homogeneous coordinates</h1></a><p>In $\mathbb{R}^{3}$, the representation of $(x, y, z)$ as $(X, Y, Z, H)$ for any $H \neq 0$, where $x=X / H$, $y=Y / H$, and $z=Z / H$. In $\mathbb{R}^{2}, H$ is usually taken as 1 , and the homogeneous coordinates of $(x, y)$ are written as $(x, y, 1)$.</p><a href=#homogeneous-equation><h1 id=homogeneous-equation><span class=hanchor arialabel=Anchor># </span>homogeneous equation</h1></a><p>An equation of the form $A \mathbf{x}=\mathbf{0}$, possibly written as a vector equation or as a system of linear equations.</p><a href=#homogeneous-form-of-a-vector-mathbfv-in-mathbbrn><h1 id=homogeneous-form-of-a-vector-mathbfv-in-mathbbrn><span class=hanchor arialabel=Anchor># </span>homogeneous form of (a vector) $\mathbf{v}$ in $\mathbb{R}^{n}</h1></a><p>$ The point $\tilde{\mathbf{v}}=\left[\begin{array}{l}\mathbf{v} \\ 1\end{array}\right]$ in $\mathbb{R}^{n+1}$.</p><a href=#householder-reflection><h1 id=householder-reflection><span class=hanchor arialabel=Anchor># </span>Householder reflection</h1></a><p>A transformation $\mathbf{x} \mapsto Q \mathbf{x}$, where $Q=I-2 \mathbf{u u}^{T}$ and $\mathbf{u}$ is a unit vector $\left(\mathbf{u}^{T} \mathbf{u}=1\right)$.</p><a href=#hyperplane-in-mathbbrn><h1 id=hyperplane-in-mathbbrn><span class=hanchor arialabel=Anchor># </span>hyperplane (in $\mathbb{R}^{n}$)</h1></a><p>A flat in $\mathbb{R}^{n}$ of dimension $n-1$. Also: a translate of a subspace of dimension $n-1$.</p><a href=#identity-matrix-denoted-by-i-or-i_n><h1 id=identity-matrix-denoted-by-i-or-i_n><span class=hanchor arialabel=Anchor># </span>identity matrix (denoted by $I$ or $I_{n}$)</h1></a><p>A square matrix with ones on the diagonal and zeros elsewhere.</p><a href=#ill-conditioned-matrix><h1 id=ill-conditioned-matrix><span class=hanchor arialabel=Anchor># </span>ill-conditioned matrix</h1></a><p>A square matrix with a large (or possibly infinite) condition number; a matrix that is singular or can become singular if some of its entries are changed ever so slightly.</p><a href=#image-of-a-vector-mathbfx-under-a-transformation-t><h1 id=image-of-a-vector-mathbfx-under-a-transformation-t><span class=hanchor arialabel=Anchor># </span>image (of a vector $\mathbf{x}$ under a transformation $T$)</h1></a><p>The vector $T(\mathbf{x})$ assigned to $\mathbf{x}$ by $T$. implicit description (of a subspace $W$ of $\mathbb{R}^{n}$): A set of one or more homogeneous equations that characterize the points of $W$.</p><a href=#im-mathbfx><h1 id=im-mathbfx><span class=hanchor arialabel=Anchor># </span>Im $\mathbf{x}$</h1></a><p>The vector in $\mathbb{R}^{n}$ formed from the imaginary parts of the entries of a vector $\mathbf{x}$ in $\mathbb{C}^{n}$.</p><a href=#inconsistent-linear-system><h1 id=inconsistent-linear-system><span class=hanchor arialabel=Anchor># </span>inconsistent linear system</h1></a><p>A linear system with no solution.</p><a href=#indefinite-matrix><h1 id=indefinite-matrix><span class=hanchor arialabel=Anchor># </span>indefinite matrix</h1></a><p>A symmetric matrix $A$ such that $\mathbf{x}^{T} A \mathbf{x}$ assumes both positive and negative values.</p><a href=#indefinite-quadratic-form><h1 id=indefinite-quadratic-form><span class=hanchor arialabel=Anchor># </span>indefinite quadratic form</h1></a><p>A quadratic form $Q$ such that $Q(\mathbf{x})$ assumes both positive and negative values.</p><a href=#infinite-dimensional-vector-space><h1 id=infinite-dimensional-vector-space><span class=hanchor arialabel=Anchor># </span>infinite-dimensional (vector space)</h1></a><p>A nonzero vector space $V$ that has no finite basis.</p><a href=#inner-product><h1 id=inner-product><span class=hanchor arialabel=Anchor># </span>inner product</h1></a><p>The scalar $\mathbf{u}^{T} \mathbf{v}$, usually written as $\mathbf{u} \cdot \mathbf{v}$, where $\mathbf{u}$ and $\mathbf{v}$ are vectors in $\mathbb{R}^{n}$ viewed as $n \times 1$ matrices. Also called the dot product of $\mathbf{u}$ and $\mathbf{v}$. In general, a function on a vector space that assigns to each pair of vectors $\mathbf{u}$ and $\mathbf{v}$ a number $\langle\mathbf{u}, \mathbf{v}\rangle$, subject to certain axioms. See Section 6.7.</p><a href=#inner-product-space><h1 id=inner-product-space><span class=hanchor arialabel=Anchor># </span>inner product space</h1></a><p>A vector space on which is defined an inner product.</p><a href=#input-output-matrix><h1 id=input-output-matrix><span class=hanchor arialabel=Anchor># </span>input-output matrix</h1></a><p>See consumption matrix.</p><a href=#input-output-model><h1 id=input-output-model><span class=hanchor arialabel=Anchor># </span>input-output model</h1></a><p>See Leontief input-output model.</p><a href=#interior-point-of-a-set-s-in-mathbbrn><h1 id=interior-point-of-a-set-s-in-mathbbrn><span class=hanchor arialabel=Anchor># </span>interior point (of a set $S$ in $\mathbb{R}^{n}$)</h1></a><p>A point $\mathbf{p}$ in $S$ such that for some $\delta>0$, the open ball $\mathbf{B}(\mathbf{p}, \delta)$ centered at $\mathbf{p}$ is contained in $S$.</p><a href=#intermediate-demands><h1 id=intermediate-demands><span class=hanchor arialabel=Anchor># </span>intermediate demands</h1></a><p>Demands for goods or services that will be consumed in the process of producing other goods and services for consumers. If $\mathbf{x}$ is the production level and $C$ is the consumption matrix, then $C \mathbf{x}$ lists the intermediate demands.</p><a href=#interpolating-polynomial><h1 id=interpolating-polynomial><span class=hanchor arialabel=Anchor># </span>interpolating polynomial</h1></a><p>A polynomial whose graph passes through every point in a set of data points in $\mathbb{R}^{2}$.</p><a href=#invariant-subspace-for-a><h1 id=invariant-subspace-for-a><span class=hanchor arialabel=Anchor># </span>invariant subspace (for $A$)</h1></a><p>A subspace $H$ such that $A \mathbf{x}$ is in $H$ whenever $\mathbf{x}$ is in $H$.</p><a href=#inverse-of-an-n-times-n-matrix-a><h1 id=inverse-of-an-n-times-n-matrix-a><span class=hanchor arialabel=Anchor># </span>inverse (of an $n \times n$ matrix $A$)</h1></a><p>An $n \times n$ matrix $A^{-1}$ such that $A A^{-1}=A^{-1} A=I_{n}$.</p><a href=#inverse-power-method><h1 id=inverse-power-method><span class=hanchor arialabel=Anchor># </span>inverse power method</h1></a><p>An algorithm for estimating an eigenvalue $\lambda$ of a square matrix, when a good initial estimate of $\lambda$ is available.</p><a href=#invertible-linear-transformation><h1 id=invertible-linear-transformation><span class=hanchor arialabel=Anchor># </span>invertible linear transformation</h1></a><p>A linear transformation $T: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ such that there exists a function $S: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ satisfying both $T(S(\mathbf{x}))=\mathbf{x}$ and $S(T(\mathbf{x}))=\mathbf{x}$ for all $\mathbf{x}$ in $\mathbb{R}^{n}$.</p><a href=#invertible-matrix><h1 id=invertible-matrix><span class=hanchor arialabel=Anchor># </span>invertible matrix</h1></a><p>A square matrix that possesses an inverse.</p><a href=#isomorphic-vector-spaces><h1 id=isomorphic-vector-spaces><span class=hanchor arialabel=Anchor># </span>isomorphic vector spaces</h1></a><p>Two vector spaces $V$ and $W$ for which there is a one-to-one linear transformation $T$ that maps $V$ onto $W$</p><a href=#isomorphism><h1 id=isomorphism><span class=hanchor arialabel=Anchor># </span>isomorphism</h1></a><p>A one-to-one linear mapping from one vector space onto another.</p><a href=#kernel-of-a-linear-transformation-t><h1 id=kernel-of-a-linear-transformation-t><span class=hanchor arialabel=Anchor># </span>kernel (of a linear transformation $T</h1></a><p>V \rightarrow W$): The set of $\mathbf{x}$ in $V$ such that $T(\mathbf{x})=\mathbf{0}$.</p><a href=#kirchhoffs-laws><h1 id=kirchhoffs-laws><span class=hanchor arialabel=Anchor># </span>Kirchhoff&rsquo;s laws</h1></a><p>(1) (voltage law) The algebraic sum of the $R I$ voltage drops in one direction around a loop equals the algebraic sum of the voltage sources in the same direction around the loop. (2) (current law) The current in a branch is the algebraic sum of the loop currents flowing through that branch.</p><a href=#ladder-network><h1 id=ladder-network><span class=hanchor arialabel=Anchor># </span>ladder network</h1></a><p>An electrical network assembled by connecting in series two or more electrical circuits.</p><a href=#leading-entry><h1 id=leading-entry><span class=hanchor arialabel=Anchor># </span>leading entry</h1></a><p>The leftmost nonzero entry in a row of a matrix. least-squares error: The distance $|\mathbf{b}-A \hat{\mathbf{x}}|$ from $\mathbf{b}$ to $A \hat{\mathbf{x}}$, when $\hat{\mathbf{x}}$ is a least-squares solution of $A \mathbf{x}=\mathbf{b}$.</p><a href=#least-squares-line><h1 id=least-squares-line><span class=hanchor arialabel=Anchor># </span>least-squares line</h1></a><p>The line $y=\hat{\beta}_{0}+\hat{\beta}_{1} x$ that minimizes the least-squares error in the equation $\mathbf{y}=X \boldsymbol{\beta}+\boldsymbol{\epsilon}$.</p><a href=#least-squares-solution-of-a-mathbfxmathbfb><h1 id=least-squares-solution-of-a-mathbfxmathbfb><span class=hanchor arialabel=Anchor># </span>least-squares solution (of $A \mathbf{x}=\mathbf{b}$)</h1></a><p>A vector $\hat{\mathbf{x}}$ such that $|\mathbf{b}-A \hat{\mathbf{x}}| \leq|\mathbf{b}-A \mathbf{x}|$ for all $\mathbf{x}$ in $\mathbb{R}^{n}$</p><a href=#left-inverse-of-a><h1 id=left-inverse-of-a><span class=hanchor arialabel=Anchor># </span>left inverse (of $A$)</h1></a><p>Any rectangular matrix $C$ such that $C A=I$.</p><a href=#left-multiplication-by-a><h1 id=left-multiplication-by-a><span class=hanchor arialabel=Anchor># </span>left-multiplication (by $A$)</h1></a><p>Multiplication of a vector or matrix on the left by $A$.</p><a href=#left-singular-vectors-of-a><h1 id=left-singular-vectors-of-a><span class=hanchor arialabel=Anchor># </span>left singular vectors (of $A$)</h1></a><p>The columns of $U$ in the singular value decomposition $A=U \Sigma V^{T}$.</p><a href=#length-or-norm-of-mathbfv><h1 id=length-or-norm-of-mathbfv><span class=hanchor arialabel=Anchor># </span>length (or norm, of $\mathbf{v}$)</h1></a><p>The scalar $|\mathbf{v}|=\sqrt{\mathbf{v} \cdot \mathbf{v}}=\sqrt{\langle\mathbf{v}, \mathbf{v}\rangle}$.</p><a href=#leontief-exchange-or-closed-model><h1 id=leontief-exchange-or-closed-model><span class=hanchor arialabel=Anchor># </span>Leontief exchange (or closed) model</h1></a><p>A model of an economy where inputs and outputs are fixed, and where a set of prices for the outputs of the sectors is sought such that the income of each sector equals its expenditures. This &ldquo;equilibrium&rdquo; condition is expressed as a system of linear equations, with the prices as the unknowns.</p><a href=#leontief-input-output-model-or-leontief-production-equation><h1 id=leontief-input-output-model-or-leontief-production-equation><span class=hanchor arialabel=Anchor># </span>Leontief input-output model (or Leontief production equation)</h1></a><p>The equation $\mathbf{x}=C \mathbf{x}+\mathbf{d}$, where $\mathbf{x}$ is production, $\mathbf{d}$ is final demand, and $C$ is the consumption (or input-output) matrix. The $j$ th column of $C$ lists the inputs that sector $j$ consumes per unit of output.</p><a href=#level-set-or-gradient-of-a-linear-functional-f-on-mathbbrn><h1 id=level-set-or-gradient-of-a-linear-functional-f-on-mathbbrn><span class=hanchor arialabel=Anchor># </span>level set (or gradient) of a linear functional $f$ on $\mathbb{R}^{n}$</h1></a><p>A set $[f: d]=\left{\mathbf{x} \in \mathbb{R}^{n}: f(\mathbf{x})=d\right}$</p><a href=#linear-combination><h1 id=linear-combination><span class=hanchor arialabel=Anchor># </span>linear combination</h1></a><p>A sum of scalar multiples of vectors. The scalars are called the weights.</p><a href=#linear-dependence-relation><h1 id=linear-dependence-relation><span class=hanchor arialabel=Anchor># </span>linear dependence relation</h1></a><p>A homogeneous vector equation where the weights are all specified and at least one weight is nonzero.</p><a href=#linear-equation-in-the-variables-x_1-ldots-x_n><h1 id=linear-equation-in-the-variables-x_1-ldots-x_n><span class=hanchor arialabel=Anchor># </span>linear equation (in the variables $x_{1}, \ldots, x_{n}$)</h1></a><p>An equation that can be written in the form $a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{n} x_{n}=b$, where $b$ and the coefficients $a_{1}, \ldots, a_{n}$ are real or complex numbers.</p><a href=#linear-filter><h1 id=linear-filter><span class=hanchor arialabel=Anchor># </span>linear filter</h1></a><p>A linear difference equation used to transform discrete-time signals.</p><a href=#linear-functional-on-mathbbrn><h1 id=linear-functional-on-mathbbrn><span class=hanchor arialabel=Anchor># </span>linear functional (on $\mathbb{R}^{n}$)</h1></a><p>A linear transformation $f$ from $\mathbb{R}^{n}$ into $\mathbb{R}$.</p><a href=#linearly-dependent-vectors><h1 id=linearly-dependent-vectors><span class=hanchor arialabel=Anchor># </span>linearly dependent (vectors)</h1></a><p>An indexed set $\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$ with the property that there exist weights $c_{1}, \ldots, c_{p}$, not all zero, such that $c_{1} \mathbf{v}_{1}+\cdots+c_{p} \mathbf{v}_{p}=\mathbf{0}$. That is, the vector equation $c_{1} \mathbf{v}_{1}+c_{2} \mathbf{v}_{2}+\cdots+c_{p} \mathbf{v}_{p}=\mathbf{0}$ has a nontrivial solution.</p><a href=#linearly-independent-vectors><h1 id=linearly-independent-vectors><span class=hanchor arialabel=Anchor># </span>linearly independent (vectors)</h1></a><p>An indexed set $\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$ with the property that the vector equation $c_{1} \mathbf{v}_{1}+$ $c_{2} \mathbf{v}_{2}+\cdots+c_{p} \mathbf{v}_{p}=\mathbf{0}$ has only the trivial solution, $c_{1}=\cdots=c_{p}=0$.</p><a href=#linear-model-in-statistics><h1 id=linear-model-in-statistics><span class=hanchor arialabel=Anchor># </span>linear model (in statistics)</h1></a><p>Any equation of the form $\mathbf{y}=X \boldsymbol{\beta}+\boldsymbol{\epsilon}$, where $X$ and $\mathbf{y}$ are known and $\boldsymbol{\beta}$ is to be chosen to minimize the length of the residual vector, $\epsilon$.</p><a href=#linear-system><h1 id=linear-system><span class=hanchor arialabel=Anchor># </span>linear system</h1></a><p>A collection of one or more linear equations involving the same variables, say, $x_{1}, \ldots, x_{n}$.</p><a href=#linear-transformation-boldsymbolt-from-a-vector-space-v-into-a-vector-space-w><h1 id=linear-transformation-boldsymbolt-from-a-vector-space-v-into-a-vector-space-w><span class=hanchor arialabel=Anchor># </span>linear transformation $\boldsymbol{T}$ (from a vector space $V$ into a vector space $W$)</h1></a><p>A rule $T$ that assigns to each vector $\mathbf{x}$ in $V$ a unique vector $T(\mathbf{x})$ in $W$, such that (i) $T(\mathbf{u}+\mathbf{v})=T(\mathbf{u})+T(\mathbf{v})$ for all $\mathbf{u}, \mathbf{v}$ in $V$, and (ii) $T(c \mathbf{u})=c T(\mathbf{u})$ for all $\mathbf{u}$ in $V$ and all scalars $c$. Notation: $T: V \rightarrow W ;$ also, $\mathbf{x} \mapsto A \mathbf{x}$ when $T: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ and $A$ is the standard matrix for $T$.</p><a href=#line-through-p-parallel-to-mathbfv><h1 id=line-through-p-parallel-to-mathbfv><span class=hanchor arialabel=Anchor># </span>line through p parallel to $\mathbf{v}$</h1></a><p>The set ${\mathbf{p}+t \mathbf{v}: t$ in $\mathbb{R}}$.</p><a href=#loop-current><h1 id=loop-current><span class=hanchor arialabel=Anchor># </span>loop current</h1></a><p>The amount of electric current flowing through a loop that makes the algebraic sum of the $R I$ voltage drops around the loop equal to the algebraic sum of the voltage sources in the loop.</p><a href=#lower-triangular-matrix><h1 id=lower-triangular-matrix><span class=hanchor arialabel=Anchor># </span>lower triangular matrix</h1></a><p>A matrix with zeros above the main diagonal.</p><a href=#lower-triangular-part-of-a><h1 id=lower-triangular-part-of-a><span class=hanchor arialabel=Anchor># </span>lower triangular part (of $A$)</h1></a><p>A lower triangular matrix whose entries on the main diagonal and below agree with those in $A$.</p><a href=#lu-factorization><h1 id=lu-factorization><span class=hanchor arialabel=Anchor># </span>LU factorization</h1></a><p>The representation of a matrix $A$ in the form $A=L U$ where $L$ is a square lower triangular matrix with ones on the diagonal (a unit lower triangular matrix) and $U$ is an echelon form of $A$.</p><a href=#magnitude-of-a-vector><h1 id=magnitude-of-a-vector><span class=hanchor arialabel=Anchor># </span>magnitude (of a vector)</h1></a><p>See norm.</p><a href=#main-diagonal-of-a-matrix><h1 id=main-diagonal-of-a-matrix><span class=hanchor arialabel=Anchor># </span>main diagonal (of a matrix)</h1></a><p>The entries with equal row and column indices.</p><a href=#mapping><h1 id=mapping><span class=hanchor arialabel=Anchor># </span>mapping</h1></a><p>See transformation.</p><a href=#markov-chain><h1 id=markov-chain><span class=hanchor arialabel=Anchor># </span>Markov chain</h1></a><p>A sequence of probability vectors $\mathbf{x}_{0}, \mathbf{x}_{1}$, $\mathbf{x}_{2}, \ldots$, together with a stochastic matrix $P$ such that $\mathbf{x}_{k+1}=P \mathbf{x}_{k}$ for $k=0,1,2, \ldots$</p><a href=#matrix><h1 id=matrix><span class=hanchor arialabel=Anchor># </span>matrix</h1></a><p>A rectangular array of numbers.</p><a href=#matrix-equation><h1 id=matrix-equation><span class=hanchor arialabel=Anchor># </span>matrix equation</h1></a><p>An equation that involves at least one matrix; for instance, $A \mathbf{x}=\mathbf{b}$.</p><a href=#matrix-for-t-relative-to-bases-mathcalb-and-mathcalc><h1 id=matrix-for-t-relative-to-bases-mathcalb-and-mathcalc><span class=hanchor arialabel=Anchor># </span>matrix for $T$ relative to bases $\mathcal{B}$ and $\mathcal{C}$</h1></a><p>A matrix $M$ for a linear transformation $T: V \rightarrow W$ with the property that $[T(\mathbf{x})]_{\mathcal{C}}=M[\mathbf{x}]_{\mathcal{B}}$ for all $\mathbf{x}$ in $V$, where $\mathcal{B}$ is a basis for $V$ and $\mathcal{C}$ is a basis for $W$. When $W=V$ and $\mathcal{C}=\mathcal{B}$, the matrix $M$ is called the $\mathcal{B}$-matrix for $T$ and is denoted by $[T]_{\mathcal{B}}$.</p><a href=#matrix-of-observations><h1 id=matrix-of-observations><span class=hanchor arialabel=Anchor># </span>matrix of observations</h1></a><p>A $p \times N$ matrix whose columns are observation vectors, each column listing $p$ measurements made on an individual or object in a specified population or set. matrix transformation: A mapping $\mathbf{x} \mapsto A \mathbf{x}$, where $A$ is an $m \times n$ matrix and $\mathbf{x}$ represents any vector in $\mathbb{R}^{n}$.</p><a href=#maximal-linearly-independent-set-in-v><h1 id=maximal-linearly-independent-set-in-v><span class=hanchor arialabel=Anchor># </span>maximal linearly independent set (in $V$)</h1></a><p>A linearly independent set $\mathcal{B}$ in $V$ such that if a vector $\mathbf{v}$ in $V$ but not in $\mathcal{B}$ is added to $\mathcal{B}$, then the new set is linearly dependent.</p><a href=#mean-deviation-form-of-a-matrix-of-observations><h1 id=mean-deviation-form-of-a-matrix-of-observations><span class=hanchor arialabel=Anchor># </span>mean-deviation form (of a matrix of observations)</h1></a><p>A matrix whose row vectors are in mean-deviation form. For each row, the entries sum to zero.</p><a href=#mean-deviation-form-of-a-vector><h1 id=mean-deviation-form-of-a-vector><span class=hanchor arialabel=Anchor># </span>mean-deviation form (of a vector)</h1></a><p>A vector whose entries sum to zero.</p><a href=#mean-square-error><h1 id=mean-square-error><span class=hanchor arialabel=Anchor># </span>mean square error</h1></a><p>The error of an approximation in an inner product space, where the inner product is defined by a definite integral.</p><a href=#migration-matrix><h1 id=migration-matrix><span class=hanchor arialabel=Anchor># </span>migration matrix</h1></a><p>A matrix that gives the percentage movement between different locations, from one period to the next.</p><a href=#minimal-spanning-set-for-a-subspace-h><h1 id=minimal-spanning-set-for-a-subspace-h><span class=hanchor arialabel=Anchor># </span>minimal spanning set (for a subspace $H$)</h1></a><p>A set $\mathcal{B}$ that spans $H$ and has the property that if one of the elements of $\mathcal{B}$ is removed from $\mathcal{B}$, then the new set does not span $H$.</p><a href=#m-times-n-matrix><h1 id=m-times-n-matrix><span class=hanchor arialabel=Anchor># </span>$m \times n$ matrix</h1></a><p>A matrix with $m$ rows and $n$ columns.</p><a href=#moore-penrose-inverse><h1 id=moore-penrose-inverse><span class=hanchor arialabel=Anchor># </span>Moore-Penrose inverse</h1></a><p>See pseudoinverse.</p><a href=#multiple-regression><h1 id=multiple-regression><span class=hanchor arialabel=Anchor># </span>multiple regression</h1></a><p>A linear model involving several independent variables and one dependent variable.</p><a href=#nearly-singular-matrix><h1 id=nearly-singular-matrix><span class=hanchor arialabel=Anchor># </span>nearly singular matrix</h1></a><p>An ill-conditioned matrix.</p><a href=#negative-definite-matrix><h1 id=negative-definite-matrix><span class=hanchor arialabel=Anchor># </span>negative definite matrix</h1></a><p>A symmetric matrix $A$ such that $\mathbf{x}^{T} A \mathbf{x}&lt;0$ for all $\mathbf{x} \neq \mathbf{0}$.</p><a href=#negative-definite-quadratic-form><h1 id=negative-definite-quadratic-form><span class=hanchor arialabel=Anchor># </span>negative definite quadratic form</h1></a><p>A quadratic form $Q$ such that $Q(\mathbf{x})&lt;0$ for all $\mathbf{x} \neq \mathbf{0}$.</p><a href=#negative-semidefinite-matrix><h1 id=negative-semidefinite-matrix><span class=hanchor arialabel=Anchor># </span>negative semidefinite matrix</h1></a><p>A symmetric matrix $A$ such that $\mathbf{x}^{T} A \mathbf{x} \leq 0$ for all $\mathbf{x}$.</p><a href=#negative-semidefinite-quadratic-form><h1 id=negative-semidefinite-quadratic-form><span class=hanchor arialabel=Anchor># </span>negative semidefinite quadratic form</h1></a><p>A quadratic form $Q$ such that $Q(\mathbf{x}) \leq 0$ for all $\mathbf{x}$.</p><a href=#nonhomogeneous-equation><h1 id=nonhomogeneous-equation><span class=hanchor arialabel=Anchor># </span>nonhomogeneous equation</h1></a><p>An equation of the form $A \mathbf{x}=\mathbf{b}$ with $\mathbf{b} \neq \mathbf{0}$, possibly written as a vector equation or as a system of linear equations.</p><a href=#nonsingular-matrix><h1 id=nonsingular-matrix><span class=hanchor arialabel=Anchor># </span>nonsingular (matrix)</h1></a><p>An invertible matrix.</p><a href=#nontrivial-solution><h1 id=nontrivial-solution><span class=hanchor arialabel=Anchor># </span>nontrivial solution</h1></a><p>A nonzero solution of a homogeneous equation or system of homogeneous equations.</p><a href=#nonzero-matrix-or-vector><h1 id=nonzero-matrix-or-vector><span class=hanchor arialabel=Anchor># </span>nonzero (matrix or vector)</h1></a><p>A matrix (with possibly only one row or column) that contains at least one nonzero entry.</p><a href=#norm-or-length-of-mathbfv><h1 id=norm-or-length-of-mathbfv><span class=hanchor arialabel=Anchor># </span>norm (or length, of $\mathbf{v}$)</h1></a><p>The scalar $|\mathbf{v}|=\sqrt{\mathbf{v} \cdot \mathbf{v}}=\sqrt{\langle\mathbf{v}, \mathbf{v}\rangle}$.</p><a href=#normal-equations><h1 id=normal-equations><span class=hanchor arialabel=Anchor># </span>normal equations</h1></a><p>The system of equations represented by $A^{T} A \mathbf{x}=A^{T} \mathbf{b}$, whose solution yields all least-squares solutions of $A \mathbf{x}=\mathbf{b}$. In statistics, a common notation is $X^{T} X \boldsymbol{\beta}=X^{T} \mathbf{y}$</p><a href=#normalizing-a-nonzero-vector-mathbfv><h1 id=normalizing-a-nonzero-vector-mathbfv><span class=hanchor arialabel=Anchor># </span>normalizing (a nonzero vector $\mathbf{v}$)</h1></a><p>The process of creating a unit vector $\mathbf{u}$ that is a positive multiple of $\mathbf{v}$.</p><a href=#normal-vector-to-a-subspace-v-of-mathbbrn><h1 id=normal-vector-to-a-subspace-v-of-mathbbrn><span class=hanchor arialabel=Anchor># </span>normal vector (to a subspace $V$ of $\mathbb{R}^{n}$)</h1></a><p>A vector $\mathbf{n}$ in $\mathbb{R}^{n}$ such that $\mathbf{n} \cdot \mathbf{x}=0$ for all $\mathbf{x}$ in $V$.</p><a href=#null-space--of-an-m-times-n-matrix-a><h1 id=null-space--of-an-m-times-n-matrix-a><span class=hanchor arialabel=Anchor># </span>null space ( of an $m \times n$ matrix $A$)</h1></a><p>The set $\operatorname{Nul} A$ of all solutions to the homogeneous equation $A \mathbf{x}=\mathbf{0}$. Nul $A={\mathbf{x}: \mathbf{x}$ is in $\mathbb{R}^{n}$ and $\left.A \mathbf{x}=\mathbf{0}\right}$</p><a href=#observation-vector><h1 id=observation-vector><span class=hanchor arialabel=Anchor># </span>observation vector</h1></a><p>The vector $\mathbf{y}$ in the linear model $\mathbf{y}=X \boldsymbol{\beta}+\boldsymbol{\epsilon}$, where the entries in $\mathbf{y}$ are the observed values of a dependent variable.</p><a href=#one-to-one-mapping><h1 id=one-to-one-mapping><span class=hanchor arialabel=Anchor># </span>one-to-one (mapping)</h1></a><p>A mapping $T: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ such that each $\mathbf{b}$ in $R^{m}$ is the image of at most one $\mathbf{x}$ in $\mathbb{R}^{n}$.</p><a href=#onto-mapping><h1 id=onto-mapping><span class=hanchor arialabel=Anchor># </span>onto (mapping)</h1></a><p>A mapping $T: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ such that each $\mathbf{b}$ in $R^{m}$ is the image of at least one $\mathbf{x}$ in $\mathbb{R}^{n}$.</p><a href=#open-ball-mathbfbmathbfp-delta-in-mathbbrn><h1 id=open-ball-mathbfbmathbfp-delta-in-mathbbrn><span class=hanchor arialabel=Anchor># </span>open ball $\mathbf{B}(\mathbf{p}, \delta)$ in $\mathbb{R}^{n}$</h1></a><p>The set ${\mathbf{x}:|\mathbf{x}-\mathbf{p}|&lt;\delta}$ in $\mathbb{R}^{n}$, where $\delta>0$</p><a href=#open-set-s-in-mathbbrn><h1 id=open-set-s-in-mathbbrn><span class=hanchor arialabel=Anchor># </span>open set $S$ in $\mathbb{R}^{n}$</h1></a><p>A set that contains none of its boundary points. (Equivalently, $S$ is open if every point of $S$ is an interior point.)</p><a href=#origin><h1 id=origin><span class=hanchor arialabel=Anchor># </span>origin</h1></a><p>The zero vector.</p><a href=#orthogonal-basis><h1 id=orthogonal-basis><span class=hanchor arialabel=Anchor># </span>orthogonal basis</h1></a><p>A basis that is also an orthogonal set.</p><a href=#orthogonal-complement--of-w><h1 id=orthogonal-complement--of-w><span class=hanchor arialabel=Anchor># </span>orthogonal complement ( of $W$)</h1></a><p>The set $W^{\perp}$ of all vectors orthogonal to $W$.</p><a href=#orthogonal-decomposition><h1 id=orthogonal-decomposition><span class=hanchor arialabel=Anchor># </span>orthogonal decomposition</h1></a><p>The representation of a vector $\mathbf{y}$ as the sum of two vectors, one in a specified subspace $W$ and the other in $W^{\perp}$. In general, a decomposition $\mathbf{y}=c_{1} \mathbf{u}_{1}+\cdots+c_{p} \mathbf{u}_{p}$, where $\left{\mathbf{u}_{1}, \ldots, \mathbf{u}_{p}\right}$ is an orthogonal basis for a subspace that contains $\mathbf{y}$.</p><a href=#orthogonally-diagonalizable-matrix><h1 id=orthogonally-diagonalizable-matrix><span class=hanchor arialabel=Anchor># </span>orthogonally diagonalizable (matrix)</h1></a><p>A matrix $A$ that admits a factorization, $A=P D P^{-1}$, with $P$ an orthogonal matrix $\left(P^{-1}=P^{T}\right)$ and $D$ diagonal.</p><a href=#orthogonal-matrix><h1 id=orthogonal-matrix><span class=hanchor arialabel=Anchor># </span>orthogonal matrix</h1></a><p>A square invertible matrix $U$ such that $U^{-1}=U^{T}$</p><a href=#orthogonal-projection-of-mathbfy-onto-mathbfu-or-onto-the-line-through-mathbfu-and-the-origin-for-mathbfu-neq-mathbf0><h1 id=orthogonal-projection-of-mathbfy-onto-mathbfu-or-onto-the-line-through-mathbfu-and-the-origin-for-mathbfu-neq-mathbf0><span class=hanchor arialabel=Anchor># </span>orthogonal projection of $\mathbf{y}$ onto $\mathbf{u}$ (or onto the line through $\mathbf{u}$ and the origin, for $\mathbf{u} \neq \mathbf{0}$)</h1></a><p>The vector $\hat{\mathbf{y}}$ defined by $\hat{\mathbf{y}}=\frac{\mathbf{y} \cdot \mathbf{u}}{\mathbf{u} \cdot \mathbf{u}} \mathbf{u}$. orthogonal projection of y onto $W$ : The unique vector $\hat{\mathbf{y}}$ in $W$ such that $\mathbf{y}-\hat{\mathbf{y}}$ is orthogonal to $W$. Notation: $\hat{\mathbf{y}}=\operatorname{proj}_{W} \mathbf{y}$.</p><a href=#orthogonal-set><h1 id=orthogonal-set><span class=hanchor arialabel=Anchor># </span>orthogonal set</h1></a><p>A set $S$ of vectors such that $\mathbf{u} \cdot \mathbf{v}=0$ for each distinct pair $\mathbf{u}, \mathbf{v}$ in $S$.</p><a href=#orthogonal-to-boldsymbolw><h1 id=orthogonal-to-boldsymbolw><span class=hanchor arialabel=Anchor># </span>orthogonal to $\boldsymbol{W}$</h1></a><p>Orthogonal to every vector in $W$.</p><a href=#orthonormal-basis><h1 id=orthonormal-basis><span class=hanchor arialabel=Anchor># </span>orthonormal basis</h1></a><p>A basis that is an orthogonal set of unit vectors.</p><a href=#orthonormal-set><h1 id=orthonormal-set><span class=hanchor arialabel=Anchor># </span>orthonormal set</h1></a><p>An orthogonal set of unit vectors.</p><a href=#outer-product><h1 id=outer-product><span class=hanchor arialabel=Anchor># </span>outer product</h1></a><p>A matrix product $\mathbf{u v}^{T}$ where $\mathbf{u}$ and $\mathbf{v}$ are vectors in $\mathbb{R}^{n}$ viewed as $n \times 1$ matrices. (The transpose symbol is on the &ldquo;outside&rdquo; of the symbols $\mathbf{u}$ and $\mathbf{v}$.)</p><a href=#overdetermined-system><h1 id=overdetermined-system><span class=hanchor arialabel=Anchor># </span>overdetermined system</h1></a><p>A system of equations with more equations than unknowns.</p><a href=#parallel-flats><h1 id=parallel-flats><span class=hanchor arialabel=Anchor># </span>parallel flats</h1></a><p>Two or more flats such that each flat is a translate of the other flats. parallelogram rule for addition: A geometric interpretation of the sum of two vectors $\mathbf{u}, \mathbf{v}$ as the diagonal of the parallelogram determined by $\mathbf{u}, \mathbf{v}$, and $\mathbf{0}$.</p><a href=#parameter-vector><h1 id=parameter-vector><span class=hanchor arialabel=Anchor># </span>parameter vector</h1></a><p>The unknown vector $\boldsymbol{\beta}$ in the linear model $\mathbf{y}=X \boldsymbol{\beta}+\boldsymbol{\epsilon}$</p><a href=#parametric-equation-of-a-line><h1 id=parametric-equation-of-a-line><span class=hanchor arialabel=Anchor># </span>parametric equation of a line</h1></a><p>An equation of the form $\mathbf{x}=\mathbf{p}+t \mathbf{v}(t$ in $\mathbb{R})$.</p><a href=#parametric-equation-of-a-plane><h1 id=parametric-equation-of-a-plane><span class=hanchor arialabel=Anchor># </span>parametric equation of a plane</h1></a><p>An equation of the form $\mathbf{x}=\mathbf{p}+s \mathbf{u}+t \mathbf{v} \quad(s, t$ in $\mathbb{R})$, with $\mathbf{u}$ and $\mathbf{v}$ linearly independent.</p><a href=#partitioned-matrix-or-block-matrix><h1 id=partitioned-matrix-or-block-matrix><span class=hanchor arialabel=Anchor># </span>partitioned matrix (or block matrix)</h1></a><p>A matrix whose entries are themselves matrices of appropriate sizes.</p><a href=#permuted-lower-triangular-matrix><h1 id=permuted-lower-triangular-matrix><span class=hanchor arialabel=Anchor># </span>permuted lower triangular matrix</h1></a><p>A matrix such that a permutation of its rows will form a lower triangular matrix.</p><a href=#permuted-lu-factorization><h1 id=permuted-lu-factorization><span class=hanchor arialabel=Anchor># </span>permuted LU factorization</h1></a><p>The representation of a matrix $A$ in the form $A=L U$ where $L$ is a square matrix such that a permutation of its rows will form a unit lower triangular matrix, and $U$ is an echelon form of $A$.</p><a href=#pivot><h1 id=pivot><span class=hanchor arialabel=Anchor># </span>pivot</h1></a><p>A nonzero number that either is used in a pivot position to create zeros through row operations or is changed into a leading 1 , which in turn is used to create zeros.</p><a href=#pivot-column><h1 id=pivot-column><span class=hanchor arialabel=Anchor># </span>pivot column</h1></a><p>A column that contains a pivot position.</p><a href=#pivot-position><h1 id=pivot-position><span class=hanchor arialabel=Anchor># </span>pivot position</h1></a><p>A position in a matrix $A$ that corresponds to a leading entry in an echelon form of $A$.</p><a href=#plane-through-mathbfu-mathbfv-and-the-origin><h1 id=plane-through-mathbfu-mathbfv-and-the-origin><span class=hanchor arialabel=Anchor># </span>plane through $\mathbf{u}, \mathbf{v}$, and the origin</h1></a><p>A set whose parametric equation is $\mathbf{x}=s \mathbf{u}+t \mathbf{v}(s, t$ in $\mathbb{R})$, with $\mathbf{u}$ and $\mathbf{v}$ linearly independent.</p><a href=#polar-decomposition-of-a><h1 id=polar-decomposition-of-a><span class=hanchor arialabel=Anchor># </span>polar decomposition (of $A$)</h1></a><p>A factorization $A=P Q$, where $P$ is an $n \times n$ positive semidefinite matrix with the same rank as $A$, and $Q$ is an $n \times n$ orthogonal matrix.</p><a href=#polygon><h1 id=polygon><span class=hanchor arialabel=Anchor># </span>polygon</h1></a><p>A polytope in $\mathbb{R}^{2}$.</p><a href=#polyhedron><h1 id=polyhedron><span class=hanchor arialabel=Anchor># </span>polyhedron</h1></a><p>A polytope in $\mathbb{R}^{3}$.</p><a href=#polytope><h1 id=polytope><span class=hanchor arialabel=Anchor># </span>polytope</h1></a><p>The convex hull of a finite set of points in $\mathbb{R}^{n}$ (a special type of compact convex set).</p><a href=#positive-combination-of-points-mathbfv_1-ldots-mathbfv_m-in-mathbbrn><h1 id=positive-combination-of-points-mathbfv_1-ldots-mathbfv_m-in-mathbbrn><span class=hanchor arialabel=Anchor># </span>positive combination (of points $\mathbf{v}_{1}, \ldots, \mathbf{v}_{m}$ in $\mathbb{R}^{n}$)</h1></a><p>A linear combination $c_{1} \mathbf{v}_{1}+\cdots+c_{m} \mathbf{v}_{m}$, where all $c_{i} \geq 0$.</p><a href=#positive-definite-matrix><h1 id=positive-definite-matrix><span class=hanchor arialabel=Anchor># </span>positive definite matrix</h1></a><p>A symmetric matrix $A$ such that $\mathbf{x}^{T} A \mathbf{x}>0$ for all $\mathbf{x} \neq \mathbf{0}$.</p><a href=#positive-definite-quadratic-form><h1 id=positive-definite-quadratic-form><span class=hanchor arialabel=Anchor># </span>positive definite quadratic form</h1></a><p>A quadratic form $Q$ such that $Q(\mathbf{x})>0$ for all $\mathbf{x} \neq \mathbf{0}$.</p><a href=#positive-hull-of-a-set-s><h1 id=positive-hull-of-a-set-s><span class=hanchor arialabel=Anchor># </span>positive hull (of a set $S$)</h1></a><p>The set of all positive combinations of points in $S$, denoted by pos $S$.</p><a href=#positive-semidefinite-matrix><h1 id=positive-semidefinite-matrix><span class=hanchor arialabel=Anchor># </span>positive semidefinite matrix</h1></a><p>A symmetric matrix $A$ such that $\mathbf{x}^{T} A \mathbf{x} \geq 0$ for all $\mathbf{x}$.</p><a href=#positive-semidefinite-quadratic-form><h1 id=positive-semidefinite-quadratic-form><span class=hanchor arialabel=Anchor># </span>positive semidefinite quadratic form</h1></a><p>A quadratic form $Q$ such that $Q(\mathbf{x}) \geq 0$ for all $\mathbf{x}$.</p><a href=#power-method><h1 id=power-method><span class=hanchor arialabel=Anchor># </span>power method</h1></a><p>An algorithm for estimating a strictly dominant eigenvalue of a square matrix.</p><a href=#principal-axes-of-a-quadratic-form-mathbfxt-a-mathbfx><h1 id=principal-axes-of-a-quadratic-form-mathbfxt-a-mathbfx><span class=hanchor arialabel=Anchor># </span>principal axes (of a quadratic form $\mathbf{x}^{T} A \mathbf{x}$)</h1></a><p>The orthonormal columns of an orthogonal matrix $P$ such that $P^{-1} A P$ is diagonal. (These columns are unit eigenvectors of $A$.) Usually the columns of $P$ are ordered in such a way that the corresponding eigenvalues of $A$ are arranged in decreasing order of magnitude.</p><a href=#principal-components-of-the-data-in-a-matrix-b-of-observations><h1 id=principal-components-of-the-data-in-a-matrix-b-of-observations><span class=hanchor arialabel=Anchor># </span>principal components (of the data in a matrix $B$ of observations)</h1></a><p>The unit eigenvectors of a sample covariance matrix $S$ for $B$, with the eigenvectors arranged so that the corresponding eigenvalues of $S$ decrease in magnitude. If $B$ is in mean-deviation form, then the principal components are the right singular vectors in a singular value decomposition of $B^{T}$.</p><a href=#probability-vector><h1 id=probability-vector><span class=hanchor arialabel=Anchor># </span>probability vector</h1></a><p>A vector in $\mathbb{R}^{n}$ whose entries are nonnegative and sum to one.</p><a href=#product-a-mathbfx><h1 id=product-a-mathbfx><span class=hanchor arialabel=Anchor># </span>product $A \mathbf{x}$</h1></a><p>The linear combination of the columns of $A$ using the corresponding entries in $\mathbf{x}$ as weights.</p><a href=#production-vector><h1 id=production-vector><span class=hanchor arialabel=Anchor># </span>production vector</h1></a><p>The vector in the Leontief input-output model that lists the amounts that are to be produced by the various sectors of an economy.</p><a href=#profile-of-a-set-s-in-mathbbrn><h1 id=profile-of-a-set-s-in-mathbbrn><span class=hanchor arialabel=Anchor># </span>profile (of a set $S$ in $\mathbb{R}^{n}$)</h1></a><p>The set of extreme points of $S$.</p><a href=#projection-matrix-or-orthogonal-projection-matrix><h1 id=projection-matrix-or-orthogonal-projection-matrix><span class=hanchor arialabel=Anchor># </span>projection matrix (or orthogonal projection matrix)</h1></a><p>A symmetric matrix $B$ such that $B^{2}=B$. A simple example is $B=\mathbf{v}^{T}$, where $\mathbf{v}$ is a unit vector.</p><a href=#proper-subset-of-a-set-s><h1 id=proper-subset-of-a-set-s><span class=hanchor arialabel=Anchor># </span>proper subset of a set $S$</h1></a><p>A subset of $S$ that does not equal $S$ itself.</p><a href=#proper-subspace><h1 id=proper-subspace><span class=hanchor arialabel=Anchor># </span>proper subspace</h1></a><p>Any subspace of a vector space $V$ other than $V$ itself.</p><a href=#pseudoinverse-of-a><h1 id=pseudoinverse-of-a><span class=hanchor arialabel=Anchor># </span>pseudoinverse (of $A$)</h1></a><p>The matrix $V D^{-1} U^{T}$, when $U D V^{T}$ is a reduced singular value decomposition of $A$.</p><a href=#qr-factorization><h1 id=qr-factorization><span class=hanchor arialabel=Anchor># </span>QR factorization</h1></a><p>A factorization of an $m \times n$ matrix $A$ with linearly independent columns, $A=Q R$, where $Q$ is an $m \times n$ matrix whose columns form an orthonormal basis for $\operatorname{Col} A$, and $R$ is an $n \times n$ upper triangular invertible matrix with positive entries on its diagonal.</p><a href=#quadratic-bÃ©zier-curve><h1 id=quadratic-bÃ©zier-curve><span class=hanchor arialabel=Anchor># </span>quadratic BÃ©zier curve</h1></a><p>A curve whose description may be written in the form $\mathbf{g}(t)=(1-t) \mathbf{f}_{0}(t)+t \mathbf{f}_{1}(t)$ for $0 \leq t \leq$ 1 , where $\mathbf{f}_{0}(t)=(1-t) \mathbf{p}_{0}+t \mathbf{p}_{1}$ and $\mathbf{f}_{1}(t)=(1-t) \mathbf{p}_{1}+$ $t \mathbf{p}_{2}$. The points $\mathbf{p}_{0}, \mathbf{p}_{1}, \mathbf{p}_{2}$ are called the control points for the curve.</p><a href=#quadratic-form><h1 id=quadratic-form><span class=hanchor arialabel=Anchor># </span>quadratic form</h1></a><p>A function $Q$ defined for $\mathbf{x}$ in $\mathbb{R}^{n}$ by $Q(\mathbf{x})=$ $\mathbf{x}^{T} A \mathbf{x}$, where $A$ is an $n \times n$ symmetric matrix (called the matrix of the quadratic form).</p><a href=#range-of-a-linear-transformation-t><h1 id=range-of-a-linear-transformation-t><span class=hanchor arialabel=Anchor># </span>range (of a linear transformation $T$)</h1></a><p>The set of all vectors of the form $T(\mathbf{x})$ for some $\mathbf{x}$ in the domain of $T$.</p><a href=#rank-of-a-matrix-a><h1 id=rank-of-a-matrix-a><span class=hanchor arialabel=Anchor># </span>rank (of a matrix $A$)</h1></a><p>The dimension of the column space of $A$, denoted by $\operatorname{rank} A$.</p><a href=#rayleigh-quotient><h1 id=rayleigh-quotient><span class=hanchor arialabel=Anchor># </span>Rayleigh quotient</h1></a><p>$R(\mathbf{x})=\left(\mathbf{x}^{T} A \mathbf{x}\right) /\left(\mathbf{x}^{T} \mathbf{x}\right)$. An estimate of an eigenvalue of $A$ (usually a symmetric matrix).</p><a href=#recurrence-relation><h1 id=recurrence-relation><span class=hanchor arialabel=Anchor># </span>recurrence relation</h1></a><p>See difference equation.</p><a href=#reduced-echelon-form-or-reduced-row-echelon-form><h1 id=reduced-echelon-form-or-reduced-row-echelon-form><span class=hanchor arialabel=Anchor># </span>reduced echelon form (or reduced row echelon form)</h1></a><p>A reduced echelon matrix that is row equivalent to a given matrix.</p><a href=#reduced-echelon-matrix><h1 id=reduced-echelon-matrix><span class=hanchor arialabel=Anchor># </span>reduced echelon matrix</h1></a><p>A rectangular matrix in echelon form that has these additional properties: The leading entry in each nonzero row is 1 , and each leading 1 is the only nonzero entry in its column.</p><a href=#reduced-singular-value-decomposition><h1 id=reduced-singular-value-decomposition><span class=hanchor arialabel=Anchor># </span>reduced singular value decomposition</h1></a><p>A factorization $A=U D V^{T}$, for an $m \times n$ matrix $A$ of rank $r$, where $U$ is $m \times r$ with orthonormal columns, $D$ is an $r \times r$ diagonal matrix with the $r$ nonzero singular values of $A$ on its diagonal, and $V$ is $n \times r$ with orthonormal columns.</p><a href=#regression-coefficients><h1 id=regression-coefficients><span class=hanchor arialabel=Anchor># </span>regression coefficients</h1></a><p>The coefficients $\beta_{0}$ and $\beta_{1}$ in the leastsquares line $y=\beta_{0}+\beta_{1} x$.</p><a href=#regular-solid><h1 id=regular-solid><span class=hanchor arialabel=Anchor># </span>regular solid</h1></a><p>One of the five possible regular polyhedrons in $\mathbb{R}^{3}$ : the tetrahedron (4 equal triangular faces), the cube (6 square faces), the octahedron (8 equal triangular faces), the dodecahedron (12 equal pentagonal faces), and the icosahedron (20 equal triangular faces).</p><a href=#regular-stochastic-matrix><h1 id=regular-stochastic-matrix><span class=hanchor arialabel=Anchor># </span>regular stochastic matrix</h1></a><p>A stochastic matrix $P$ such that some matrix power $P^{k}$ contains only strictly positive entries.</p><a href=#relative-change-or-relative-error-in-b><h1 id=relative-change-or-relative-error-in-b><span class=hanchor arialabel=Anchor># </span>relative change or relative error (in b)</h1></a><p>The quantity $|\Delta \mathbf{b}| /|\mathbf{b}|$ when $\mathbf{b}$ is changed to $\mathbf{b}+\Delta \mathbf{b}$.</p><a href=#repellor-of-a-dynamical-system-in-mathbbr2><h1 id=repellor-of-a-dynamical-system-in-mathbbr2><span class=hanchor arialabel=Anchor># </span>repellor (of a dynamical system in $\mathbb{R}^{2}$)</h1></a><p>The origin when all trajectories except the constant zero sequence or function tend away from $\mathbf{0}$.</p><a href=#residual-vector><h1 id=residual-vector><span class=hanchor arialabel=Anchor># </span>residual vector</h1></a><p>The quantity $\epsilon$ that appears in the general linear model: $\mathbf{y}=X \boldsymbol{\beta}+\boldsymbol{\epsilon}$; that is, $\boldsymbol{\epsilon}=\mathbf{y}-X \boldsymbol{\beta}$, the difference between the observed values and the predicted values (of $y$).</p><a href=#operatornamere-mathbfx><h1 id=operatornamere-mathbfx><span class=hanchor arialabel=Anchor># </span>$\operatorname{Re} \mathbf{x}$</h1></a><p>The vector in $\mathbb{R}^{n}$ formed from the real parts of the entries of a vector $\mathbf{x}$ in $\mathbb{C}^{n}$.</p><a href=#right-inverse-of-a><h1 id=right-inverse-of-a><span class=hanchor arialabel=Anchor># </span>right inverse (of $A$)</h1></a><p>Any rectangular matrix $C$ such that $A C=I$</p><a href=#right-multiplication-by-a><h1 id=right-multiplication-by-a><span class=hanchor arialabel=Anchor># </span>right-multiplication (by $A$)</h1></a><p>Multiplication of a matrix on the right by $A$.</p><a href=#right-singular-vectors-of-a><h1 id=right-singular-vectors-of-a><span class=hanchor arialabel=Anchor># </span>right singular vectors (of $A$)</h1></a><p>The columns of $V$ in the singular value decomposition $A=U \Sigma V^{T}$.</p><a href=#roundoff-error><h1 id=roundoff-error><span class=hanchor arialabel=Anchor># </span>roundoff error</h1></a><p>Error in floating point arithmetic caused when the result of a calculation is rounded (or truncated) to the number of floating point digits stored. Also, the error that results when the decimal representation of a number such as $1 / 3$ is approximated by a floating point number with a finite number of digits.</p><a href=#row-column-rule><h1 id=row-column-rule><span class=hanchor arialabel=Anchor># </span>row-column rule</h1></a><p>The rule for computing a product $A B$ in which the $(i, j)$-entry of $A B$ is the sum of the products of corresponding entries from row $i$ of $A$ and column $j$ of $B$.</p><a href=#row-equivalent-matrices><h1 id=row-equivalent-matrices><span class=hanchor arialabel=Anchor># </span>row equivalent (matrices)</h1></a><p>Two matrices for which there exists a (finite) sequence of row operations that transforms one matrix into the other.</p><a href=#row-reduction-algorithm><h1 id=row-reduction-algorithm><span class=hanchor arialabel=Anchor># </span>row reduction algorithm</h1></a><p>A systematic method using elementary row operations that reduces a matrix to echelon form or reduced echelon form. row replacement: An elementary row operation that replaces one row of a matrix by the sum of the row and a multiple of another row.</p><a href=#row-space-of-a-matrix-a><h1 id=row-space-of-a-matrix-a><span class=hanchor arialabel=Anchor># </span>row space (of a matrix $A$)</h1></a><p>The set Row $A$ of all linear combinations of the vectors formed from the rows of $A$; also denoted by $\operatorname{Col} A^{T}$.</p><a href=#row-sum><h1 id=row-sum><span class=hanchor arialabel=Anchor># </span>row sum</h1></a><p>The sum of the entries in a row of a matrix.</p><a href=#row-vector><h1 id=row-vector><span class=hanchor arialabel=Anchor># </span>row vector</h1></a><p>A matrix with only one row, or a single row of a matrix that has several rows.</p><a href=#row-vector-rule-for-computing-boldsymbola-mathbfx><h1 id=row-vector-rule-for-computing-boldsymbola-mathbfx><span class=hanchor arialabel=Anchor># </span>row-vector rule for computing $\boldsymbol{A} \mathbf{x}$</h1></a><p>The rule for computing a product $A \mathbf{x}$ in which the $i$ th entry of $A \mathbf{x}$ is the sum of the products of corresponding entries from row $i$ of $A$ and from the vector $\mathbf{x}$.</p><a href=#saddle-point-of-a-dynamical-system-in-mathbbr2><h1 id=saddle-point-of-a-dynamical-system-in-mathbbr2><span class=hanchor arialabel=Anchor># </span>saddle point (of a dynamical system in $\mathbb{R}^{2}$)</h1></a><p>The origin when some trajectories are attracted to $\mathbf{0}$ and other trajectories are repelled from $\mathbf{0}$.</p><a href=#same-direction-as-a-vector-mathbfv><h1 id=same-direction-as-a-vector-mathbfv><span class=hanchor arialabel=Anchor># </span>same direction (as a vector $\mathbf{v}$)</h1></a><p>A vector that is a positive multiple of $\mathbf{v}$.</p><a href=#sample-mean><h1 id=sample-mean><span class=hanchor arialabel=Anchor># </span>sample mean</h1></a><p>The average $M$ of a set of vectors, $\mathbf{X}_{1}, \ldots, \mathbf{X}_{N}$, given by $M=(1 / N)\left(\mathbf{X}_{1}+\cdots+\mathbf{X}_{N}\right)$.</p><a href=#scalar><h1 id=scalar><span class=hanchor arialabel=Anchor># </span>scalar</h1></a><p>A (real) number used to multiply either a vector or a matrix.</p><a href=#scalar-multiple-of-mathbfu-by-boldsymbolc><h1 id=scalar-multiple-of-mathbfu-by-boldsymbolc><span class=hanchor arialabel=Anchor># </span>scalar multiple of $\mathbf{u}$ by $\boldsymbol{c}$</h1></a><p>The vector $c \mathbf{u}$ obtained by multiplying each entry in $\mathbf{u}$ by $c$.</p><a href=#scale-a-vector><h1 id=scale-a-vector><span class=hanchor arialabel=Anchor># </span>scale (a vector)</h1></a><p>Multiply a vector (or a row or column of a matrix) by a nonzero scalar.</p><a href=#schur-complement><h1 id=schur-complement><span class=hanchor arialabel=Anchor># </span>Schur complement</h1></a><p>A certain matrix formed from the blocks of a $2 \times 2$ partitioned matrix $A=\left[A_{i j}\right]$. If $A_{11}$ is invertible, its Schur complement is given by $A_{22}-A_{21} A_{11}^{-1} A_{12}$. If $A_{22}$ is invertible, its Schur complement is given by $A_{11}-A_{12} A_{22}^{-1} A_{21}$.</p><a href=#schur-factorization-of-a-for-real-scalars><h1 id=schur-factorization-of-a-for-real-scalars><span class=hanchor arialabel=Anchor># </span>Schur factorization (of $A$, for real scalars)</h1></a><p>A factorization $A=U R U^{T}$ of an $n \times n$ matrix $A$ having $n$ real eigenvalues, where $U$ is an $n \times n$ orthogonal matrix and $R$ is an upper triangular matrix.</p><a href=#set-spanned-by-leftmathbfv_1-ldots-mathbfv_boldsymbolpright><h1 id=set-spanned-by-leftmathbfv_1-ldots-mathbfv_boldsymbolpright><span class=hanchor arialabel=Anchor># </span>set spanned by $\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{\boldsymbol{p}}\right}$</h1></a><p>The set $\operatorname{Span}\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$.</p><a href=#signal-or-discrete-time-signal><h1 id=signal-or-discrete-time-signal><span class=hanchor arialabel=Anchor># </span>signal (or discrete-time signal)</h1></a><p>A doubly infinite sequence of numbers, $\left{y_{k}\right}$; a function defined on the integers; belongs to the vector space $\mathbb{S}$.</p><a href=#similar-matrices><h1 id=similar-matrices><span class=hanchor arialabel=Anchor># </span>similar (matrices)</h1></a><p>Matrices $A$ and $B$ such that $P^{-1} A P=B$, or equivalently, $A=P B P^{-1}$, for some invertible matrix $P$.</p><a href=#similarity-transformation><h1 id=similarity-transformation><span class=hanchor arialabel=Anchor># </span>similarity transformation</h1></a><p>A transformation that changes $A$ into $P^{-1} A P$.</p><a href=#simplex><h1 id=simplex><span class=hanchor arialabel=Anchor># </span>simplex</h1></a><p>The convex hull of an affinely independent finite set of vectors in $\mathbb{R}^{n}$.</p><a href=#singular-matrix><h1 id=singular-matrix><span class=hanchor arialabel=Anchor># </span>singular (matrix)</h1></a><p>A square matrix that has no inverse.</p><a href=#singular-value-decomposition-of-an-m-times-n-matrix-a><h1 id=singular-value-decomposition-of-an-m-times-n-matrix-a><span class=hanchor arialabel=Anchor># </span>singular value decomposition (of an $m \times n$ matrix $A$)</h1></a><p>$A=$ $U \Sigma V^{T}$, where $U$ is an $m \times m$ orthogonal matrix, $V$ is an $n \times n$ orthogonal matrix, and $\Sigma$ is an $m \times n$ matrix with nonnegative entries on the main diagonal (arranged in decreasing order of magnitude) and zeros elsewhere. If $\operatorname{rank} A=r$, then $\Sigma$ has exactly $r$ positive entries (the nonzero singular values of $A$) on the diagonal.</p><a href=#singular-values-of-a><h1 id=singular-values-of-a><span class=hanchor arialabel=Anchor># </span>singular values (of $A$)</h1></a><p>The (positive) square roots of the eigenvalues of $A^{T} A$, arranged in decreasing order of magnitude.</p><a href=#size-of-a-matrix><h1 id=size-of-a-matrix><span class=hanchor arialabel=Anchor># </span>size (of a matrix)</h1></a><p>Two numbers, written in the form $m \times n$, that specify the number of rows $(m)$ and columns $(n)$ in the matrix.</p><a href=#solution-of-a-linear-system-involving-variables-x_1-ldots-x_n><h1 id=solution-of-a-linear-system-involving-variables-x_1-ldots-x_n><span class=hanchor arialabel=Anchor># </span>solution (of a linear system involving variables $x_{1}, \ldots, x_{n}$)</h1></a><p>A list $\left(s_{1}, s_{2}, \ldots, s_{n}\right)$ of numbers that makes each equation in the system a true statement when the values $s_{1}, \ldots, s_{n}$ are substituted for $x_{1}, \ldots, x_{n}$, respectively.</p><a href=#solution-set><h1 id=solution-set><span class=hanchor arialabel=Anchor># </span>solution set</h1></a><p>The set of all possible solutions of a linear system. The solution set is empty when the linear system is inconsistent.</p><a href=#operatornamespanleftmathbfv_1-ldots-mathbfv_boldsymbolpright><h1 id=operatornamespanleftmathbfv_1-ldots-mathbfv_boldsymbolpright><span class=hanchor arialabel=Anchor># </span>$\operatorname{Span}\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{\boldsymbol{p}}\right}$</h1></a><p>The set of all linear combinations of $\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}$. Also, the subspace spanned (or generated) by $\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}$.</p><a href=#spanning-set-for-a-subspace-h><h1 id=spanning-set-for-a-subspace-h><span class=hanchor arialabel=Anchor># </span>spanning set (for a subspace $H$)</h1></a><p>Any set $\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$ in $H$ such that $H=\operatorname{Span}\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$.</p><a href=#spectral-decomposition-of-a><h1 id=spectral-decomposition-of-a><span class=hanchor arialabel=Anchor># </span>spectral decomposition (of $A$)</h1></a><p>A representation</p><p>$$
A=\lambda_{1} \mathbf{u}_{1} \mathbf{u}_{1}^{T}+\cdots+\lambda_{n} \mathbf{u}_{n} \mathbf{u}_{n}^{T}
$$</p><p>where $\left{\mathbf{u}_{1}, \ldots, \mathbf{u}_{n}\right}$ is an orthonormal basis of eigenvectors of $A$, and $\lambda_{1}, \ldots, \lambda_{n}$ are the corresponding eigenvalues of $A$.</p><a href=#spiral-point-of-a-dynamical-system-in-mathbbr2><h1 id=spiral-point-of-a-dynamical-system-in-mathbbr2><span class=hanchor arialabel=Anchor># </span>spiral point (of a dynamical system in $\mathbb{R}^{2}$)</h1></a><p>The origin when the trajectories spiral about $\mathbf{0}$.</p><a href=#stage-matrix-model><h1 id=stage-matrix-model><span class=hanchor arialabel=Anchor># </span>stage-matrix model</h1></a><p>A difference equation $\mathbf{x}_{k+1}=A \mathbf{x}_{k}$ where $\mathbf{x}_{k}$ lists the number of females in a population at time $k$, with the females classified by various stages of development (such as juvenile, subadult, and adult).</p><a href=#standard-basis><h1 id=standard-basis><span class=hanchor arialabel=Anchor># </span>standard basis</h1></a><p>The basis $\mathcal{E}=\left{\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right}$ for $\mathbb{R}^{n}$ consisting of the columns of the $n \times n$ identity matrix, or the basis $\left{1, t, \ldots, t^{n}\right}$ for $\mathbb{P}_{n}$.</p><a href=#standard-matrix-for-a-linear-transformation-t><h1 id=standard-matrix-for-a-linear-transformation-t><span class=hanchor arialabel=Anchor># </span>standard matrix (for a linear transformation $T$)</h1></a><p>The matrix $A$ such that $T(\mathbf{x})=A \mathbf{x}$ for all $\mathbf{x}$ in the domain of $T$.</p><a href=#standard-position><h1 id=standard-position><span class=hanchor arialabel=Anchor># </span>standard position</h1></a><p>The position of the graph of an equation $\mathbf{x}^{T} A \mathbf{x}=c$, when $A$ is a diagonal matrix.</p><a href=#state-vector><h1 id=state-vector><span class=hanchor arialabel=Anchor># </span>state vector</h1></a><p>A probability vector. In general, a vector that describes the &ldquo;state&rdquo; of a physical system, often in connection with a difference equation $\mathbf{x}_{k+1}=A \mathbf{x}_{k}$.</p><a href=#steady-state-vector-for-a-stochastic-matrix-p><h1 id=steady-state-vector-for-a-stochastic-matrix-p><span class=hanchor arialabel=Anchor># </span>steady-state vector (for a stochastic matrix $P$)</h1></a><p>A probability vector $\mathbf{q}$ such that $P \mathbf{q}=\mathbf{q}$.</p><a href=#stiffness-matrix><h1 id=stiffness-matrix><span class=hanchor arialabel=Anchor># </span>stiffness matrix</h1></a><p>The inverse of a flexibility matrix. The $j$ th column of a stiffness matrix gives the loads that must be applied at specified points on an elastic beam in order to produce a unit deflection at the $j$ th point on the beam.</p><a href=#stochastic-matrix><h1 id=stochastic-matrix><span class=hanchor arialabel=Anchor># </span>stochastic matrix</h1></a><p>A square matrix whose columns are probability vectors.</p><p>strictly dominant eigenvalue: An eigenvalue $\lambda_{1}$ of a matrix $A$ with the property that $\left|\lambda_{1}\right|>\left|\lambda_{k}\right|$ for all other eigenvalues $\lambda_{k}$ of $A$. submatrix (of $A$): Any matrix obtained by deleting some rows and/or columns of $A$; also, $A$ itself.</p><a href=#subspace><h1 id=subspace><span class=hanchor arialabel=Anchor># </span>subspace</h1></a><p>A subset $H$ of some vector space $V$ such that $H$ has these properties: (1) the zero vector of $V$ is in $H$; (2) $H$ is closed under vector addition; and (3) $H$ is closed under multiplication by scalars.</p><a href=#supporting-hyperplane-to-a-compact-convex-set-s-in-mathbbrn><h1 id=supporting-hyperplane-to-a-compact-convex-set-s-in-mathbbrn><span class=hanchor arialabel=Anchor># </span>supporting hyperplane (to a compact convex set $S$ in $\mathbb{R}^{n}$)</h1></a><p>A hyperplane $H=[f: d]$ such that $H \cap S \neq \varnothing$ and either $f(x) \leq d$ for all $x$ in $S$ or $f(x) \geq d$ for all $x$ in $S$.</p><a href=#symmetric-matrix><h1 id=symmetric-matrix><span class=hanchor arialabel=Anchor># </span>symmetric matrix</h1></a><p>A matrix $A$ such that $A^{T}=A$.</p><a href=#system-of-linear-equations-or-a-linear-system><h1 id=system-of-linear-equations-or-a-linear-system><span class=hanchor arialabel=Anchor># </span>system of linear equations (or a linear system)</h1></a><p>A collection of one or more linear equations involving the same set of variables, say, $x_{1}, \ldots, x_{n}$.</p><a href=#tetrahedron><h1 id=tetrahedron><span class=hanchor arialabel=Anchor># </span>tetrahedron</h1></a><p>A three-dimensional solid object bounded by four equal triangular faces, with three faces meeting at each vertex.</p><a href=#total-variance><h1 id=total-variance><span class=hanchor arialabel=Anchor># </span>total variance</h1></a><p>The trace of the covariance matrix $S$ of a matrix of observations.</p><a href=#trace-of-a-square-matrix-a><h1 id=trace-of-a-square-matrix-a><span class=hanchor arialabel=Anchor># </span>trace (of a square matrix $A$)</h1></a><p>The sum of the diagonal entries in $A$, denoted by $\operatorname{tr} A$.</p><a href=#trajectory><h1 id=trajectory><span class=hanchor arialabel=Anchor># </span>trajectory</h1></a><p>The graph of a solution $\left{\mathbf{x}_{0}, \mathbf{x}_{1}, \mathbf{x}_{2}, \ldots\right}$ of a dynamical system $\mathbf{x}_{k+1}=A \mathbf{x}_{k}$, often connected by a thin curve to make the trajectory easier to see. Also, the graph of $\mathbf{x}(t)$ for $t \geq 0$, when $\mathbf{x}(t)$ is a solution of a differential equation $\mathbf{x}^{\prime}(t)=A \mathbf{x}(t)$</p><a href=#transfer-matrix><h1 id=transfer-matrix><span class=hanchor arialabel=Anchor># </span>transfer matrix</h1></a><p>A matrix $A$ associated with an electrical circuit having input and output terminals, such that the output vector is $A$ times the input vector.</p><a href=#transformation-or-function-or-mapping-t-from-mathbbrn-to-mathbbrboldsymbolm><h1 id=transformation-or-function-or-mapping-t-from-mathbbrn-to-mathbbrboldsymbolm><span class=hanchor arialabel=Anchor># </span>transformation (or function, or mapping) $T$ from $\mathbb{R}^{n}$ to $\mathbb{R}^{\boldsymbol{m}}</h1></a><p>$ A rule that assigns to each vector $\mathbf{x}$ in $\mathbb{R}^{n}$ a unique vector $T(\mathbf{x})$ in $\mathbb{R}^{m}$. Notation: $T: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$. Also, $T: V \rightarrow W$ denotes a rule that assigns to each $\mathbf{x}$ in $V$ a unique vector $T(\mathbf{x})$ in $W$.</p><a href=#translation-by-a-vector-mathbfp><h1 id=translation-by-a-vector-mathbfp><span class=hanchor arialabel=Anchor># </span>translation (by a vector $\mathbf{p}$)</h1></a><p>The operation of adding $\mathbf{p}$ to a vector or to each vector in a given set.</p><a href=#transpose-of-a><h1 id=transpose-of-a><span class=hanchor arialabel=Anchor># </span>transpose (of $A$)</h1></a><p>An $n \times m$ matrix $A^{T}$ whose columns are the corresponding rows of the $m \times n$ matrix $A$.</p><a href=#trend-analysis><h1 id=trend-analysis><span class=hanchor arialabel=Anchor># </span>trend analysis</h1></a><p>The use of orthogonal polynomials to fit data, with the inner product given by evaluation at a finite set of points.</p><a href=#triangle-inequality><h1 id=triangle-inequality><span class=hanchor arialabel=Anchor># </span>triangle inequality</h1></a><p>$|\mathbf{u}+\mathbf{v}| \leq|\mathbf{u}|+|\mathbf{v}|$ for all $\mathbf{u}, \mathbf{v}$.</p><a href=#triangular-matrix><h1 id=triangular-matrix><span class=hanchor arialabel=Anchor># </span>triangular matrix</h1></a><p>A matrix $A$ with either zeros above or zeros below the diagonal entries.</p><a href=#trigonometric-polynomial><h1 id=trigonometric-polynomial><span class=hanchor arialabel=Anchor># </span>trigonometric polynomial</h1></a><p>A linear combination of the constant function 1 and sine and cosine functions such as $\cos n t$ and $\sin n t$.</p><a href=#trivial-solution><h1 id=trivial-solution><span class=hanchor arialabel=Anchor># </span>trivial solution</h1></a><p>The solution $\mathbf{x}=\mathbf{0}$ of a homogeneous equation $A \mathbf{x}=\mathbf{0}$.</p><a href=#uncorrelated-variables><h1 id=uncorrelated-variables><span class=hanchor arialabel=Anchor># </span>uncorrelated variables</h1></a><p>Any two variables $x_{i}$ and $x_{j}$ (with $i \neq j$) that range over the $i$ th and $j$ th coordinates of the observation vectors in an observation matrix, such that the covariance $s_{i j}$ is zero.</p><a href=#underdetermined-system><h1 id=underdetermined-system><span class=hanchor arialabel=Anchor># </span>underdetermined system</h1></a><p>A system of equations with fewer equations than unknowns.</p><a href=#uniqueness-question><h1 id=uniqueness-question><span class=hanchor arialabel=Anchor># </span>uniqueness question</h1></a><p>Asks, &ldquo;If a solution of a system exists, is it unique-that is, is it the only one?&rdquo;</p><a href=#unit-consumption-vector><h1 id=unit-consumption-vector><span class=hanchor arialabel=Anchor># </span>unit consumption vector</h1></a><p>A column vector in the Leontief input-output model that lists the inputs a sector needs for each unit of its output; a column of the consumption matrix.</p><a href=#unit-lower-triangular-matrix><h1 id=unit-lower-triangular-matrix><span class=hanchor arialabel=Anchor># </span>unit lower triangular matrix</h1></a><p>A square lower triangular matrix with ones on the main diagonal.</p><a href=#unit-vector><h1 id=unit-vector><span class=hanchor arialabel=Anchor># </span>unit vector</h1></a><p>A vector $\mathbf{v}$ such that $|\mathbf{v}|=1$.</p><a href=#upper-triangular-matrix><h1 id=upper-triangular-matrix><span class=hanchor arialabel=Anchor># </span>upper triangular matrix</h1></a><p>A matrix $U$ (not necessarily square) with zeros below the diagonal entries $u_{11}, u_{22}, \ldots$</p><a href=#vandermonde-matrix><h1 id=vandermonde-matrix><span class=hanchor arialabel=Anchor># </span>Vandermonde matrix</h1></a><p>An $n \times n$ matrix $V$ or its transpose, when $V$ has the form</p><p>$$
V=\left[\begin{array}{ccccc}
1 & x_{1} & x_{1}^{2} & \cdots & x_{1}^{n-1} \\ 1 & x_{2} & x_{2}^{2} & \cdots & x_{2}^{n-1} \\ \vdots & \vdots & \vdots & & \vdots \\ 1 & x_{n} & x_{n}^{2} & \cdots & x_{n}^{n-1}
\end{array}\right]
$$</p><a href=#variance-of-a-variable-x_j><h1 id=variance-of-a-variable-x_j><span class=hanchor arialabel=Anchor># </span>variance (of a variable $x_{j}$)</h1></a><p>The diagonal entry $s_{j j}$ in the covariance matrix $S$ for a matrix of observations, where $x_{j}$ varies over the $j$ th coordinates of the observation vectors.</p><a href=#vector><h1 id=vector><span class=hanchor arialabel=Anchor># </span>vector</h1></a><p>A list of numbers; a matrix with only one column. In general, any element of a vector space.</p><a href=#vector-addition><h1 id=vector-addition><span class=hanchor arialabel=Anchor># </span>vector addition</h1></a><p>Adding vectors by adding corresponding entries.</p><a href=#vector-equation><h1 id=vector-equation><span class=hanchor arialabel=Anchor># </span>vector equation</h1></a><p>An equation involving a linear combination of vectors with undetermined weights.</p><a href=#vector-space><h1 id=vector-space><span class=hanchor arialabel=Anchor># </span>vector space</h1></a><p>A set of objects, called vectors, on which two operations are defined, called addition and multiplication by scalars. Ten axioms must be satisfied. See the first definition in Section 4.1.</p><a href=#vector-subtraction><h1 id=vector-subtraction><span class=hanchor arialabel=Anchor># </span>vector subtraction</h1></a><p>Computing $\mathbf{u}+(-1) \mathbf{v}$ and writing the result as $\mathbf{u}-\mathbf{v}$.</p><a href=#weighted-least-squares><h1 id=weighted-least-squares><span class=hanchor arialabel=Anchor># </span>weighted least squares</h1></a><p>Least-squares problems with a weighted inner product such as</p><p>$$
\langle\mathbf{x}, \mathbf{y}\rangle=w_{1}^{2} x_{1} y_{1}+\cdots+w_{n}^{2} x_{n} y_{n} .
$$</p><a href=#weights><h1 id=weights><span class=hanchor arialabel=Anchor># </span>weights</h1></a><p>The scalars used in a linear combination.</p><a href=#zero-subspace><h1 id=zero-subspace><span class=hanchor arialabel=Anchor># </span>zero subspace</h1></a><p>The subspace ${\boldsymbol{0}}$ consisting of only the zero vector.</p><a href=#zero-vector><h1 id=zero-vector><span class=hanchor arialabel=Anchor># </span>zero vector</h1></a><p>The unique vector, denoted by $\mathbf{0}$, such that $\mathbf{u}+\mathbf{0}=\mathbf{u}$ for all $\mathbf{u}$. In $\mathbb{R}^{n}, \mathbf{0}$ is the vector whose entries are all zeros.</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/quartz// data-ctx=glossary data-src=/ class=internal-link>Linear Algebra Concept Map</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://yanjinhai.github.io/quartz/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Jinhai Yan using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><ul><li><a href=https://yanjinhai.github.io/quartz/>Home</a></li><li><a href=https://github.com/yanjinhai>Github</a></li></ul></footer></div></div></body></html>