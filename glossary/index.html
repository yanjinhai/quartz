<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="adjugate (or classical adjoint): The matrix adj $A$ formed from a square matrix $A$ by replacing the $(i, j)$-entry of $A$ by the $(i, j)$-cofactor, for all $i$ and $j$, and then transposing the resulting matrix."><title>ðŸª´ Quartz 3.3</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://yanjinhai.github.io/quartz//icon.png><link href=https://yanjinhai.github.io/quartz/styles.706bb6073ba85d26809f9096dae23a6b.min.css rel=stylesheet><link href=https://yanjinhai.github.io/quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://yanjinhai.github.io/quartz/js/darkmode.f8a7799ddfa4bf9eced5584f1780e8b3.min.js></script>
<script src=https://yanjinhai.github.io/quartz/js/util.5e39932758f9ecaf45fd54506ed61416.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://yanjinhai.github.io/quartz/js/popover.6da9b273c092cc16fc1aa904d71a2163.min.js></script>
<script src=https://yanjinhai.github.io/quartz/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://yanjinhai.github.io/quartz/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://yanjinhai.github.io/quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://yanjinhai.github.io/quartz/",fetchData=Promise.all([fetch("https://yanjinhai.github.io/quartz/indices/linkIndex.e6bc758255cec383b3768a102dd43d89.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://yanjinhai.github.io/quartz/indices/contentIndex.60bf4e658656f69504fed2379ed9b6f7.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://yanjinhai.github.io/quartz",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://yanjinhai.github.io/quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/yanjinhai.github.io\/quartz\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://yanjinhai.github.io/quartz/js/full-text-search.51f0b1753e9b30839d053f8a98cc20d1.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://yanjinhai.github.io/quartz/>ðŸª´ Quartz 3.3</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Unknown
<a href=https://github.com/yanjinhai/quartz/tree/hugo/content/glossary.md rel=noopener>Edit Source</a></p><ul class=tags></ul><p>adjugate (or classical adjoint): The matrix adj $A$ formed from a square matrix $A$ by replacing the $(i, j)$-entry of $A$ by the $(i, j)$-cofactor, for all $i$ and $j$, and then transposing the resulting matrix.</p><p>affine combination: A linear combination of vectors (points in $\mathbb{R}^{n}$) in which the sum of the weights involved is 1 .</p><p>affine dependence relation: An equation of the form $c_{1} \mathbf{v}_{1}+$ $\cdots+c_{p} \mathbf{v}_{p}=\mathbf{0}$, where the weights $c_{1}, \ldots, c_{p}$ are not all zero, and $c_{1}+\cdots+c_{p}=0$.</p><p>affine hull (or affine span) of a set $S$ : The set of all affine combinations of points in $S$, denoted by aff $S$.</p><p>affinely dependent set: A set $\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$ in $\mathbb{R}^{n}$ such that there are real numbers $c_{1}, \ldots, c_{p}$, not all zero, such that $c_{1}+\cdots+$ $c_{p}=0$ and $c_{1} \mathbf{v}_{1}+\cdots+c_{p} \mathbf{v}_{p}=\mathbf{0}$.</p><p>affinely independent set: A set $\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$ in $\mathbb{R}^{n}$ that is not affinely dependent.</p><p>affine set (or affine subset): A set $S$ of points such that if $\mathbf{p}$ and $\mathbf{q}$ are in $S$, then $(1-t) \mathbf{p}+t \mathbf{q} \in S$ for each real number $t$.</p><p>affine transformation: A mapping $T: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ of the form $T(\mathbf{x})=A \mathbf{x}+\mathbf{b}$, with $A$ an $m \times n$ matrix and $\mathbf{b}$ in $\mathbb{R}^{m}$.</p><p>algebraic multiplicity: The multiplicity of an eigenvalue as a root of the characteristic equation.</p><p>angle (between nonzero vectors $\mathbf{u}$ and $\mathbf{v}$ in $\mathbb{R}^{2}$ or $\mathbb{R}^{3}$ ): The angle $\vartheta$ between the two directed line segments from the origin to the points $\mathbf{u}$ and $\mathbf{v}$. Related to the scalar product by</p><p>$$
\mathbf{u} \cdot \mathbf{v}=|\mathbf{u}||\mathbf{v}| \cos \vartheta
$$</p><p>associative law of multiplication: $\quad A(B C)=(A B) C$, for all $A$, $B, C$.</p><p>attractor (of a dynamical system in $\mathbb{R}^{2}$ ): The origin when all trajectories tend toward $\mathbf{0}$.</p><p>augmented matrix: A matrix made up of a coefficient matrix for a linear system and one or more columns to the right. Each extra column contains the constants from the right side of a system with the given coefficient matrix.</p><p>auxiliary equation: A polynomial equation in a variable $r$, created from the coefficients of a homogeneous difference equation.</p><p>back-substitution (with matrix notation): The backward phase of row reduction of an augmented matrix that transforms an echelon matrix into a reduced echelon matrix; used to find the solution(s) of a system of linear equations.</p><p>backward phase (of row reduction): The last part of the algorithm that reduces a matrix in echelon form to a reduced echelon form.</p><p>band matrix: A matrix whose nonzero entries lie within a band along the main diagonal.</p><p>barycentric coordinates (of a point $\mathbf{p}$ with respect to an affinely independent set $S=\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{k}\right}$ ): The (unique) set of weights $c_{1}, \ldots, c_{k}$ such that $\mathbf{p}=c_{1} \mathbf{v}_{1}+\cdots+c_{k} \mathbf{v}_{k}$ and $c_{1}+$ $\cdots+c_{k}=1$. (Sometimes also called the affine coordinates of $\mathbf{p}$ with respect to $S$.)</p><p>basic variable: A variable in a linear system that corresponds to a pivot column in the coefficient matrix.</p><p>basis (for a nontrivial subspace $H$ of a vector space $V$ ): An indexed set $\mathcal{B}=\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$ in $V$ such that: (i) $\mathcal{B}$ is a linearly independent set and (ii) the subspace spanned by $\mathcal{B}$ coincides with $H$, that is, $H=\operatorname{Span}\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$.</p><p>$\mathcal{B}$-coordinates of $\mathbf{x}$ : See coordinates of $\mathbf{x}$ relative to the basis $\mathcal{B}$.</p><p>best approximation: The closest point in a given subspace to a given vector.</p><p>bidiagonal matrix: A matrix whose nonzero entries lie on the main diagonal and on one diagonal adjacent to the main diagonal.</p><p>block diagonal (matrix): A partitioned matrix $A=\left[A_{i j}\right]$ such that each block $A_{i j}$ is a zero matrix for $i \neq j$.</p><p>block matrix: See partitioned matrix.</p><p>block matrix multiplication: The row-column multiplication of partitioned matrices as if the block entries were scalars. block upper triangular (matrix): A partitioned matrix $A=\left[A_{i j}\right]$ such that each block $A_{i j}$ is a zero matrix for $i>j$.</p><p>boundary point of a set $S$ in $\mathbb{R}^{n}$ : A point $\mathbf{p}$ such that every open ball in $\mathbb{R}^{n}$ centered at $\mathbf{p}$ intersects both $S$ and the complement of $S$.</p><p>bounded set in $\mathbb{R}^{n}$ : A set that is contained in an open ball $B(\mathbf{0}, \delta)$ for some $\delta>0$.</p><p>$\mathcal{B}$-matrix (for $T$ ): A matrix $[T]<em>{\mathcal{B}}$ for a linear transformation $T: V \rightarrow V$ relative to a basis $\mathcal{B}$ for $V$, with the property that $[T(\mathbf{x})]</em>{\mathcal{B}}=[T]<em>{\mathcal{B}}[\mathbf{x}]</em>{\mathcal{B}}$ for all $\mathbf{x}$ in $V$.</p><p>Cauchy-Schwarz inequality: $|\langle\mathbf{u}, \mathbf{v}\rangle| \leq|u| \cdot|v|$ for all u, $\mathbf{v}$. change of basis: See change-of-coordinates matrix.</p><p>change-of-coordinates matrix (from a basis $\mathcal{B}$ to a basis $\mathcal{C}$ ): A matrix $\underset{\mathcal{C} \leftarrow \mathcal{B}}{P}$ that transforms $\mathcal{B}$-coordinate vectors into $\mathcal{C}$ coordinate vectors: $[\mathbf{x}]<em>{\mathcal{C}}={ }_{\mathcal{C} \leftarrow \mathcal{B}}^{P}[\mathbf{x}]</em>{\mathcal{B}}$. If $\mathcal{C}$ is the standard basis for $\mathbb{R}^{n}$, then ${ }_{\mathcal{C} \leftarrow \mathcal{B}}$ is sometimes written as $P_{\mathcal{B}}$.</p><p>characteristic equation (of $A$ ): $\quad \operatorname{det}(A-\lambda I)=0$.</p><p>characteristic polynomial (of $A$ ): $\operatorname{det}(A-\lambda I$ ) or, in some texts, $\operatorname{det}(\lambda I-A)$.</p><p>Cholesky factorization: A factorization $A=R^{T} R$, where $R$ is an invertible upper triangular matrix whose diagonal entries are all positive.</p><p>closed ball (in $\mathbb{R}^{n}$ ): A set ${\mathbf{x}:|\mathbf{x}-\mathbf{p}|&lt;\delta}$ in $\mathbb{R}^{n}$, where $\mathbf{p}$ is in $\mathbb{R}^{n}$ and $\delta>0$.</p><p>closed set (in $\mathbb{R}^{n}$ ): A set that contains all of its boundary points. codomain (of a transformation $T: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ ): The set $\mathbb{R}^{m}$ that contains the range of $T$. In general, if $T$ maps a vector space $V$ into a vector space $W$, then $W$ is called the codomain of $T$.</p><p>coefficient matrix: A matrix whose entries are the coefficients of a system of linear equations.</p><p>cofactor: A number $C_{i j}=(-1)^{i+j} \operatorname{det} A_{i j}$, called the $(i, j)$ cofactor of $A$, where $A_{i j}$ is the submatrix formed by deleting the $i$ th row and the $j$ th column of $A$.</p><p>cofactor expansion: A formula for $\operatorname{det} A$ using cofactors associated with one row or one column, such as for row 1:</p><p>$$
\operatorname{det} A=a_{11} C_{11}+\cdots+a_{1 n} C_{1 n}
$$</p><p>column-row expansion: The expression of a product $A B$ as a sum of outer products: $\operatorname{col}_{1}(A) \operatorname{row}_{1}(B)+\cdots+$ $\operatorname{col}_{n}(A)$ row $_{n}(B)$, where $n$ is the number of columns of $A$.</p><p>column space (of an $m \times n$ matrix $A$ ): The set $\operatorname{Col} A$ of all linear combinations of the columns of $A$. If $A=\left[\mathbf{a}_{1} \cdots \mathbf{a}_{n}\right]$, then $\operatorname{Col} A=\operatorname{Span}\left{\mathbf{a}_{1}, \ldots, \mathbf{a}_{n}\right}$. Equivalently,</p><p>$$
\operatorname{Col} A=\left{\mathbf{y}: \mathbf{y}=A \mathbf{x} \text { for some } \mathbf{x} \text { in } \mathbb{R}^{n}\right}
$$</p><p>column sum: The sum of the entries in a column of a matrix. column vector: A matrix with only one column, or a single column of a matrix that has several columns.</p><p>commuting matrices: Two matrices $A$ and $B$ such that $A B=B A$.</p><p>compact set (in $\mathbb{R}^{n}$ ): A set in $\mathbb{R}^{n}$ that is both closed and bounded.</p><p>companion matrix: A special form of matrix whose characteristic polynomial is $(-1)^{n} p(\lambda)$ when $p(\lambda)$ is a specified polynomial whose leading term is $\lambda^{n}$.</p><p>complex eigenvalue: A nonreal root of the characteristic equation of an $n \times n$ matrix.</p><p>complex eigenvector: A nonzero vector $\mathbf{x}$ in $\mathbb{C}^{n}$ such that $A \mathbf{x}=\lambda \mathbf{x}$, where $A$ is an $n \times n$ matrix and $\lambda$ is a complex eigenvalue.</p><p>component of $\mathbf{y}$ orthogonal to $\mathbf{u}$ (for $\mathbf{u} \neq \mathbf{0}$ ): The vector $\mathbf{y}-\frac{\mathbf{y} \cdot \mathbf{u}}{\mathbf{u} \cdot \mathbf{u}} \mathbf{u}$.</p><p>composition of linear transformations: A mapping produced by applying two or more linear transformations in succession. If the transformations are matrix transformations, say left-multiplication by $B$ followed by left-multiplication by $A$, then the composition is the mapping $\mathbf{x} \mapsto A(B \mathbf{x})$.</p><p>condition number (of $A$ ): The quotient $\sigma_{1} / \sigma_{n}$, where $\sigma_{1}$ is the largest singular value of $A$ and $\sigma_{n}$ is the smallest singular value. The condition number is $+\infty$ when $\sigma_{n}$ is zero.</p><p>conformable for block multiplication: Two partitioned matrices $A$ and $B$ such that the block product $A B$ is defined: The column partition of $A$ must match the row partition of $B$.</p><p>consistent linear system: A linear system with at least one solution.</p><p>constrained optimization: The problem of maximizing a quantity such as $\mathbf{x}^{T} A \mathbf{x}$ or $|A \mathbf{x}|$ when $\mathbf{x}$ is subject to one or more constraints, such as $\mathbf{x}^{T} \mathbf{x}=1$ or $\mathbf{x}^{T} \mathbf{v}=0$.</p><p>consumption matrix: A matrix in the Leontief input-output model whose columns are the unit consumption vectors for the various sectors of an economy.</p><p>contraction: A mapping $\mathbf{x} \mapsto r \mathbf{x}$ for some scalar $r$, with $0 \leq r \leq 1$</p><p>controllable (pair of matrices): A matrix pair $(A, B)$ where $A$ is $n \times n, B$ has $n$ rows, and</p><p>$$
\operatorname{rank}\left[\begin{array}{lllll}
B & A B & A^{2} B & \cdots & A^{n-1} B
\end{array}\right]=n
$$</p><p>Related to a state-space model of a control system and the difference equation $\mathbf{x}_{k+1}=A \mathbf{x}_{k}+B \mathbf{u}_{k}(k=0,1, \ldots)$.</p><p>convergent (sequence of vectors): A sequence $\left{\mathbf{x}_{k}\right}$ such that the entries in $\mathbf{x}_{k}$ can be made as close as desired to the entries in some fixed vector for all $k$ sufficiently large.</p><p>convex combination (of points $\mathbf{v}_{1}, \ldots, \mathbf{v}_{k}$ in $\mathbb{R}^{n}$ ): A linear combination of vectors (points) in which the weights in the combination are nonnegative and the sum of the weights is 1.</p><p>convex hull (of a set $S$ ): The set of all convex combinations of points in $S$, denoted by: conv $S$. convex set: A set $S$ with the property that for each $\mathbf{p}$ and $\mathbf{q}$ in $S$, the line segment $\overline{\mathbf{p q}}$ is contained in $S$.</p><p>coordinate mapping (determined by an ordered basis $\mathcal{B}$ in a vector space $V$ ): A mapping that associates to each $\mathbf{x}$ in $V$ its coordinate vector $[\mathbf{x}]_{\mathcal{B}}$.</p><p>coordinates of $x$ relative to the basis $\mathcal{B}=\left{\mathbf{b}_{1}, \ldots, \mathbf{b}_{\boldsymbol{n}}\right}$ : The weights $c_{1}, \ldots, c_{n}$ in the equation $\mathbf{x}=c_{1} \mathbf{b}_{1}+\cdots+c_{n} \mathbf{b}_{n}$.</p><p>coordinate vector of $\mathbf{x}$ relative to $\mathcal{B}$ : The vector $[\mathbf{x}]_{\mathcal{B}}$ whose entries are the coordinates of $\mathbf{x}$ relative to the basis $\mathcal{B}$.</p><p>covariance (of variables $x_{i}$ and $x_{j}$, for $i \neq j$ ): The entry $s_{i j}$ in the covariance matrix $S$ for a matrix of observations, where $x_{i}$ and $x_{j}$ vary over the $i$ th and $j$ th coordinates, respectively, of the observation vectors.</p><p>covariance matrix (or sample covariance matrix): The $p \times p$ matrix $S$ defined by $S=(N-1)^{-1} B B^{T}$, where $B$ is a $p \times N$ matrix of observations in mean-deviation form.</p><p>Cramer&rsquo;s rule: A formula for each entry in the solution $\mathbf{x}$ of the equation $A \mathbf{x}=\mathbf{b}$ when $A$ is an invertible matrix.</p><p>cross-product term: A term $c x_{i} x_{j}$ in a quadratic form, with $i \neq j$.</p><p>cube: A three-dimensional solid object bounded by six square faces, with three faces meeting at each vertex.</p><p>decoupled system: A difference equation $\mathbf{y}_{k+1}=A \mathbf{y}_{k}$, or a differential equation $\mathbf{y}^{\prime}(t)=A \mathbf{y}(t)$, in which $A$ is a diagonal matrix. The discrete evolution of each entry in $\mathbf{y}_{k}$ (as a function of $k$ ), or the continuous evolution of each entry in the vector-valued function $\mathbf{y}(t)$, is unaffected by what happens to the other entries as $k \rightarrow \infty$ or $t \rightarrow \infty$.</p><p>design matrix: The matrix $X$ in the linear model $\mathbf{y}=\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\epsilon}$, where the columns of $X$ are determined in some way by the observed values of some independent variables.</p><p>determinant (of a square matrix $A$ ): The number $\operatorname{det} A$ defined inductively by a cofactor expansion along the first row of $A$. Also, $(-1)^{r}$ times the product of the diagonal entries in any echelon form $U$ obtained from $A$ by row replacements and $r$ row interchanges (but no scaling operations).</p><p>diagonal entries (in a matrix): Entries having equal row and column indices.</p><p>diagonalizable (matrix): A matrix that can be written in factored form as $P D P^{-1}$, where $D$ is a diagonal matrix and $P$ is an invertible matrix.</p><p>diagonal matrix: A square matrix whose entries not on the main diagonal are all zero.</p><p>difference equation (or linear recurrence relation): An equation of the form $\mathbf{x}_{k+1}=A \mathbf{x}_{k}(k=0,1,2, \ldots)$ whose solution is a sequence of vectors, $\mathbf{x}_{0}, \mathbf{x}_{1}, \ldots$</p><p>dilation: A mapping $\mathbf{x} \mapsto r \mathbf{x}$ for some scalar $r$, with $1&lt;r$. dimension:</p><p>of a flat $S$ : The dimension of the corresponding parallel subspace.</p><p>of a set $S$ : The dimension of the smallest flat containing $S$. of a subspace $S$ : The number of vectors in a basis for $S$, written as $\operatorname{dim} S$.</p><p>of a vector space $V$ : The number of vectors in a basis for $V$, written as $\operatorname{dim} V$. The dimension of the zero space is 0 .</p><p>discrete linear dynamical system: A difference equation of the form $\mathbf{x}_{k+1}=A \mathbf{x}_{k}$ that describes the changes in a system (usually a physical system) as time passes. The physical system is measured at discrete times, when $k=0,1,2, \ldots$, and the state of the system at time $k$ is a vector $\mathbf{x}_{k}$ whose entries provide certain facts of interest about the system.</p><p>distance between $\mathbf{u}$ and $\mathbf{v}$ : The length of the vector $\mathbf{u}-\mathbf{v}$, denoted by dist $(\mathbf{u}, \mathbf{v})$.</p><p>distance to a subspace: The distance from a given point (vector) $\mathbf{v}$ to the nearest point in the subspace.</p><p>distributive laws: (left) $A(B+C)=A B+A C$, and (right) $(B+C) A=B A+C A$, for all $A, B, C$.</p><p>domain (of a transformation $T$ ): The set of all vectors $\mathbf{x}$ for which $T(\mathbf{x})$ is defined.</p><p>dot product: See inner product.</p><p>dynamical system: See discrete linear dynamical system.</p><p>echelon form (or row echelon form, of a matrix): An echelon matrix that is row equivalent to the given matrix.</p><p>echelon matrix (or row echelon matrix): A rectangular matrix that has three properties: (1) All nonzero rows are above any row of all zeros. (2) Each leading entry of a row is in a column to the right of the leading entry of the row above it. (3) All entries in a column below a leading entry are zero.</p><p>eigenfunctions (of a differential equation $\mathbf{x}^{\prime}(t)=A \mathbf{x}(t)$ ): $\quad \mathrm{A}$ function $\mathbf{x}(t)=\mathbf{v} e^{\lambda t}$, where $\mathbf{v}$ is an eigenvector of $A$ and $\lambda$ is the corresponding eigenvalue.</p><p>eigenspace (of $A$ corresponding to $\lambda$ ): The set of all solutions of $A \mathbf{x}=\lambda \mathbf{x}$, where $\lambda$ is an eigenvalue of $A$. Consists of the zero vector and all eigenvectors corresponding to $\lambda$.</p><p>eigenvalue (of $A$ ): A scalar $\lambda$ such that the equation $A \mathbf{x}=\lambda \mathbf{x}$ has a solution for some nonzero vector $\mathbf{x}$.</p><p>eigenvector (of $A$ ): A nonzero vector $\mathbf{x}$ such that $A \mathbf{x}=\lambda \mathbf{x}$ for some scalar $\lambda$.</p><p>eigenvector basis: A basis consisting entirely of eigenvectors of a given matrix.</p><p>eigenvector decomposition (of $\mathbf{x}$ ): An equation, $\mathbf{x}=c_{1} \mathbf{v}_{1}+$ $\cdots+c_{n} \mathbf{v}_{n}$, expressing $\mathbf{x}$ as a linear combination of eigenvectors of a matrix.</p><p>elementary matrix: An invertible matrix that results by performing one elementary row operation on an identity matrix.</p><p>elementary row operations: (1) (Replacement) Replace one row by the sum of itself and a multiple of another row. (2) Interchange two rows. (3) (Scaling) Multiply all entries in a row by a nonzero constant.</p><p>equal vectors: Vectors in $\mathbb{R}^{n}$ whose corresponding entries are the same. equilibrium prices: A set of prices for the total output of the various sectors in an economy, such that the income of each sector exactly balances its expenses.</p><p>equilibrium vector: See steady-state vector.</p><p>equivalent (linear) systems: Linear systems with the same solution set.</p><p>exchange model: See Leontief exchange model.</p><p>existence question: Asks, &ldquo;Does a solution to the system exist?&rdquo; That is, &ldquo;Is the system consistent?&rdquo; Also, &ldquo;Does a solution of $A \mathbf{x}=\mathbf{b}$ exist for all possible $\mathbf{b}$ ?&rdquo;</p><p>expansion by cofactors: See cofactor expansion.</p><p>explicit description (of a subspace $W$ of $\mathbb{R}^{n}$ ): A parametric representation of $W$ as the set of all linear combinations of a set of specified vectors.</p><p>extreme point (of a convex set $S$ ): A point $\mathbf{p}$ in $S$ such that $\mathbf{p}$ is not in the interior of any line segment that lies in $S$. (That is, if $\mathbf{x}, \mathbf{y}$ are in $S$ and $\mathbf{p}$ is on the line segment $\overline{\mathbf{x y}}$, then $\mathbf{p}=\mathbf{x}$ or $\mathbf{p}=\mathbf{y}$.</p><p>factorization ( of $A$ ): An equation that expresses $A$ as a product of two or more matrices.</p><p>final demand vector (or bill of final demands): The vector d in the Leontief input-output model that lists the dollar values of the goods and services demanded from the various sectors by the nonproductive part of the economy. The vector $\mathbf{d}$ can represent consumer demand, government consumption, surplus production, exports, or other external demand.</p><p>finite-dimensional (vector space): A vector space that is spanned by a finite set of vectors.</p><p>flat $\left(\right.$ in $\mathbb{R}^{n}$ ): A translate of a subspace of $\mathbb{R}^{n}$.</p><p>flexibility matrix: A matrix whose $j$ th column gives the deflections of an elastic beam at specified points when a unit force is applied at the $j$ th point on the beam.</p><p>floating point arithmetic: Arithmetic with numbers represented as decimals $\pm . d_{1} \cdots d_{p} \times 10^{r}$, where $r$ is an integer and the number $p$ of digits to the right of the decimal point is usually between 8 and 16 .</p><p>flop: One arithmetic operation $(+,-, *, /)$ on two real floating point numbers.</p><p>forward phase (of row reduction): The first part of the algorithm that reduces a matrix to echelon form.</p><p>Fourier approximation (of order $n$ ): The closest point in the subspace of $n$ th-order trigonometric polynomials to a given function in $C[0,2 \pi]$.</p><p>Fourier coefficients: The weights used to make a trigonometric polynomial as a Fourier approximation to a function.</p><p>Fourier series: An infinite series that converges to a function in the inner product space $C[0,2 \pi]$, with the inner product given by a definite integral.</p><p>free variable: Any variable in a linear system that is not a basic variable. full rank (matrix): An $m \times n$ matrix whose rank is the smaller of $m$ and $n$.</p><p>fundamental set of solutions: A basis for the set of all solutions of a homogeneous linear difference or differential equation.</p><p>fundamental subspaces (determined by $A$ ): The null space and column space of $A$, and the null space and column space of $A^{T}$, with $\operatorname{Col} A^{T}$ commonly called the row space of $A$.</p><p>Gaussian elimination: See row reduction algorithm.</p><p>general least-squares problem: Given an $m \times n$ matrix $A$ and a vector $\mathbf{b}$ in $\mathbb{R}^{m}$, find $\hat{\mathbf{x}}$ in $\mathbb{R}^{n}$ such that $|\mathbf{b}-A \hat{\mathbf{x}}| \leq|\mathbf{b}-A \mathbf{x}|$ for all $\mathbf{x}$ in $\mathbb{R}^{n}$.</p><p>general solution (of a linear system): A parametric description of a solution set that expresses the basic variables in terms of the free variables (the parameters), if any. After Section 1.5, the parametric description is written in vector form.</p><p>Givens rotation: A linear transformation from $\mathbb{R}^{n}$ to $\mathbb{R}^{n}$ used in computer programs to create zero entries in a vector (usually a column of a matrix).</p><p>Gram matrix (of $A$ ): The matrix $A^{T} A$.</p><p>Gram-Schmidt process: An algorithm for producing an orthogonal or orthonormal basis for a subspace that is spanned by a given set of vectors.</p><p>homogeneous coordinates: In $\mathbb{R}^{3}$, the representation of $(x, y, z)$ as $(X, Y, Z, H)$ for any $H \neq 0$, where $x=X / H$, $y=Y / H$, and $z=Z / H$. In $\mathbb{R}^{2}, H$ is usually taken as 1 , and the homogeneous coordinates of $(x, y)$ are written as $(x, y, 1)$.</p><p>homogeneous equation: An equation of the form $A \mathbf{x}=\mathbf{0}$, possibly written as a vector equation or as a system of linear equations.</p><p>homogeneous form of (a vector) $\mathbf{v}$ in $\mathbb{R}^{n}:$ The point $\tilde{\mathbf{v}}=\left[\begin{array}{l}\mathbf{v} \\ 1\end{array}\right]$ in $\mathbb{R}^{n+1}$.</p><p>Householder reflection: A transformation $\mathbf{x} \mapsto Q \mathbf{x}$, where $Q=I-2 \mathbf{u u}^{T}$ and $\mathbf{u}$ is a unit vector $\left(\mathbf{u}^{T} \mathbf{u}=1\right)$.</p><p>hyperplane (in $\mathbb{R}^{n}$ ): A flat in $\mathbb{R}^{n}$ of dimension $n-1$. Also: a translate of a subspace of dimension $n-1$.</p><p>identity matrix (denoted by $I$ or $I_{n}$ ): A square matrix with ones on the diagonal and zeros elsewhere.</p><p>ill-conditioned matrix: A square matrix with a large (or possibly infinite) condition number; a matrix that is singular or can become singular if some of its entries are changed ever so slightly.</p><p>image (of a vector $\mathbf{x}$ under a transformation $T$ ): The vector $T(\mathbf{x})$ assigned to $\mathbf{x}$ by $T$. implicit description (of a subspace $W$ of $\mathbb{R}^{n}$ ): A set of one or more homogeneous equations that characterize the points of $W$.</p><p>Im $\mathbf{x}$ : The vector in $\mathbb{R}^{n}$ formed from the imaginary parts of the entries of a vector $\mathbf{x}$ in $\mathbb{C}^{n}$.</p><p>inconsistent linear system: A linear system with no solution.</p><p>indefinite matrix: A symmetric matrix $A$ such that $\mathbf{x}^{T} A \mathbf{x}$ assumes both positive and negative values.</p><p>indefinite quadratic form: A quadratic form $Q$ such that $Q(\mathbf{x})$ assumes both positive and negative values.</p><p>infinite-dimensional (vector space): A nonzero vector space $V$ that has no finite basis.</p><p>inner product: The scalar $\mathbf{u}^{T} \mathbf{v}$, usually written as $\mathbf{u} \cdot \mathbf{v}$, where $\mathbf{u}$ and $\mathbf{v}$ are vectors in $\mathbb{R}^{n}$ viewed as $n \times 1$ matrices. Also called the dot product of $\mathbf{u}$ and $\mathbf{v}$. In general, a function on a vector space that assigns to each pair of vectors $\mathbf{u}$ and $\mathbf{v}$ a number $\langle\mathbf{u}, \mathbf{v}\rangle$, subject to certain axioms. See Section 6.7.</p><p>inner product space: A vector space on which is defined an inner product.</p><p>input-output matrix: See consumption matrix.</p><p>input-output model: See Leontief input-output model.</p><p>interior point (of a set $S$ in $\mathbb{R}^{n}$ ): A point $\mathbf{p}$ in $S$ such that for some $\delta>0$, the open ball $\mathbf{B}(\mathbf{p}, \delta)$ centered at $\mathbf{p}$ is contained in $S$.</p><p>intermediate demands: Demands for goods or services that will be consumed in the process of producing other goods and services for consumers. If $\mathbf{x}$ is the production level and $C$ is the consumption matrix, then $C \mathbf{x}$ lists the intermediate demands.</p><p>interpolating polynomial: A polynomial whose graph passes through every point in a set of data points in $\mathbb{R}^{2}$.</p><p>invariant subspace (for $A$ ): A subspace $H$ such that $A \mathbf{x}$ is in $H$ whenever $\mathbf{x}$ is in $H$.</p><p>inverse (of an $n \times n$ matrix $A$ ): An $n \times n$ matrix $A^{-1}$ such that $A A^{-1}=A^{-1} A=I_{n}$.</p><p>inverse power method: An algorithm for estimating an eigenvalue $\lambda$ of a square matrix, when a good initial estimate of $\lambda$ is available.</p><p>invertible linear transformation: A linear transformation $T: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ such that there exists a function $S: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ satisfying both $T(S(\mathbf{x}))=\mathbf{x}$ and $S(T(\mathbf{x}))=\mathbf{x}$ for all $\mathbf{x}$ in $\mathbb{R}^{n}$.</p><p>invertible matrix: A square matrix that possesses an inverse.</p><p>isomorphic vector spaces: Two vector spaces $V$ and $W$ for which there is a one-to-one linear transformation $T$ that maps $V$ onto $W$</p><p>isomorphism: A one-to-one linear mapping from one vector space onto another.</p><p>kernel (of a linear transformation $T: V \rightarrow W$ ): The set of $\mathbf{x}$ in $V$ such that $T(\mathbf{x})=\mathbf{0}$.</p><p>Kirchhoff&rsquo;s laws: (1) (voltage law) The algebraic sum of the $R I$ voltage drops in one direction around a loop equals the algebraic sum of the voltage sources in the same direction around the loop. (2) (current law) The current in a branch is the algebraic sum of the loop currents flowing through that branch.</p><p>ladder network: An electrical network assembled by connecting in series two or more electrical circuits.</p><p>leading entry: The leftmost nonzero entry in a row of a matrix. least-squares error: The distance $|\mathbf{b}-A \hat{\mathbf{x}}|$ from $\mathbf{b}$ to $A \hat{\mathbf{x}}$, when $\hat{\mathbf{x}}$ is a least-squares solution of $A \mathbf{x}=\mathbf{b}$.</p><p>least-squares line: The line $y=\hat{\beta}_{0}+\hat{\beta}_{1} x$ that minimizes the least-squares error in the equation $\mathbf{y}=X \boldsymbol{\beta}+\boldsymbol{\epsilon}$.</p><p>least-squares solution (of $A \mathbf{x}=\mathbf{b}$ ): A vector $\hat{\mathbf{x}}$ such that $|\mathbf{b}-A \hat{\mathbf{x}}| \leq|\mathbf{b}-A \mathbf{x}|$ for all $\mathbf{x}$ in $\mathbb{R}^{n}$</p><p>left inverse (of $A$ ): Any rectangular matrix $C$ such that $C A=I$.</p><p>left-multiplication (by $A$ ): Multiplication of a vector or matrix on the left by $A$.</p><p>left singular vectors (of $A$ ): The columns of $U$ in the singular value decomposition $A=U \Sigma V^{T}$.</p><p>length (or norm, of $\mathbf{v}$ ): The scalar $|\mathbf{v}|=\sqrt{\mathbf{v} \cdot \mathbf{v}}=\sqrt{\langle\mathbf{v}, \mathbf{v}\rangle}$.</p><p>Leontief exchange (or closed) model: A model of an economy where inputs and outputs are fixed, and where a set of prices for the outputs of the sectors is sought such that the income of each sector equals its expenditures. This &ldquo;equilibrium&rdquo; condition is expressed as a system of linear equations, with the prices as the unknowns.</p><p>Leontief input-output model (or Leontief production equation): The equation $\mathbf{x}=C \mathbf{x}+\mathbf{d}$, where $\mathbf{x}$ is production, $\mathbf{d}$ is final demand, and $C$ is the consumption (or input-output) matrix. The $j$ th column of $C$ lists the inputs that sector $j$ consumes per unit of output.</p><p>level set (or gradient) of a linear functional $f$ on $\mathbb{R}^{n}$ : A set $[f: d]=\left{\mathbf{x} \in \mathbb{R}^{n}: f(\mathbf{x})=d\right}$</p><p>linear combination: A sum of scalar multiples of vectors. The scalars are called the weights.</p><p>linear dependence relation: A homogeneous vector equation where the weights are all specified and at least one weight is nonzero.</p><p>linear equation (in the variables $x_{1}, \ldots, x_{n}$ ): An equation that can be written in the form $a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{n} x_{n}=b$, where $b$ and the coefficients $a_{1}, \ldots, a_{n}$ are real or complex numbers.</p><p>linear filter: A linear difference equation used to transform discrete-time signals.</p><p>linear functional (on $\mathbb{R}^{n}$ ): A linear transformation $f$ from $\mathbb{R}^{n}$ into $\mathbb{R}$.</p><p>linearly dependent (vectors): An indexed set $\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$ with the property that there exist weights $c_{1}, \ldots, c_{p}$, not all zero, such that $c_{1} \mathbf{v}_{1}+\cdots+c_{p} \mathbf{v}_{p}=\mathbf{0}$. That is, the vector equation $c_{1} \mathbf{v}_{1}+c_{2} \mathbf{v}_{2}+\cdots+c_{p} \mathbf{v}_{p}=\mathbf{0}$ has a nontrivial solution.</p><p>linearly independent (vectors): An indexed set $\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$ with the property that the vector equation $c_{1} \mathbf{v}_{1}+$ $c_{2} \mathbf{v}_{2}+\cdots+c_{p} \mathbf{v}_{p}=\mathbf{0}$ has only the trivial solution, $c_{1}=\cdots=c_{p}=0$.</p><p>linear model (in statistics): Any equation of the form $\mathbf{y}=X \boldsymbol{\beta}+\boldsymbol{\epsilon}$, where $X$ and $\mathbf{y}$ are known and $\boldsymbol{\beta}$ is to be chosen to minimize the length of the residual vector, $\epsilon$.</p><p>linear system: A collection of one or more linear equations involving the same variables, say, $x_{1}, \ldots, x_{n}$.</p><p>linear transformation $\boldsymbol{T}$ (from a vector space $V$ into a vector space $W$ ): A rule $T$ that assigns to each vector $\mathbf{x}$ in $V$ a unique vector $T(\mathbf{x})$ in $W$, such that (i) $T(\mathbf{u}+\mathbf{v})=T(\mathbf{u})+T(\mathbf{v})$ for all $\mathbf{u}, \mathbf{v}$ in $V$, and (ii) $T(c \mathbf{u})=c T(\mathbf{u})$ for all $\mathbf{u}$ in $V$ and all scalars $c$. Notation: $T: V \rightarrow W ;$ also, $\mathbf{x} \mapsto A \mathbf{x}$ when $T: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ and $A$ is the standard matrix for $T$.</p><p>line through p parallel to $\mathbf{v}$ : The set ${\mathbf{p}+t \mathbf{v}: t$ in $\mathbb{R}}$.</p><p>loop current: The amount of electric current flowing through a loop that makes the algebraic sum of the $R I$ voltage drops around the loop equal to the algebraic sum of the voltage sources in the loop.</p><p>lower triangular matrix: A matrix with zeros above the main diagonal.</p><p>lower triangular part (of $A$ ): A lower triangular matrix whose entries on the main diagonal and below agree with those in $A$.</p><p>LU factorization: The representation of a matrix $A$ in the form $A=L U$ where $L$ is a square lower triangular matrix with ones on the diagonal (a unit lower triangular matrix) and $U$ is an echelon form of $A$.</p><p>magnitude (of a vector): See norm.</p><p>main diagonal (of a matrix): The entries with equal row and column indices.</p><p>mapping: See transformation.</p><p>Markov chain: A sequence of probability vectors $\mathbf{x}_{0}, \mathbf{x}_{1}$, $\mathbf{x}_{2}, \ldots$, together with a stochastic matrix $P$ such that $\mathbf{x}_{k+1}=P \mathbf{x}_{k}$ for $k=0,1,2, \ldots$</p><p>matrix: A rectangular array of numbers.</p><p>matrix equation: An equation that involves at least one matrix; for instance, $A \mathbf{x}=\mathbf{b}$.</p><p>matrix for $T$ relative to bases $\mathcal{B}$ and $\mathcal{C}$ : A matrix $M$ for a linear transformation $T: V \rightarrow W$ with the property that $[T(\mathbf{x})]<em>{\mathcal{C}}=M[\mathbf{x}]</em>{\mathcal{B}}$ for all $\mathbf{x}$ in $V$, where $\mathcal{B}$ is a basis for $V$ and $\mathcal{C}$ is a basis for $W$. When $W=V$ and $\mathcal{C}=\mathcal{B}$, the matrix $M$ is called the $\mathcal{B}$-matrix for $T$ and is denoted by $[T]_{\mathcal{B}}$.</p><p>matrix of observations: A $p \times N$ matrix whose columns are observation vectors, each column listing $p$ measurements made on an individual or object in a specified population or set. matrix transformation: A mapping $\mathbf{x} \mapsto A \mathbf{x}$, where $A$ is an $m \times n$ matrix and $\mathbf{x}$ represents any vector in $\mathbb{R}^{n}$.</p><p>maximal linearly independent set (in $V$ ): A linearly independent set $\mathcal{B}$ in $V$ such that if a vector $\mathbf{v}$ in $V$ but not in $\mathcal{B}$ is added to $\mathcal{B}$, then the new set is linearly dependent.</p><p>mean-deviation form (of a matrix of observations): A matrix whose row vectors are in mean-deviation form. For each row, the entries sum to zero.</p><p>mean-deviation form (of a vector): A vector whose entries sum to zero.</p><p>mean square error: The error of an approximation in an inner product space, where the inner product is defined by a definite integral.</p><p>migration matrix: A matrix that gives the percentage movement between different locations, from one period to the next.</p><p>minimal spanning set (for a subspace $H$ ): A set $\mathcal{B}$ that spans $H$ and has the property that if one of the elements of $\mathcal{B}$ is removed from $\mathcal{B}$, then the new set does not span $H$.</p><p>$m \times n$ matrix: A matrix with $m$ rows and $n$ columns.</p><p>Moore-Penrose inverse: See pseudoinverse.</p><p>multiple regression: A linear model involving several independent variables and one dependent variable.</p><p>nearly singular matrix: An ill-conditioned matrix.</p><p>negative definite matrix: A symmetric matrix $A$ such that $\mathbf{x}^{T} A \mathbf{x}&lt;0$ for all $\mathbf{x} \neq \mathbf{0}$.</p><p>negative definite quadratic form: A quadratic form $Q$ such that $Q(\mathbf{x})&lt;0$ for all $\mathbf{x} \neq \mathbf{0}$.</p><p>negative semidefinite matrix: A symmetric matrix $A$ such that $\mathbf{x}^{T} A \mathbf{x} \leq 0$ for all $\mathbf{x}$.</p><p>negative semidefinite quadratic form: A quadratic form $Q$ such that $Q(\mathbf{x}) \leq 0$ for all $\mathbf{x}$.</p><p>nonhomogeneous equation: An equation of the form $A \mathbf{x}=\mathbf{b}$ with $\mathbf{b} \neq \mathbf{0}$, possibly written as a vector equation or as a system of linear equations.</p><p>nonsingular (matrix): An invertible matrix.</p><p>nontrivial solution: A nonzero solution of a homogeneous equation or system of homogeneous equations.</p><p>nonzero (matrix or vector): A matrix (with possibly only one row or column) that contains at least one nonzero entry.</p><p>norm (or length, of $\mathbf{v}$ ): The scalar $|\mathbf{v}|=\sqrt{\mathbf{v} \cdot \mathbf{v}}=\sqrt{\langle\mathbf{v}, \mathbf{v}\rangle}$.</p><p>normal equations: The system of equations represented by $A^{T} A \mathbf{x}=A^{T} \mathbf{b}$, whose solution yields all least-squares solutions of $A \mathbf{x}=\mathbf{b}$. In statistics, a common notation is $X^{T} X \boldsymbol{\beta}=X^{T} \mathbf{y}$</p><p>normalizing (a nonzero vector $\mathbf{v}$ ): The process of creating a unit vector $\mathbf{u}$ that is a positive multiple of $\mathbf{v}$.</p><p>normal vector (to a subspace $V$ of $\mathbb{R}^{n}$ ): A vector $\mathbf{n}$ in $\mathbb{R}^{n}$ such that $\mathbf{n} \cdot \mathbf{x}=0$ for all $\mathbf{x}$ in $V$.</p><p>null space ( of an $m \times n$ matrix $A$ ): The set $\operatorname{Nul} A$ of all solutions to the homogeneous equation $A \mathbf{x}=\mathbf{0}$. Nul $A={\mathbf{x}: \mathbf{x}$ is in $\mathbb{R}^{n}$ and $\left.A \mathbf{x}=\mathbf{0}\right}$</p><p>observation vector: The vector $\mathbf{y}$ in the linear model $\mathbf{y}=X \boldsymbol{\beta}+\boldsymbol{\epsilon}$, where the entries in $\mathbf{y}$ are the observed values of a dependent variable.</p><p>one-to-one (mapping): A mapping $T: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ such that each $\mathbf{b}$ in $R^{m}$ is the image of at most one $\mathbf{x}$ in $\mathbb{R}^{n}$.</p><p>onto (mapping): A mapping $T: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ such that each $\mathbf{b}$ in $R^{m}$ is the image of at least one $\mathbf{x}$ in $\mathbb{R}^{n}$.</p><p>open ball $\mathbf{B}(\mathbf{p}, \delta)$ in $\mathbb{R}^{n}$ : The set ${\mathbf{x}:|\mathbf{x}-\mathbf{p}|&lt;\delta}$ in $\mathbb{R}^{n}$, where $\delta>0$</p><p>open set $S$ in $\mathbb{R}^{n}$ : A set that contains none of its boundary points. (Equivalently, $S$ is open if every point of $S$ is an interior point.)</p><p>origin: The zero vector.</p><p>orthogonal basis: A basis that is also an orthogonal set.</p><p>orthogonal complement ( of $W$ ): The set $W^{\perp}$ of all vectors orthogonal to $W$.</p><p>orthogonal decomposition: The representation of a vector $\mathbf{y}$ as the sum of two vectors, one in a specified subspace $W$ and the other in $W^{\perp}$. In general, a decomposition $\mathbf{y}=c_{1} \mathbf{u}_{1}+\cdots+c_{p} \mathbf{u}_{p}$, where $\left{\mathbf{u}_{1}, \ldots, \mathbf{u}_{p}\right}$ is an orthogonal basis for a subspace that contains $\mathbf{y}$.</p><p>orthogonally diagonalizable (matrix): A matrix $A$ that admits a factorization, $A=P D P^{-1}$, with $P$ an orthogonal matrix $\left(P^{-1}=P^{T}\right)$ and $D$ diagonal.</p><p>orthogonal matrix: A square invertible matrix $U$ such that $U^{-1}=U^{T}$</p><p>orthogonal projection of $\mathbf{y}$ onto $\mathbf{u}$ (or onto the line through $\mathbf{u}$ and the origin, for $\mathbf{u} \neq \mathbf{0}$ ): The vector $\hat{\mathbf{y}}$ defined by $\hat{\mathbf{y}}=\frac{\mathbf{y} \cdot \mathbf{u}}{\mathbf{u} \cdot \mathbf{u}} \mathbf{u}$. orthogonal projection of y onto $W$ : The unique vector $\hat{\mathbf{y}}$ in $W$ such that $\mathbf{y}-\hat{\mathbf{y}}$ is orthogonal to $W$. Notation: $\hat{\mathbf{y}}=\operatorname{proj}_{W} \mathbf{y}$.</p><p>orthogonal set: A set $S$ of vectors such that $\mathbf{u} \cdot \mathbf{v}=0$ for each distinct pair $\mathbf{u}, \mathbf{v}$ in $S$.</p><p>orthogonal to $\boldsymbol{W}$ : Orthogonal to every vector in $W$.</p><p>orthonormal basis: A basis that is an orthogonal set of unit vectors.</p><p>orthonormal set: An orthogonal set of unit vectors.</p><p>outer product: A matrix product $\mathbf{u v}^{T}$ where $\mathbf{u}$ and $\mathbf{v}$ are vectors in $\mathbb{R}^{n}$ viewed as $n \times 1$ matrices. (The transpose symbol is on the &ldquo;outside&rdquo; of the symbols $\mathbf{u}$ and $\mathbf{v}$.)</p><p>overdetermined system: A system of equations with more equations than unknowns.</p><p>parallel flats: Two or more flats such that each flat is a translate of the other flats. parallelogram rule for addition: A geometric interpretation of the sum of two vectors $\mathbf{u}, \mathbf{v}$ as the diagonal of the parallelogram determined by $\mathbf{u}, \mathbf{v}$, and $\mathbf{0}$.</p><p>parameter vector: The unknown vector $\boldsymbol{\beta}$ in the linear model $\mathbf{y}=X \boldsymbol{\beta}+\boldsymbol{\epsilon}$</p><p>parametric equation of a line: An equation of the form $\mathbf{x}=\mathbf{p}+t \mathbf{v}(t$ in $\mathbb{R})$.</p><p>parametric equation of a plane: An equation of the form $\mathbf{x}=\mathbf{p}+s \mathbf{u}+t \mathbf{v} \quad(s, t$ in $\mathbb{R})$, with $\mathbf{u}$ and $\mathbf{v}$ linearly independent.</p><p>partitioned matrix (or block matrix): A matrix whose entries are themselves matrices of appropriate sizes.</p><p>permuted lower triangular matrix: A matrix such that a permutation of its rows will form a lower triangular matrix.</p><p>permuted LU factorization: The representation of a matrix $A$ in the form $A=L U$ where $L$ is a square matrix such that a permutation of its rows will form a unit lower triangular matrix, and $U$ is an echelon form of $A$.</p><p>pivot: A nonzero number that either is used in a pivot position to create zeros through row operations or is changed into a leading 1 , which in turn is used to create zeros.</p><p>pivot column: A column that contains a pivot position.</p><p>pivot position: A position in a matrix $A$ that corresponds to a leading entry in an echelon form of $A$.</p><p>plane through $\mathbf{u}, \mathbf{v}$, and the origin: A set whose parametric equation is $\mathbf{x}=s \mathbf{u}+t \mathbf{v}(s, t$ in $\mathbb{R})$, with $\mathbf{u}$ and $\mathbf{v}$ linearly independent.</p><p>polar decomposition (of $A$ ): A factorization $A=P Q$, where $P$ is an $n \times n$ positive semidefinite matrix with the same rank as $A$, and $Q$ is an $n \times n$ orthogonal matrix.</p><p>polygon: A polytope in $\mathbb{R}^{2}$.</p><p>polyhedron: A polytope in $\mathbb{R}^{3}$.</p><p>polytope: The convex hull of a finite set of points in $\mathbb{R}^{n}$ (a special type of compact convex set).</p><p>positive combination (of points $\mathbf{v}_{1}, \ldots, \mathbf{v}_{m}$ in $\mathbb{R}^{n}$ ): A linear combination $c_{1} \mathbf{v}_{1}+\cdots+c_{m} \mathbf{v}_{m}$, where all $c_{i} \geq 0$.</p><p>positive definite matrix: A symmetric matrix $A$ such that $\mathbf{x}^{T} A \mathbf{x}>0$ for all $\mathbf{x} \neq \mathbf{0}$.</p><p>positive definite quadratic form: A quadratic form $Q$ such that $Q(\mathbf{x})>0$ for all $\mathbf{x} \neq \mathbf{0}$.</p><p>positive hull (of a set $S$ ): The set of all positive combinations of points in $S$, denoted by pos $S$.</p><p>positive semidefinite matrix: A symmetric matrix $A$ such that $\mathbf{x}^{T} A \mathbf{x} \geq 0$ for all $\mathbf{x}$.</p><p>positive semidefinite quadratic form: A quadratic form $Q$ such that $Q(\mathbf{x}) \geq 0$ for all $\mathbf{x}$.</p><p>power method: An algorithm for estimating a strictly dominant eigenvalue of a square matrix.</p><p>principal axes (of a quadratic form $\mathbf{x}^{T} A \mathbf{x}$ ): The orthonormal columns of an orthogonal matrix $P$ such that $P^{-1} A P$ is diagonal. (These columns are unit eigenvectors of $A$.) Usually the columns of $P$ are ordered in such a way that the corresponding eigenvalues of $A$ are arranged in decreasing order of magnitude.</p><p>principal components (of the data in a matrix $B$ of observations): The unit eigenvectors of a sample covariance matrix $S$ for $B$, with the eigenvectors arranged so that the corresponding eigenvalues of $S$ decrease in magnitude. If $B$ is in mean-deviation form, then the principal components are the right singular vectors in a singular value decomposition of $B^{T}$.</p><p>probability vector: A vector in $\mathbb{R}^{n}$ whose entries are nonnegative and sum to one.</p><p>product $A \mathbf{x}$ : The linear combination of the columns of $A$ using the corresponding entries in $\mathbf{x}$ as weights.</p><p>production vector: The vector in the Leontief input-output model that lists the amounts that are to be produced by the various sectors of an economy.</p><p>profile (of a set $S$ in $\mathbb{R}^{n}$ ): The set of extreme points of $S$.</p><p>projection matrix (or orthogonal projection matrix): A symmetric matrix $B$ such that $B^{2}=B$. A simple example is $B=\mathbf{v}^{T}$, where $\mathbf{v}$ is a unit vector.</p><p>proper subset of a set $S$ : A subset of $S$ that does not equal $S$ itself.</p><p>proper subspace: Any subspace of a vector space $V$ other than $V$ itself.</p><p>pseudoinverse ( of $A$ ): The matrix $V D^{-1} U^{T}$, when $U D V^{T}$ is a reduced singular value decomposition of $A$.</p><p>QR factorization: A factorization of an $m \times n$ matrix $A$ with linearly independent columns, $A=Q R$, where $Q$ is an $m \times n$ matrix whose columns form an orthonormal basis for $\operatorname{Col} A$, and $R$ is an $n \times n$ upper triangular invertible matrix with positive entries on its diagonal.</p><p>quadratic BÃ©zier curve: A curve whose description may be written in the form $\mathbf{g}(t)=(1-t) \mathbf{f}_{0}(t)+t \mathbf{f}_{1}(t)$ for $0 \leq t \leq$ 1 , where $\mathbf{f}_{0}(t)=(1-t) \mathbf{p}_{0}+t \mathbf{p}_{1}$ and $\mathbf{f}_{1}(t)=(1-t) \mathbf{p}_{1}+$ $t \mathbf{p}_{2}$. The points $\mathbf{p}_{0}, \mathbf{p}_{1}, \mathbf{p}_{2}$ are called the control points for the curve.</p><p>quadratic form: A function $Q$ defined for $\mathbf{x}$ in $\mathbb{R}^{n}$ by $Q(\mathbf{x})=$ $\mathbf{x}^{T} A \mathbf{x}$, where $A$ is an $n \times n$ symmetric matrix (called the matrix of the quadratic form).</p><p>range (of a linear transformation $T$ ): The set of all vectors of the form $T(\mathbf{x})$ for some $\mathbf{x}$ in the domain of $T$.</p><p>rank (of a matrix $A$ ): The dimension of the column space of $A$, denoted by $\operatorname{rank} A$.</p><p>Rayleigh quotient: $R(\mathbf{x})=\left(\mathbf{x}^{T} A \mathbf{x}\right) /\left(\mathbf{x}^{T} \mathbf{x}\right)$. An estimate of an eigenvalue of $A$ (usually a symmetric matrix).</p><p>recurrence relation: See difference equation.</p><p>reduced echelon form (or reduced row echelon form): A reduced echelon matrix that is row equivalent to a given matrix.</p><p>reduced echelon matrix: A rectangular matrix in echelon form that has these additional properties: The leading entry in each nonzero row is 1 , and each leading 1 is the only nonzero entry in its column.</p><p>reduced singular value decomposition: A factorization $A=U D V^{T}$, for an $m \times n$ matrix $A$ of rank $r$, where $U$ is $m \times r$ with orthonormal columns, $D$ is an $r \times r$ diagonal matrix with the $r$ nonzero singular values of $A$ on its diagonal, and $V$ is $n \times r$ with orthonormal columns.</p><p>regression coefficients: The coefficients $\beta_{0}$ and $\beta_{1}$ in the leastsquares line $y=\beta_{0}+\beta_{1} x$.</p><p>regular solid: One of the five possible regular polyhedrons in $\mathbb{R}^{3}$ : the tetrahedron (4 equal triangular faces), the cube (6 square faces), the octahedron (8 equal triangular faces), the dodecahedron (12 equal pentagonal faces), and the icosahedron (20 equal triangular faces).</p><p>regular stochastic matrix: A stochastic matrix $P$ such that some matrix power $P^{k}$ contains only strictly positive entries.</p><p>relative change or relative error (in b): The quantity $|\Delta \mathbf{b}| /|\mathbf{b}|$ when $\mathbf{b}$ is changed to $\mathbf{b}+\Delta \mathbf{b}$.</p><p>repellor (of a dynamical system in $\mathbb{R}^{2}$ ): The origin when all trajectories except the constant zero sequence or function tend away from $\mathbf{0}$.</p><p>residual vector: The quantity $\epsilon$ that appears in the general linear model: $\mathbf{y}=X \boldsymbol{\beta}+\boldsymbol{\epsilon}$; that is, $\boldsymbol{\epsilon}=\mathbf{y}-X \boldsymbol{\beta}$, the difference between the observed values and the predicted values (of $y$ ).</p><p>$\operatorname{Re} \mathbf{x}$ : The vector in $\mathbb{R}^{n}$ formed from the real parts of the entries of a vector $\mathbf{x}$ in $\mathbb{C}^{n}$.</p><p>right inverse (of $A$ ): Any rectangular matrix $C$ such that $A C=I$</p><p>right-multiplication (by $A$ ): Multiplication of a matrix on the right by $A$.</p><p>right singular vectors (of $A$ ): The columns of $V$ in the singular value decomposition $A=U \Sigma V^{T}$.</p><p>roundoff error: Error in floating point arithmetic caused when the result of a calculation is rounded (or truncated) to the number of floating point digits stored. Also, the error that results when the decimal representation of a number such as $1 / 3$ is approximated by a floating point number with a finite number of digits.</p><p>row-column rule: The rule for computing a product $A B$ in which the $(i, j)$-entry of $A B$ is the sum of the products of corresponding entries from row $i$ of $A$ and column $j$ of $B$.</p><p>row equivalent (matrices): Two matrices for which there exists a (finite) sequence of row operations that transforms one matrix into the other.</p><p>row reduction algorithm: A systematic method using elementary row operations that reduces a matrix to echelon form or reduced echelon form. row replacement: An elementary row operation that replaces one row of a matrix by the sum of the row and a multiple of another row.</p><p>row space (of a matrix $A$ ): The set Row $A$ of all linear combinations of the vectors formed from the rows of $A$; also denoted by $\operatorname{Col} A^{T}$.</p><p>row sum: The sum of the entries in a row of a matrix.</p><p>row vector: A matrix with only one row, or a single row of a matrix that has several rows.</p><p>row-vector rule for computing $\boldsymbol{A} \mathbf{x}$ : The rule for computing a product $A \mathbf{x}$ in which the $i$ th entry of $A \mathbf{x}$ is the sum of the products of corresponding entries from row $i$ of $A$ and from the vector $\mathbf{x}$.</p><p>saddle point (of a dynamical system in $\mathbb{R}^{2}$ ): The origin when some trajectories are attracted to $\mathbf{0}$ and other trajectories are repelled from $\mathbf{0}$.</p><p>same direction (as a vector $\mathbf{v}$ ): A vector that is a positive multiple of $\mathbf{v}$.</p><p>sample mean: The average $M$ of a set of vectors, $\mathbf{X}_{1}, \ldots, \mathbf{X}_{N}$, given by $M=(1 / N)\left(\mathbf{X}_{1}+\cdots+\mathbf{X}_{N}\right)$.</p><p>scalar: A (real) number used to multiply either a vector or a matrix.</p><p>scalar multiple of $\mathbf{u}$ by $\boldsymbol{c}$ : The vector $c \mathbf{u}$ obtained by multiplying each entry in $\mathbf{u}$ by $c$.</p><p>scale (a vector): Multiply a vector (or a row or column of a matrix) by a nonzero scalar.</p><p>Schur complement: A certain matrix formed from the blocks of a $2 \times 2$ partitioned matrix $A=\left[A_{i j}\right]$. If $A_{11}$ is invertible, its Schur complement is given by $A_{22}-A_{21} A_{11}^{-1} A_{12}$. If $A_{22}$ is invertible, its Schur complement is given by $A_{11}-A_{12} A_{22}^{-1} A_{21}$.</p><p>Schur factorization (of $A$, for real scalars): A factorization $A=U R U^{T}$ of an $n \times n$ matrix $A$ having $n$ real eigenvalues, where $U$ is an $n \times n$ orthogonal matrix and $R$ is an upper triangular matrix.</p><p>set spanned by $\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{\boldsymbol{p}}\right}$ : The set $\operatorname{Span}\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$.</p><p>signal (or discrete-time signal): A doubly infinite sequence of numbers, $\left{y_{k}\right}$; a function defined on the integers; belongs to the vector space $\mathbb{S}$.</p><p>similar (matrices): Matrices $A$ and $B$ such that $P^{-1} A P=B$, or equivalently, $A=P B P^{-1}$, for some invertible matrix $P$.</p><p>similarity transformation: A transformation that changes $A$ into $P^{-1} A P$.</p><p>simplex: The convex hull of an affinely independent finite set of vectors in $\mathbb{R}^{n}$.</p><p>singular (matrix): A square matrix that has no inverse.</p><p>singular value decomposition (of an $m \times n$ matrix $A$ ): $A=$ $U \Sigma V^{T}$, where $U$ is an $m \times m$ orthogonal matrix, $V$ is an $n \times n$ orthogonal matrix, and $\Sigma$ is an $m \times n$ matrix with nonnegative entries on the main diagonal (arranged in decreasing order of magnitude) and zeros elsewhere. If $\operatorname{rank} A=r$, then $\Sigma$ has exactly $r$ positive entries (the nonzero singular values of $A$ ) on the diagonal.</p><p>singular values (of $A$ ): The (positive) square roots of the eigenvalues of $A^{T} A$, arranged in decreasing order of magnitude.</p><p>size (of a matrix): Two numbers, written in the form $m \times n$, that specify the number of rows $(m)$ and columns $(n)$ in the matrix.</p><p>solution (of a linear system involving variables $x_{1}, \ldots, x_{n}$ ): A list $\left(s_{1}, s_{2}, \ldots, s_{n}\right)$ of numbers that makes each equation in the system a true statement when the values $s_{1}, \ldots, s_{n}$ are substituted for $x_{1}, \ldots, x_{n}$, respectively.</p><p>solution set: The set of all possible solutions of a linear system. The solution set is empty when the linear system is inconsistent.</p><p>$\operatorname{Span}\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{\boldsymbol{p}}\right}$ : The set of all linear combinations of $\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}$. Also, the subspace spanned (or generated) by $\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}$.</p><p>spanning set (for a subspace $H$ ): Any set $\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$ in $H$ such that $H=\operatorname{Span}\left{\mathbf{v}_{1}, \ldots, \mathbf{v}_{p}\right}$.</p><p>spectral decomposition (of $A$ ): A representation</p><p>$$
A=\lambda_{1} \mathbf{u}_{1} \mathbf{u}_{1}^{T}+\cdots+\lambda_{n} \mathbf{u}_{n} \mathbf{u}_{n}^{T}
$$</p><p>where $\left{\mathbf{u}_{1}, \ldots, \mathbf{u}_{n}\right}$ is an orthonormal basis of eigenvectors of $A$, and $\lambda_{1}, \ldots, \lambda_{n}$ are the corresponding eigenvalues of $A$.</p><p>spiral point (of a dynamical system in $\mathbb{R}^{2}$ ): The origin when the trajectories spiral about $\mathbf{0}$.</p><p>stage-matrix model: A difference equation $\mathbf{x}_{k+1}=A \mathbf{x}_{k}$ where $\mathbf{x}_{k}$ lists the number of females in a population at time $k$, with the females classified by various stages of development (such as juvenile, subadult, and adult).</p><p>standard basis: The basis $\mathcal{E}=\left{\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right}$ for $\mathbb{R}^{n}$ consisting of the columns of the $n \times n$ identity matrix, or the basis $\left{1, t, \ldots, t^{n}\right}$ for $\mathbb{P}_{n}$.</p><p>standard matrix (for a linear transformation $T$ ): The matrix $A$ such that $T(\mathbf{x})=A \mathbf{x}$ for all $\mathbf{x}$ in the domain of $T$.</p><p>standard position: The position of the graph of an equation $\mathbf{x}^{T} A \mathbf{x}=c$, when $A$ is a diagonal matrix.</p><p>state vector: A probability vector. In general, a vector that describes the &ldquo;state&rdquo; of a physical system, often in connection with a difference equation $\mathbf{x}_{k+1}=A \mathbf{x}_{k}$.</p><p>steady-state vector (for a stochastic matrix $P$ ): A probability vector $\mathbf{q}$ such that $P \mathbf{q}=\mathbf{q}$.</p><p>stiffness matrix: The inverse of a flexibility matrix. The $j$ th column of a stiffness matrix gives the loads that must be applied at specified points on an elastic beam in order to produce a unit deflection at the $j$ th point on the beam.</p><p>stochastic matrix: A square matrix whose columns are probability vectors.</p><p>strictly dominant eigenvalue: An eigenvalue $\lambda_{1}$ of a matrix $A$ with the property that $\left|\lambda_{1}\right|>\left|\lambda_{k}\right|$ for all other eigenvalues $\lambda_{k}$ of $A$. submatrix (of $A$ ): Any matrix obtained by deleting some rows and/or columns of $A$; also, $A$ itself.</p><p>subspace: A subset $H$ of some vector space $V$ such that $H$ has these properties: (1) the zero vector of $V$ is in $H$; (2) $H$ is closed under vector addition; and (3) $H$ is closed under multiplication by scalars.</p><p>supporting hyperplane (to a compact convex set $S$ in $\mathbb{R}^{n}$ ): A hyperplane $H=[f: d]$ such that $H \cap S \neq \varnothing$ and either $f(x) \leq d$ for all $x$ in $S$ or $f(x) \geq d$ for all $x$ in $S$.</p><p>symmetric matrix: A matrix $A$ such that $A^{T}=A$.</p><p>system of linear equations (or a linear system): A collection of one or more linear equations involving the same set of variables, say, $x_{1}, \ldots, x_{n}$.</p><p>tetrahedron: A three-dimensional solid object bounded by four equal triangular faces, with three faces meeting at each vertex.</p><p>total variance: The trace of the covariance matrix $S$ of a matrix of observations.</p><p>trace (of a square matrix $A$ ): The sum of the diagonal entries in $A$, denoted by $\operatorname{tr} A$.</p><p>trajectory: The graph of a solution $\left{\mathbf{x}_{0}, \mathbf{x}_{1}, \mathbf{x}_{2}, \ldots\right}$ of a dynamical system $\mathbf{x}_{k+1}=A \mathbf{x}_{k}$, often connected by a thin curve to make the trajectory easier to see. Also, the graph of $\mathbf{x}(t)$ for $t \geq 0$, when $\mathbf{x}(t)$ is a solution of a differential equation $\mathbf{x}^{\prime}(t)=A \mathbf{x}(t)$</p><p>transfer matrix: A matrix $A$ associated with an electrical circuit having input and output terminals, such that the output vector is $A$ times the input vector.</p><p>transformation (or function, or mapping) $T$ from $\mathbb{R}^{n}$ to $\mathbb{R}^{\boldsymbol{m}}:$ A rule that assigns to each vector $\mathbf{x}$ in $\mathbb{R}^{n}$ a unique vector $T(\mathbf{x})$ in $\mathbb{R}^{m}$. Notation: $T: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$. Also, $T: V \rightarrow W$ denotes a rule that assigns to each $\mathbf{x}$ in $V$ a unique vector $T(\mathbf{x})$ in $W$.</p><p>translation (by a vector $\mathbf{p}$ ): The operation of adding $\mathbf{p}$ to a vector or to each vector in a given set.</p><p>transpose (of $A$ ): An $n \times m$ matrix $A^{T}$ whose columns are the corresponding rows of the $m \times n$ matrix $A$.</p><p>trend analysis: The use of orthogonal polynomials to fit data, with the inner product given by evaluation at a finite set of points.</p><p>triangle inequality: $|\mathbf{u}+\mathbf{v}| \leq|\mathbf{u}|+|\mathbf{v}|$ for all $\mathbf{u}, \mathbf{v}$.</p><p>triangular matrix: A matrix $A$ with either zeros above or zeros below the diagonal entries.</p><p>trigonometric polynomial: A linear combination of the constant function 1 and sine and cosine functions such as $\cos n t$ and $\sin n t$.</p><p>trivial solution: The solution $\mathbf{x}=\mathbf{0}$ of a homogeneous equation $A \mathbf{x}=\mathbf{0}$.</p><p>uncorrelated variables: Any two variables $x_{i}$ and $x_{j}$ (with $i \neq j$ ) that range over the $i$ th and $j$ th coordinates of the observation vectors in an observation matrix, such that the covariance $s_{i j}$ is zero.</p><p>underdetermined system: A system of equations with fewer equations than unknowns.</p><p>uniqueness question: Asks, &ldquo;If a solution of a system exists, is it unique-that is, is it the only one?&rdquo;</p><p>unit consumption vector: A column vector in the Leontief input-output model that lists the inputs a sector needs for each unit of its output; a column of the consumption matrix.</p><p>unit lower triangular matrix: A square lower triangular matrix with ones on the main diagonal.</p><p>unit vector: A vector $\mathbf{v}$ such that $|\mathbf{v}|=1$.</p><p>upper triangular matrix: A matrix $U$ (not necessarily square) with zeros below the diagonal entries $u_{11}, u_{22}, \ldots$</p><p>Vandermonde matrix: An $n \times n$ matrix $V$ or its transpose, when $V$ has the form</p><p>$$
V=\left[\begin{array}{ccccc}
1 & x_{1} & x_{1}^{2} & \cdots & x_{1}^{n-1} \\ 1 & x_{2} & x_{2}^{2} & \cdots & x_{2}^{n-1} \\ \vdots & \vdots & \vdots & & \vdots \\ 1 & x_{n} & x_{n}^{2} & \cdots & x_{n}^{n-1}
\end{array}\right]
$$</p><p>variance (of a variable $x_{j}$ ): The diagonal entry $s_{j j}$ in the covariance matrix $S$ for a matrix of observations, where $x_{j}$ varies over the $j$ th coordinates of the observation vectors.</p><p>vector: A list of numbers; a matrix with only one column. In general, any element of a vector space.</p><p>vector addition: Adding vectors by adding corresponding entries.</p><p>vector equation: An equation involving a linear combination of vectors with undetermined weights.</p><p>vector space: A set of objects, called vectors, on which two operations are defined, called addition and multiplication by scalars. Ten axioms must be satisfied. See the first definition in Section 4.1.</p><p>vector subtraction: Computing $\mathbf{u}+(-1) \mathbf{v}$ and writing the result as $\mathbf{u}-\mathbf{v}$.</p><p>weighted least squares: Least-squares problems with a weighted inner product such as</p><p>$$
\langle\mathbf{x}, \mathbf{y}\rangle=w_{1}^{2} x_{1} y_{1}+\cdots+w_{n}^{2} x_{n} y_{n} .
$$</p><p>weights: The scalars used in a linear combination.</p><p>zero subspace: The subspace ${\boldsymbol{0}}$ consisting of only the zero vector.</p><p>zero vector: The unique vector, denoted by $\mathbf{0}$, such that $\mathbf{u}+\mathbf{0}=\mathbf{u}$ for all $\mathbf{u}$. In $\mathbb{R}^{n}, \mathbf{0}$ is the vector whose entries are all zeros.</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/quartz// data-ctx=glossary data-src=/ class=internal-link>Linear Algebra Concept Map</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://yanjinhai.github.io/quartz/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Jinhai Yan using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><ul><li><a href=https://yanjinhai.github.io/quartz/>Home</a></li><li><a href=https://github.com/yanjinhai>Github</a></li></ul></footer></div></div></body></html>