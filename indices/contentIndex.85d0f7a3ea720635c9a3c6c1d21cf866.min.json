{"/":{"title":"Linear Algebra Concept Map","content":"\nThis is a WIP concept map of the topics covered in linear algebra.\n\nUse the search bar to get started.\n\n[glossary](glossary)\n","lastmodified":"2022-12-23T17:10:38.478086923Z","tags":null},"/glossary":{"title":"","content":"adjugate (or classical adjoint): The matrix adj $A$ formed from a square matrix $A$ by replacing the $(i, j)$-entry of $A$ by the $(i, j)$-cofactor, for all $i$ and $j$, and then transposing the resulting matrix.\n\naffine combination: A linear combination of vectors (points in $\\mathbb{R}^{n}$ ) in which the sum of the weights involved is 1 .\n\naffine dependence relation: An equation of the form $c_{1} \\mathbf{v}_{1} + $ $\\cdots+c_{p} \\mathbf{v}_{p}=\\mathbf{0}$, where the weights $c_{1}, \\ldots, c_{p}$ are not all zero, and $c_{1}+\\cdots+c_{p}=0$.\n\naffine hull (or affine span) of a set $S$ : The set of all affine combinations of points in $S$, denoted by aff $S$.\n\naffinely dependent set: A set $\\left\\{\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{p}\\right\\}$ in $\\mathbb{R}^{n}$ such that there are real numbers $c_{1}, \\ldots, c_{p}$, not all zero, such that $c_{1}+\\cdots+$ $c_{p}=0$ and $c_{1} \\mathbf{v}_{1}+\\cdots+c_{p} \\mathbf{v}_{p}=\\mathbf{0}$.\n\naffinely independent set: A set $\\left\\{\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{p}\\right\\}$ in $\\mathbb{R}^{n}$ that is not affinely dependent.\n\naffine set (or affine subset): A set $S$ of points such that if $\\mathbf{p}$ and $\\mathbf{q}$ are in $S$, then $(1-t) \\mathbf{p}+t \\mathbf{q} \\in S$ for each real number $t$.\n\naffine transformation: A mapping $T: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ of the form $T(\\mathbf{x})=A \\mathbf{x}+\\mathbf{b}$, with $A$ an $m \\times n$ matrix and $\\mathbf{b}$ in $\\mathbb{R}^{m}$.\n\nalgebraic multiplicity: The multiplicity of an eigenvalue as a root of the characteristic equation.\n\nangle (between nonzero vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^{2}$ or $\\mathbb{R}^{3}$ ): The angle $\\vartheta$ between the two directed line segments from the origin to the points $\\mathbf{u}$ and $\\mathbf{v}$. Related to the scalar product by\n\n$$\n\\mathbf{u} \\cdot \\mathbf{v}=\\|\\mathbf{u}\\|\\|\\mathbf{v}\\| \\cos \\vartheta\n$$\n\nassociative law of multiplication: $\\quad A(B C)=(A B) C$, for all $A$, $B, C$.\n\nattractor (of a dynamical system in $\\mathbb{R}^{2}$ ): The origin when all trajectories tend toward $\\mathbf{0}$.\n\naugmented matrix: A matrix made up of a coefficient matrix for a linear system and one or more columns to the right. Each extra column contains the constants from the right side of a system with the given coefficient matrix. \n\nauxiliary equation: A polynomial equation in a variable $r$, created from the coefficients of a homogeneous difference equation.\n\nback-substitution (with matrix notation): The backward phase of row reduction of an augmented matrix that transforms an echelon matrix into a reduced echelon matrix; used to find the solution(s) of a system of linear equations.\n\nbackward phase (of row reduction): The last part of the algorithm that reduces a matrix in echelon form to a reduced echelon form.\n\nband matrix: A matrix whose nonzero entries lie within a band along the main diagonal.\n\nbarycentric coordinates (of a point $\\mathbf{p}$ with respect to an affinely independent set $S=\\left\\{\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{k}\\right\\}$ ): The (unique) set of weights $c_{1}, \\ldots, c_{k}$ such that $\\mathbf{p}=c_{1} \\mathbf{v}_{1}+\\cdots+c_{k} \\mathbf{v}_{k}$ and $c_{1}+$ $\\cdots+c_{k}=1$. (Sometimes also called the affine coordinates of $\\mathbf{p}$ with respect to $S$.)\n\nbasic variable: A variable in a linear system that corresponds to a pivot column in the coefficient matrix.\n\nbasis (for a nontrivial subspace $H$ of a vector space $V$ ): An indexed set $\\mathcal{B}=\\left\\{\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{p}\\right\\}$ in $V$ such that: (i) $\\mathcal{B}$ is a linearly independent set and (ii) the subspace spanned by $\\mathcal{B}$ coincides with $H$, that is, $H=\\operatorname{Span}\\left\\{\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{p}\\right\\}$.\n\n$\\mathcal{B}$-coordinates of $\\mathbf{x}$ : See coordinates of $\\mathbf{x}$ relative to the basis $\\mathcal{B}$.\n\nbest approximation: The closest point in a given subspace to a given vector.\n\nbidiagonal matrix: A matrix whose nonzero entries lie on the main diagonal and on one diagonal adjacent to the main diagonal.\n\nblock diagonal (matrix): A partitioned matrix $A=\\left[A_{i j}\\right]$ such that each block $A_{i j}$ is a zero matrix for $i \\neq j$.\n\nblock matrix: See partitioned matrix.\n\nblock matrix multiplication: The row-column multiplication of partitioned matrices as if the block entries were scalars. block upper triangular (matrix): A partitioned matrix $A=\\left[A_{i j}\\right]$ such that each block $A_{i j}$ is a zero matrix for $i\u003ej$.\n\nboundary point of a set $S$ in $\\mathbb{R}^{n}$ : A point $\\mathbf{p}$ such that every open ball in $\\mathbb{R}^{n}$ centered at $\\mathbf{p}$ intersects both $S$ and the complement of $S$.\n\nbounded set in $\\mathbb{R}^{n}$ : A set that is contained in an open ball $B(\\mathbf{0}, \\delta)$ for some $\\delta\u003e0$.\n\n$\\mathcal{B}$-matrix (for $T$ ): A matrix $[T]_{\\mathcal{B}}$ for a linear transformation $T: V \\rightarrow V$ relative to a basis $\\mathcal{B}$ for $V$, with the property that $[T(\\mathbf{x})]_{\\mathcal{B}}=[T]_{\\mathcal{B}}[\\mathbf{x}]_{\\mathcal{B}}$ for all $\\mathbf{x}$ in $V$.\n\nCauchy-Schwarz inequality: $|\\langle\\mathbf{u}, \\mathbf{v}\\rangle| \\leq\\|u\\| \\cdot\\|v\\|$ for all u, $\\mathbf{v}$. change of basis: See change-of-coordinates matrix.\n\nchange-of-coordinates matrix (from a basis $\\mathcal{B}$ to a basis $\\mathcal{C}$ ): A matrix $\\underset{\\mathcal{C} \\leftarrow \\mathcal{B}}{P}$ that transforms $\\mathcal{B}$-coordinate vectors into $\\mathcal{C}$ coordinate vectors: $[\\mathbf{x}]_{\\mathcal{C}}={ }_{\\mathcal{C} \\leftarrow \\mathcal{B}}^{P}[\\mathbf{x}]_{\\mathcal{B}}$. If $\\mathcal{C}$ is the standard basis for $\\mathbb{R}^{n}$, then ${ }_{\\mathcal{C} \\leftarrow \\mathcal{B}}$ is sometimes written as $P_{\\mathcal{B}}$.\n\ncharacteristic equation (of $A$ ): $\\quad \\operatorname{det}(A-\\lambda I)=0$.\n\ncharacteristic polynomial (of $A$ ): $\\operatorname{det}(A-\\lambda I$ ) or, in some texts, $\\operatorname{det}(\\lambda I-A)$.\n\nCholesky factorization: A factorization $A=R^{T} R$, where $R$ is an invertible upper triangular matrix whose diagonal entries are all positive.\n\nclosed ball (in $\\mathbb{R}^{n}$ ): A set $\\{\\mathbf{x}:\\|\\mathbf{x}-\\mathbf{p}\\|\u003c\\delta\\}$ in $\\mathbb{R}^{n}$, where $\\mathbf{p}$ is in $\\mathbb{R}^{n}$ and $\\delta\u003e0$.\n\nclosed set (in $\\mathbb{R}^{n}$ ): A set that contains all of its boundary points. codomain (of a transformation $T: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ ): The set $\\mathbb{R}^{m}$ that contains the range of $T$. In general, if $T$ maps a vector space $V$ into a vector space $W$, then $W$ is called the codomain of $T$.\n\ncoefficient matrix: A matrix whose entries are the coefficients of a system of linear equations.\n\ncofactor: A number $C_{i j}=(-1)^{i+j} \\operatorname{det} A_{i j}$, called the $(i, j)$ cofactor of $A$, where $A_{i j}$ is the submatrix formed by deleting the $i$ th row and the $j$ th column of $A$.\n\ncofactor expansion: A formula for $\\operatorname{det} A$ using cofactors associated with one row or one column, such as for row 1:\n\n$$\n\\operatorname{det} A=a_{11} C_{11}+\\cdots+a_{1 n} C_{1 n}\n$$\n\ncolumn-row expansion: The expression of a product $A B$ as a sum of outer products: $\\operatorname{col}_{1}(A) \\operatorname{row}_{1}(B)+\\cdots+$ $\\operatorname{col}_{n}(A)$ row $_{n}(B)$, where $n$ is the number of columns of $A$.\n\ncolumn space (of an $m \\times n$ matrix $A$ ): The set $\\operatorname{Col} A$ of all linear combinations of the columns of $A$. If $A=\\left[\\mathbf{a}_{1} \\cdots \\mathbf{a}_{n}\\right]$, then $\\operatorname{Col} A=\\operatorname{Span}\\left\\{\\mathbf{a}_{1}, \\ldots, \\mathbf{a}_{n}\\right\\}$. Equivalently,\n\n$$\n\\operatorname{Col} A=\\left\\{\\mathbf{y}: \\mathbf{y}=A \\mathbf{x} \\text { for some } \\mathbf{x} \\text { in } \\mathbb{R}^{n}\\right\\}\n$$\n\ncolumn sum: The sum of the entries in a column of a matrix. column vector: A matrix with only one column, or a single column of a matrix that has several columns.\n\ncommuting matrices: Two matrices $A$ and $B$ such that $A B=B A$.\n\ncompact set (in $\\mathbb{R}^{n}$ ): A set in $\\mathbb{R}^{n}$ that is both closed and bounded.\n\ncompanion matrix: A special form of matrix whose characteristic polynomial is $(-1)^{n} p(\\lambda)$ when $p(\\lambda)$ is a specified polynomial whose leading term is $\\lambda^{n}$.\n\ncomplex eigenvalue: A nonreal root of the characteristic equation of an $n \\times n$ matrix.\n\ncomplex eigenvector: A nonzero vector $\\mathbf{x}$ in $\\mathbb{C}^{n}$ such that $A \\mathbf{x}=\\lambda \\mathbf{x}$, where $A$ is an $n \\times n$ matrix and $\\lambda$ is a complex eigenvalue.\n\ncomponent of $\\mathbf{y}$ orthogonal to $\\mathbf{u}$ (for $\\mathbf{u} \\neq \\mathbf{0}$ ): The vector $\\mathbf{y}-\\frac{\\mathbf{y} \\cdot \\mathbf{u}}{\\mathbf{u} \\cdot \\mathbf{u}} \\mathbf{u}$.\n\ncomposition of linear transformations: A mapping produced by applying two or more linear transformations in succession. If the transformations are matrix transformations, say left-multiplication by $B$ followed by left-multiplication by $A$, then the composition is the mapping $\\mathbf{x} \\mapsto A(B \\mathbf{x})$.\n\ncondition number (of $A$ ): The quotient $\\sigma_{1} / \\sigma_{n}$, where $\\sigma_{1}$ is the largest singular value of $A$ and $\\sigma_{n}$ is the smallest singular value. The condition number is $+\\infty$ when $\\sigma_{n}$ is zero.\n\nconformable for block multiplication: Two partitioned matrices $A$ and $B$ such that the block product $A B$ is defined: The column partition of $A$ must match the row partition of $B$.\n\nconsistent linear system: A linear system with at least one solution.\n\nconstrained optimization: The problem of maximizing a quantity such as $\\mathbf{x}^{T} A \\mathbf{x}$ or $\\|A \\mathbf{x}\\|$ when $\\mathbf{x}$ is subject to one or more constraints, such as $\\mathbf{x}^{T} \\mathbf{x}=1$ or $\\mathbf{x}^{T} \\mathbf{v}=0$.\n\nconsumption matrix: A matrix in the Leontief input-output model whose columns are the unit consumption vectors for the various sectors of an economy.\n\ncontraction: A mapping $\\mathbf{x} \\mapsto r \\mathbf{x}$ for some scalar $r$, with $0 \\leq r \\leq 1$\n\ncontrollable (pair of matrices): A matrix pair $(A, B)$ where $A$ is $n \\times n, B$ has $n$ rows, and\n\n$$\n\\operatorname{rank}\\left[\\begin{array}{lllll}\nB \u0026 A B \u0026 A^{2} B \u0026 \\cdots \u0026 A^{n-1} B\n\\end{array}\\right]=n\n$$\n\nRelated to a state-space model of a control system and the difference equation $\\mathbf{x}_{k+1}=A \\mathbf{x}_{k}+B \\mathbf{u}_{k}(k=0,1, \\ldots)$.\n\nconvergent (sequence of vectors): A sequence $\\left\\{\\mathbf{x}_{k}\\right\\}$ such that the entries in $\\mathbf{x}_{k}$ can be made as close as desired to the entries in some fixed vector for all $k$ sufficiently large.\n\nconvex combination (of points $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{k}$ in $\\mathbb{R}^{n}$ ): A linear combination of vectors (points) in which the weights in the combination are nonnegative and the sum of the weights is 1.\n\nconvex hull (of a set $S$ ): The set of all convex combinations of points in $S$, denoted by: conv $S$. convex set: A set $S$ with the property that for each $\\mathbf{p}$ and $\\mathbf{q}$ in $S$, the line segment $\\overline{\\mathbf{p q}}$ is contained in $S$.\n\ncoordinate mapping (determined by an ordered basis $\\mathcal{B}$ in a vector space $V$ ): A mapping that associates to each $\\mathbf{x}$ in $V$ its coordinate vector $[\\mathbf{x}]_{\\mathcal{B}}$.\n\ncoordinates of $x$ relative to the basis $\\mathcal{B}=\\left\\{\\mathbf{b}_{1}, \\ldots, \\mathbf{b}_{\\boldsymbol{n}}\\right\\}$ : The weights $c_{1}, \\ldots, c_{n}$ in the equation $\\mathbf{x}=c_{1} \\mathbf{b}_{1}+\\cdots+c_{n} \\mathbf{b}_{n}$.\n\ncoordinate vector of $\\mathbf{x}$ relative to $\\mathcal{B}$ : The vector $[\\mathbf{x}]_{\\mathcal{B}}$ whose entries are the coordinates of $\\mathbf{x}$ relative to the basis $\\mathcal{B}$.\n\ncovariance (of variables $x_{i}$ and $x_{j}$, for $i \\neq j$ ): The entry $s_{i j}$ in the covariance matrix $S$ for a matrix of observations, where $x_{i}$ and $x_{j}$ vary over the $i$ th and $j$ th coordinates, respectively, of the observation vectors.\n\ncovariance matrix (or sample covariance matrix): The $p \\times p$ matrix $S$ defined by $S=(N-1)^{-1} B B^{T}$, where $B$ is a $p \\times N$ matrix of observations in mean-deviation form.\n\nCramer's rule: A formula for each entry in the solution $\\mathbf{x}$ of the equation $A \\mathbf{x}=\\mathbf{b}$ when $A$ is an invertible matrix.\n\ncross-product term: A term $c x_{i} x_{j}$ in a quadratic form, with $i \\neq j$.\n\ncube: A three-dimensional solid object bounded by six square faces, with three faces meeting at each vertex.\n\ndecoupled system: A difference equation $\\mathbf{y}_{k+1}=A \\mathbf{y}_{k}$, or a differential equation $\\mathbf{y}^{\\prime}(t)=A \\mathbf{y}(t)$, in which $A$ is a diagonal matrix. The discrete evolution of each entry in $\\mathbf{y}_{k}$ (as a function of $k$ ), or the continuous evolution of each entry in the vector-valued function $\\mathbf{y}(t)$, is unaffected by what happens to the other entries as $k \\rightarrow \\infty$ or $t \\rightarrow \\infty$.\n\ndesign matrix: The matrix $X$ in the linear model $\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}$, where the columns of $X$ are determined in some way by the observed values of some independent variables.\n\ndeterminant (of a square matrix $A$ ): The number $\\operatorname{det} A$ defined inductively by a cofactor expansion along the first row of $A$. Also, $(-1)^{r}$ times the product of the diagonal entries in any echelon form $U$ obtained from $A$ by row replacements and $r$ row interchanges (but no scaling operations).\n\ndiagonal entries (in a matrix): Entries having equal row and column indices.\n\ndiagonalizable (matrix): A matrix that can be written in factored form as $P D P^{-1}$, where $D$ is a diagonal matrix and $P$ is an invertible matrix.\n\ndiagonal matrix: A square matrix whose entries not on the main diagonal are all zero.\n\ndifference equation (or linear recurrence relation): An equation of the form $\\mathbf{x}_{k+1}=A \\mathbf{x}_{k}(k=0,1,2, \\ldots)$ whose solution is a sequence of vectors, $\\mathbf{x}_{0}, \\mathbf{x}_{1}, \\ldots$\n\ndilation: A mapping $\\mathbf{x} \\mapsto r \\mathbf{x}$ for some scalar $r$, with $1\u003cr$. dimension:\n\nof a flat $S$ : The dimension of the corresponding parallel subspace.\n\nof a set $S$ : The dimension of the smallest flat containing $S$. of a subspace $S$ : The number of vectors in a basis for $S$, written as $\\operatorname{dim} S$.\n\nof a vector space $V$ : The number of vectors in a basis for $V$, written as $\\operatorname{dim} V$. The dimension of the zero space is 0 .\n\ndiscrete linear dynamical system: A difference equation of the form $\\mathbf{x}_{k+1}=A \\mathbf{x}_{k}$ that describes the changes in a system (usually a physical system) as time passes. The physical system is measured at discrete times, when $k=0,1,2, \\ldots$, and the state of the system at time $k$ is a vector $\\mathbf{x}_{k}$ whose entries provide certain facts of interest about the system.\n\ndistance between $\\mathbf{u}$ and $\\mathbf{v}$ : The length of the vector $\\mathbf{u}-\\mathbf{v}$, denoted by dist $(\\mathbf{u}, \\mathbf{v})$.\n\ndistance to a subspace: The distance from a given point (vector) $\\mathbf{v}$ to the nearest point in the subspace.\n\ndistributive laws: (left) $A(B+C)=A B+A C$, and (right) $(B+C) A=B A+C A$, for all $A, B, C$.\n\ndomain (of a transformation $T$ ): The set of all vectors $\\mathbf{x}$ for which $T(\\mathbf{x})$ is defined.\n\ndot product: See inner product.\n\ndynamical system: See discrete linear dynamical system.\n\nechelon form (or row echelon form, of a matrix): An echelon matrix that is row equivalent to the given matrix.\n\nechelon matrix (or row echelon matrix): A rectangular matrix that has three properties: (1) All nonzero rows are above any row of all zeros. (2) Each leading entry of a row is in a column to the right of the leading entry of the row above it. (3) All entries in a column below a leading entry are zero.\n\neigenfunctions (of a differential equation $\\mathbf{x}^{\\prime}(t)=A \\mathbf{x}(t)$ ): $\\quad \\mathrm{A}$ function $\\mathbf{x}(t)=\\mathbf{v} e^{\\lambda t}$, where $\\mathbf{v}$ is an eigenvector of $A$ and $\\lambda$ is the corresponding eigenvalue.\n\neigenspace (of $A$ corresponding to $\\lambda$ ): The set of all solutions of $A \\mathbf{x}=\\lambda \\mathbf{x}$, where $\\lambda$ is an eigenvalue of $A$. Consists of the zero vector and all eigenvectors corresponding to $\\lambda$.\n\neigenvalue (of $A$ ): A scalar $\\lambda$ such that the equation $A \\mathbf{x}=\\lambda \\mathbf{x}$ has a solution for some nonzero vector $\\mathbf{x}$.\n\neigenvector (of $A$ ): A nonzero vector $\\mathbf{x}$ such that $A \\mathbf{x}=\\lambda \\mathbf{x}$ for some scalar $\\lambda$.\n\neigenvector basis: A basis consisting entirely of eigenvectors of a given matrix.\n\neigenvector decomposition (of $\\mathbf{x}$ ): An equation, $\\mathbf{x}=c_{1} \\mathbf{v}_{1}+$ $\\cdots+c_{n} \\mathbf{v}_{n}$, expressing $\\mathbf{x}$ as a linear combination of eigenvectors of a matrix.\n\nelementary matrix: An invertible matrix that results by performing one elementary row operation on an identity matrix.\n\nelementary row operations: (1) (Replacement) Replace one row by the sum of itself and a multiple of another row. (2) Interchange two rows. (3) (Scaling) Multiply all entries in a row by a nonzero constant.\n\nequal vectors: Vectors in $\\mathbb{R}^{n}$ whose corresponding entries are the same. equilibrium prices: A set of prices for the total output of the various sectors in an economy, such that the income of each sector exactly balances its expenses.\n\nequilibrium vector: See steady-state vector.\n\nequivalent (linear) systems: Linear systems with the same solution set.\n\nexchange model: See Leontief exchange model.\n\nexistence question: Asks, \"Does a solution to the system exist?\" That is, \"Is the system consistent?\" Also, \"Does a solution of $A \\mathbf{x}=\\mathbf{b}$ exist for all possible $\\mathbf{b}$ ?\"\n\nexpansion by cofactors: See cofactor expansion.\n\nexplicit description (of a subspace $W$ of $\\mathbb{R}^{n}$ ): A parametric representation of $W$ as the set of all linear combinations of a set of specified vectors.\n\nextreme point (of a convex set $S$ ): A point $\\mathbf{p}$ in $S$ such that $\\mathbf{p}$ is not in the interior of any line segment that lies in $S$. (That is, if $\\mathbf{x}, \\mathbf{y}$ are in $S$ and $\\mathbf{p}$ is on the line segment $\\overline{\\mathbf{x y}}$, then $\\mathbf{p}=\\mathbf{x}$ or $\\mathbf{p}=\\mathbf{y}$.\n\nfactorization ( of $A$ ): An equation that expresses $A$ as a product of two or more matrices.\n\nfinal demand vector (or bill of final demands): The vector d in the Leontief input-output model that lists the dollar values of the goods and services demanded from the various sectors by the nonproductive part of the economy. The vector $\\mathbf{d}$ can represent consumer demand, government consumption, surplus production, exports, or other external demand.\n\nfinite-dimensional (vector space): A vector space that is spanned by a finite set of vectors.\n\nflat $\\left(\\right.$ in $\\mathbb{R}^{n}$ ): A translate of a subspace of $\\mathbb{R}^{n}$.\n\nflexibility matrix: A matrix whose $j$ th column gives the deflections of an elastic beam at specified points when a unit force is applied at the $j$ th point on the beam.\n\nfloating point arithmetic: Arithmetic with numbers represented as decimals $\\pm . d_{1} \\cdots d_{p} \\times 10^{r}$, where $r$ is an integer and the number $p$ of digits to the right of the decimal point is usually between 8 and 16 .\n\nflop: One arithmetic operation $(+,-, *, /)$ on two real floating point numbers.\n\nforward phase (of row reduction): The first part of the algorithm that reduces a matrix to echelon form.\n\nFourier approximation (of order $n$ ): The closest point in the subspace of $n$ th-order trigonometric polynomials to a given function in $C[0,2 \\pi]$.\n\nFourier coefficients: The weights used to make a trigonometric polynomial as a Fourier approximation to a function.\n\nFourier series: An infinite series that converges to a function in the inner product space $C[0,2 \\pi]$, with the inner product given by a definite integral.\n\nfree variable: Any variable in a linear system that is not a basic variable. full rank (matrix): An $m \\times n$ matrix whose rank is the smaller of $m$ and $n$.\n\nfundamental set of solutions: A basis for the set of all solutions of a homogeneous linear difference or differential equation.\n\nfundamental subspaces (determined by $A$ ): The null space and column space of $A$, and the null space and column space of $A^{T}$, with $\\operatorname{Col} A^{T}$ commonly called the row space of $A$.\n\nGaussian elimination: See row reduction algorithm.\n\ngeneral least-squares problem: Given an $m \\times n$ matrix $A$ and a vector $\\mathbf{b}$ in $\\mathbb{R}^{m}$, find $\\hat{\\mathbf{x}}$ in $\\mathbb{R}^{n}$ such that $\\|\\mathbf{b}-A \\hat{\\mathbf{x}}\\| \\leq\\|\\mathbf{b}-A \\mathbf{x}\\|$ for all $\\mathbf{x}$ in $\\mathbb{R}^{n}$.\n\ngeneral solution (of a linear system): A parametric description of a solution set that expresses the basic variables in terms of the free variables (the parameters), if any. After Section 1.5, the parametric description is written in vector form.\n\nGivens rotation: A linear transformation from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{n}$ used in computer programs to create zero entries in a vector (usually a column of a matrix).\n\nGram matrix (of $A$ ): The matrix $A^{T} A$.\n\nGram-Schmidt process: An algorithm for producing an orthogonal or orthonormal basis for a subspace that is spanned by a given set of vectors.\n\nhomogeneous coordinates: In $\\mathbb{R}^{3}$, the representation of $(x, y, z)$ as $(X, Y, Z, H)$ for any $H \\neq 0$, where $x=X / H$, $y=Y / H$, and $z=Z / H$. In $\\mathbb{R}^{2}, H$ is usually taken as 1 , and the homogeneous coordinates of $(x, y)$ are written as $(x, y, 1)$.\n\nhomogeneous equation: An equation of the form $A \\mathbf{x}=\\mathbf{0}$, possibly written as a vector equation or as a system of linear equations.\n\nhomogeneous form of (a vector) $\\mathbf{v}$ in $\\mathbb{R}^{n}:$ The point $\\tilde{\\mathbf{v}}=\\left[\\begin{array}{l}\\mathbf{v} \\\\ 1\\end{array}\\right]$ in $\\mathbb{R}^{n+1}$.\n\nHouseholder reflection: A transformation $\\mathbf{x} \\mapsto Q \\mathbf{x}$, where $Q=I-2 \\mathbf{u u}^{T}$ and $\\mathbf{u}$ is a unit vector $\\left(\\mathbf{u}^{T} \\mathbf{u}=1\\right)$.\n\nhyperplane (in $\\mathbb{R}^{n}$ ): A flat in $\\mathbb{R}^{n}$ of dimension $n-1$. Also: a translate of a subspace of dimension $n-1$.\n\nidentity matrix (denoted by $I$ or $I_{n}$ ): A square matrix with ones on the diagonal and zeros elsewhere.\n\nill-conditioned matrix: A square matrix with a large (or possibly infinite) condition number; a matrix that is singular or can become singular if some of its entries are changed ever so slightly.\n\nimage (of a vector $\\mathbf{x}$ under a transformation $T$ ): The vector $T(\\mathbf{x})$ assigned to $\\mathbf{x}$ by $T$. implicit description (of a subspace $W$ of $\\mathbb{R}^{n}$ ): A set of one or more homogeneous equations that characterize the points of $W$.\n\nIm $\\mathbf{x}$ : The vector in $\\mathbb{R}^{n}$ formed from the imaginary parts of the entries of a vector $\\mathbf{x}$ in $\\mathbb{C}^{n}$.\n\ninconsistent linear system: A linear system with no solution.\n\nindefinite matrix: A symmetric matrix $A$ such that $\\mathbf{x}^{T} A \\mathbf{x}$ assumes both positive and negative values.\n\nindefinite quadratic form: A quadratic form $Q$ such that $Q(\\mathbf{x})$ assumes both positive and negative values.\n\ninfinite-dimensional (vector space): A nonzero vector space $V$ that has no finite basis.\n\ninner product: The scalar $\\mathbf{u}^{T} \\mathbf{v}$, usually written as $\\mathbf{u} \\cdot \\mathbf{v}$, where $\\mathbf{u}$ and $\\mathbf{v}$ are vectors in $\\mathbb{R}^{n}$ viewed as $n \\times 1$ matrices. Also called the dot product of $\\mathbf{u}$ and $\\mathbf{v}$. In general, a function on a vector space that assigns to each pair of vectors $\\mathbf{u}$ and $\\mathbf{v}$ a number $\\langle\\mathbf{u}, \\mathbf{v}\\rangle$, subject to certain axioms. See Section 6.7.\n\ninner product space: A vector space on which is defined an inner product.\n\ninput-output matrix: See consumption matrix.\n\ninput-output model: See Leontief input-output model.\n\ninterior point (of a set $S$ in $\\mathbb{R}^{n}$ ): A point $\\mathbf{p}$ in $S$ such that for some $\\delta\u003e0$, the open ball $\\mathbf{B}(\\mathbf{p}, \\delta)$ centered at $\\mathbf{p}$ is contained in $S$.\n\nintermediate demands: Demands for goods or services that will be consumed in the process of producing other goods and services for consumers. If $\\mathbf{x}$ is the production level and $C$ is the consumption matrix, then $C \\mathbf{x}$ lists the intermediate demands.\n\ninterpolating polynomial: A polynomial whose graph passes through every point in a set of data points in $\\mathbb{R}^{2}$.\n\ninvariant subspace (for $A$ ): A subspace $H$ such that $A \\mathbf{x}$ is in $H$ whenever $\\mathbf{x}$ is in $H$.\n\ninverse (of an $n \\times n$ matrix $A$ ): An $n \\times n$ matrix $A^{-1}$ such that $A A^{-1}=A^{-1} A=I_{n}$.\n\ninverse power method: An algorithm for estimating an eigenvalue $\\lambda$ of a square matrix, when a good initial estimate of $\\lambda$ is available.\n\ninvertible linear transformation: A linear transformation $T: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ such that there exists a function $S: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ satisfying both $T(S(\\mathbf{x}))=\\mathbf{x}$ and $S(T(\\mathbf{x}))=\\mathbf{x}$ for all $\\mathbf{x}$ in $\\mathbb{R}^{n}$.\n\ninvertible matrix: A square matrix that possesses an inverse.\n\nisomorphic vector spaces: Two vector spaces $V$ and $W$ for which there is a one-to-one linear transformation $T$ that maps $V$ onto $W$\n\nisomorphism: A one-to-one linear mapping from one vector space onto another.\n\nkernel (of a linear transformation $T: V \\rightarrow W$ ): The set of $\\mathbf{x}$ in $V$ such that $T(\\mathbf{x})=\\mathbf{0}$. \n\nKirchhoff's laws: (1) (voltage law) The algebraic sum of the $R I$ voltage drops in one direction around a loop equals the algebraic sum of the voltage sources in the same direction around the loop. (2) (current law) The current in a branch is the algebraic sum of the loop currents flowing through that branch.\n\nladder network: An electrical network assembled by connecting in series two or more electrical circuits.\n\nleading entry: The leftmost nonzero entry in a row of a matrix. least-squares error: The distance $\\|\\mathbf{b}-A \\hat{\\mathbf{x}}\\|$ from $\\mathbf{b}$ to $A \\hat{\\mathbf{x}}$, when $\\hat{\\mathbf{x}}$ is a least-squares solution of $A \\mathbf{x}=\\mathbf{b}$.\n\nleast-squares line: The line $y=\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x$ that minimizes the least-squares error in the equation $\\mathbf{y}=X \\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}$.\n\nleast-squares solution (of $A \\mathbf{x}=\\mathbf{b}$ ): A vector $\\hat{\\mathbf{x}}$ such that $\\|\\mathbf{b}-A \\hat{\\mathbf{x}}\\| \\leq\\|\\mathbf{b}-A \\mathbf{x}\\|$ for all $\\mathbf{x}$ in $\\mathbb{R}^{n}$\n\nleft inverse (of $A$ ): Any rectangular matrix $C$ such that $C A=I$.\n\nleft-multiplication (by $A$ ): Multiplication of a vector or matrix on the left by $A$.\n\nleft singular vectors (of $A$ ): The columns of $U$ in the singular value decomposition $A=U \\Sigma V^{T}$.\n\nlength (or norm, of $\\mathbf{v}$ ): The scalar $\\|\\mathbf{v}\\|=\\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}=\\sqrt{\\langle\\mathbf{v}, \\mathbf{v}\\rangle}$.\n\nLeontief exchange (or closed) model: A model of an economy where inputs and outputs are fixed, and where a set of prices for the outputs of the sectors is sought such that the income of each sector equals its expenditures. This \"equilibrium\" condition is expressed as a system of linear equations, with the prices as the unknowns.\n\nLeontief input-output model (or Leontief production equation): The equation $\\mathbf{x}=C \\mathbf{x}+\\mathbf{d}$, where $\\mathbf{x}$ is production, $\\mathbf{d}$ is final demand, and $C$ is the consumption (or input-output) matrix. The $j$ th column of $C$ lists the inputs that sector $j$ consumes per unit of output.\n\nlevel set (or gradient) of a linear functional $f$ on $\\mathbb{R}^{n}$ : A set $[f: d]=\\left\\{\\mathbf{x} \\in \\mathbb{R}^{n}: f(\\mathbf{x})=d\\right\\}$\n\nlinear combination: A sum of scalar multiples of vectors. The scalars are called the weights.\n\nlinear dependence relation: A homogeneous vector equation where the weights are all specified and at least one weight is nonzero.\n\nlinear equation (in the variables $x_{1}, \\ldots, x_{n}$ ): An equation that can be written in the form $a_{1} x_{1}+a_{2} x_{2}+\\cdots+a_{n} x_{n}=b$, where $b$ and the coefficients $a_{1}, \\ldots, a_{n}$ are real or complex numbers.\n\nlinear filter: A linear difference equation used to transform discrete-time signals.\n\nlinear functional (on $\\mathbb{R}^{n}$ ): A linear transformation $f$ from $\\mathbb{R}^{n}$ into $\\mathbb{R}$.\n\nlinearly dependent (vectors): An indexed set $\\left\\{\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{p}\\right\\}$ with the property that there exist weights $c_{1}, \\ldots, c_{p}$, not all zero, such that $c_{1} \\mathbf{v}_{1}+\\cdots+c_{p} \\mathbf{v}_{p}=\\mathbf{0}$. That is, the vector equation $c_{1} \\mathbf{v}_{1}+c_{2} \\mathbf{v}_{2}+\\cdots+c_{p} \\mathbf{v}_{p}=\\mathbf{0}$ has a nontrivial solution.\n\nlinearly independent (vectors): An indexed set $\\left\\{\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{p}\\right\\}$ with the property that the vector equation $c_{1} \\mathbf{v}_{1}+$ $c_{2} \\mathbf{v}_{2}+\\cdots+c_{p} \\mathbf{v}_{p}=\\mathbf{0}$ has only the trivial solution, $c_{1}=\\cdots=c_{p}=0$.\n\nlinear model (in statistics): Any equation of the form $\\mathbf{y}=X \\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}$, where $X$ and $\\mathbf{y}$ are known and $\\boldsymbol{\\beta}$ is to be chosen to minimize the length of the residual vector, $\\epsilon$.\n\nlinear system: A collection of one or more linear equations involving the same variables, say, $x_{1}, \\ldots, x_{n}$.\n\nlinear transformation $\\boldsymbol{T}$ (from a vector space $V$ into a vector space $W$ ): A rule $T$ that assigns to each vector $\\mathbf{x}$ in $V$ a unique vector $T(\\mathbf{x})$ in $W$, such that (i) $T(\\mathbf{u}+\\mathbf{v})=T(\\mathbf{u})+T(\\mathbf{v})$ for all $\\mathbf{u}, \\mathbf{v}$ in $V$, and (ii) $T(c \\mathbf{u})=c T(\\mathbf{u})$ for all $\\mathbf{u}$ in $V$ and all scalars $c$. Notation: $T: V \\rightarrow W ;$ also, $\\mathbf{x} \\mapsto A \\mathbf{x}$ when $T: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ and $A$ is the standard matrix for $T$.\n\nline through p parallel to $\\mathbf{v}$ : The set $\\{\\mathbf{p}+t \\mathbf{v}: t$ in $\\mathbb{R}\\}$.\n\nloop current: The amount of electric current flowing through a loop that makes the algebraic sum of the $R I$ voltage drops around the loop equal to the algebraic sum of the voltage sources in the loop.\n\nlower triangular matrix: A matrix with zeros above the main diagonal.\n\nlower triangular part (of $A$ ): A lower triangular matrix whose entries on the main diagonal and below agree with those in $A$.\n\nLU factorization: The representation of a matrix $A$ in the form $A=L U$ where $L$ is a square lower triangular matrix with ones on the diagonal (a unit lower triangular matrix) and $U$ is an echelon form of $A$.\n\nmagnitude (of a vector): See norm.\n\nmain diagonal (of a matrix): The entries with equal row and column indices.\n\nmapping: See transformation.\n\nMarkov chain: A sequence of probability vectors $\\mathbf{x}_{0}, \\mathbf{x}_{1}$, $\\mathbf{x}_{2}, \\ldots$, together with a stochastic matrix $P$ such that $\\mathbf{x}_{k+1}=P \\mathbf{x}_{k}$ for $k=0,1,2, \\ldots$\n\nmatrix: A rectangular array of numbers.\n\nmatrix equation: An equation that involves at least one matrix; for instance, $A \\mathbf{x}=\\mathbf{b}$.\n\nmatrix for $T$ relative to bases $\\mathcal{B}$ and $\\mathcal{C}$ : A matrix $M$ for a linear transformation $T: V \\rightarrow W$ with the property that $[T(\\mathbf{x})]_{\\mathcal{C}}=M[\\mathbf{x}]_{\\mathcal{B}}$ for all $\\mathbf{x}$ in $V$, where $\\mathcal{B}$ is a basis for $V$ and $\\mathcal{C}$ is a basis for $W$. When $W=V$ and $\\mathcal{C}=\\mathcal{B}$, the matrix $M$ is called the $\\mathcal{B}$-matrix for $T$ and is denoted by $[T]_{\\mathcal{B}}$.\n\nmatrix of observations: A $p \\times N$ matrix whose columns are observation vectors, each column listing $p$ measurements made on an individual or object in a specified population or set. matrix transformation: A mapping $\\mathbf{x} \\mapsto A \\mathbf{x}$, where $A$ is an $m \\times n$ matrix and $\\mathbf{x}$ represents any vector in $\\mathbb{R}^{n}$.\n\nmaximal linearly independent set (in $V$ ): A linearly independent set $\\mathcal{B}$ in $V$ such that if a vector $\\mathbf{v}$ in $V$ but not in $\\mathcal{B}$ is added to $\\mathcal{B}$, then the new set is linearly dependent.\n\nmean-deviation form (of a matrix of observations): A matrix whose row vectors are in mean-deviation form. For each row, the entries sum to zero.\n\nmean-deviation form (of a vector): A vector whose entries sum to zero.\n\nmean square error: The error of an approximation in an inner product space, where the inner product is defined by a definite integral.\n\nmigration matrix: A matrix that gives the percentage movement between different locations, from one period to the next.\n\nminimal spanning set (for a subspace $H$ ): A set $\\mathcal{B}$ that spans $H$ and has the property that if one of the elements of $\\mathcal{B}$ is removed from $\\mathcal{B}$, then the new set does not span $H$.\n\n$m \\times n$ matrix: A matrix with $m$ rows and $n$ columns.\n\nMoore-Penrose inverse: See pseudoinverse.\n\nmultiple regression: A linear model involving several independent variables and one dependent variable.\n\nnearly singular matrix: An ill-conditioned matrix.\n\nnegative definite matrix: A symmetric matrix $A$ such that $\\mathbf{x}^{T} A \\mathbf{x}\u003c0$ for all $\\mathbf{x} \\neq \\mathbf{0}$.\n\nnegative definite quadratic form: A quadratic form $Q$ such that $Q(\\mathbf{x})\u003c0$ for all $\\mathbf{x} \\neq \\mathbf{0}$.\n\nnegative semidefinite matrix: A symmetric matrix $A$ such that $\\mathbf{x}^{T} A \\mathbf{x} \\leq 0$ for all $\\mathbf{x}$.\n\nnegative semidefinite quadratic form: A quadratic form $Q$ such that $Q(\\mathbf{x}) \\leq 0$ for all $\\mathbf{x}$.\n\nnonhomogeneous equation: An equation of the form $A \\mathbf{x}=\\mathbf{b}$ with $\\mathbf{b} \\neq \\mathbf{0}$, possibly written as a vector equation or as a system of linear equations.\n\nnonsingular (matrix): An invertible matrix.\n\nnontrivial solution: A nonzero solution of a homogeneous equation or system of homogeneous equations.\n\nnonzero (matrix or vector): A matrix (with possibly only one row or column) that contains at least one nonzero entry.\n\nnorm (or length, of $\\mathbf{v}$ ): The scalar $\\|\\mathbf{v}\\|=\\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}=\\sqrt{\\langle\\mathbf{v}, \\mathbf{v}\\rangle}$.\n\nnormal equations: The system of equations represented by $A^{T} A \\mathbf{x}=A^{T} \\mathbf{b}$, whose solution yields all least-squares solutions of $A \\mathbf{x}=\\mathbf{b}$. In statistics, a common notation is $X^{T} X \\boldsymbol{\\beta}=X^{T} \\mathbf{y}$\n\nnormalizing (a nonzero vector $\\mathbf{v}$ ): The process of creating a unit vector $\\mathbf{u}$ that is a positive multiple of $\\mathbf{v}$.\n\nnormal vector (to a subspace $V$ of $\\mathbb{R}^{n}$ ): A vector $\\mathbf{n}$ in $\\mathbb{R}^{n}$ such that $\\mathbf{n} \\cdot \\mathbf{x}=0$ for all $\\mathbf{x}$ in $V$. \n\nnull space ( of an $m \\times n$ matrix $A$ ): The set $\\operatorname{Nul} A$ of all solutions to the homogeneous equation $A \\mathbf{x}=\\mathbf{0}$. Nul $A=\\{\\mathbf{x}: \\mathbf{x}$ is in $\\mathbb{R}^{n}$ and $\\left.A \\mathbf{x}=\\mathbf{0}\\right\\}$\n\nobservation vector: The vector $\\mathbf{y}$ in the linear model $\\mathbf{y}=X \\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}$, where the entries in $\\mathbf{y}$ are the observed values of a dependent variable.\n\none-to-one (mapping): A mapping $T: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ such that each $\\mathbf{b}$ in $R^{m}$ is the image of at most one $\\mathbf{x}$ in $\\mathbb{R}^{n}$.\n\nonto (mapping): A mapping $T: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ such that each $\\mathbf{b}$ in $R^{m}$ is the image of at least one $\\mathbf{x}$ in $\\mathbb{R}^{n}$.\n\nopen ball $\\mathbf{B}(\\mathbf{p}, \\delta)$ in $\\mathbb{R}^{n}$ : The set $\\{\\mathbf{x}:\\|\\mathbf{x}-\\mathbf{p}\\|\u003c\\delta\\}$ in $\\mathbb{R}^{n}$, where $\\delta\u003e0$\n\nopen set $S$ in $\\mathbb{R}^{n}$ : A set that contains none of its boundary points. (Equivalently, $S$ is open if every point of $S$ is an interior point.)\n\norigin: The zero vector.\n\northogonal basis: A basis that is also an orthogonal set.\n\northogonal complement ( of $W$ ): The set $W^{\\perp}$ of all vectors orthogonal to $W$.\n\northogonal decomposition: The representation of a vector $\\mathbf{y}$ as the sum of two vectors, one in a specified subspace $W$ and the other in $W^{\\perp}$. In general, a decomposition $\\mathbf{y}=c_{1} \\mathbf{u}_{1}+\\cdots+c_{p} \\mathbf{u}_{p}$, where $\\left\\{\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{p}\\right\\}$ is an orthogonal basis for a subspace that contains $\\mathbf{y}$.\n\northogonally diagonalizable (matrix): A matrix $A$ that admits a factorization, $A=P D P^{-1}$, with $P$ an orthogonal matrix $\\left(P^{-1}=P^{T}\\right)$ and $D$ diagonal.\n\northogonal matrix: A square invertible matrix $U$ such that $U^{-1}=U^{T}$\n\northogonal projection of $\\mathbf{y}$ onto $\\mathbf{u}$ (or onto the line through $\\mathbf{u}$ and the origin, for $\\mathbf{u} \\neq \\mathbf{0}$ ): The vector $\\hat{\\mathbf{y}}$ defined by $\\hat{\\mathbf{y}}=\\frac{\\mathbf{y} \\cdot \\mathbf{u}}{\\mathbf{u} \\cdot \\mathbf{u}} \\mathbf{u}$. orthogonal projection of y onto $W$ : The unique vector $\\hat{\\mathbf{y}}$ in $W$ such that $\\mathbf{y}-\\hat{\\mathbf{y}}$ is orthogonal to $W$. Notation: $\\hat{\\mathbf{y}}=\\operatorname{proj}_{W} \\mathbf{y}$.\n\northogonal set: A set $S$ of vectors such that $\\mathbf{u} \\cdot \\mathbf{v}=0$ for each distinct pair $\\mathbf{u}, \\mathbf{v}$ in $S$.\n\northogonal to $\\boldsymbol{W}$ : Orthogonal to every vector in $W$.\n\northonormal basis: A basis that is an orthogonal set of unit vectors.\n\northonormal set: An orthogonal set of unit vectors.\n\nouter product: A matrix product $\\mathbf{u v}^{T}$ where $\\mathbf{u}$ and $\\mathbf{v}$ are vectors in $\\mathbb{R}^{n}$ viewed as $n \\times 1$ matrices. (The transpose symbol is on the \"outside\" of the symbols $\\mathbf{u}$ and $\\mathbf{v}$.)\n\noverdetermined system: A system of equations with more equations than unknowns.\n\nparallel flats: Two or more flats such that each flat is a translate of the other flats. parallelogram rule for addition: A geometric interpretation of the sum of two vectors $\\mathbf{u}, \\mathbf{v}$ as the diagonal of the parallelogram determined by $\\mathbf{u}, \\mathbf{v}$, and $\\mathbf{0}$.\n\nparameter vector: The unknown vector $\\boldsymbol{\\beta}$ in the linear model $\\mathbf{y}=X \\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}$\n\nparametric equation of a line: An equation of the form $\\mathbf{x}=\\mathbf{p}+t \\mathbf{v}(t$ in $\\mathbb{R})$.\n\nparametric equation of a plane: An equation of the form $\\mathbf{x}=\\mathbf{p}+s \\mathbf{u}+t \\mathbf{v} \\quad(s, t$ in $\\mathbb{R})$, with $\\mathbf{u}$ and $\\mathbf{v}$ linearly independent.\n\npartitioned matrix (or block matrix): A matrix whose entries are themselves matrices of appropriate sizes.\n\npermuted lower triangular matrix: A matrix such that a permutation of its rows will form a lower triangular matrix.\n\npermuted LU factorization: The representation of a matrix $A$ in the form $A=L U$ where $L$ is a square matrix such that a permutation of its rows will form a unit lower triangular matrix, and $U$ is an echelon form of $A$.\n\npivot: A nonzero number that either is used in a pivot position to create zeros through row operations or is changed into a leading 1 , which in turn is used to create zeros.\n\npivot column: A column that contains a pivot position.\n\npivot position: A position in a matrix $A$ that corresponds to a leading entry in an echelon form of $A$.\n\nplane through $\\mathbf{u}, \\mathbf{v}$, and the origin: A set whose parametric equation is $\\mathbf{x}=s \\mathbf{u}+t \\mathbf{v}(s, t$ in $\\mathbb{R})$, with $\\mathbf{u}$ and $\\mathbf{v}$ linearly independent.\n\npolar decomposition (of $A$ ): A factorization $A=P Q$, where $P$ is an $n \\times n$ positive semidefinite matrix with the same rank as $A$, and $Q$ is an $n \\times n$ orthogonal matrix.\n\npolygon: A polytope in $\\mathbb{R}^{2}$.\n\npolyhedron: A polytope in $\\mathbb{R}^{3}$.\n\npolytope: The convex hull of a finite set of points in $\\mathbb{R}^{n}$ (a special type of compact convex set).\n\npositive combination (of points $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{m}$ in $\\mathbb{R}^{n}$ ): A linear combination $c_{1} \\mathbf{v}_{1}+\\cdots+c_{m} \\mathbf{v}_{m}$, where all $c_{i} \\geq 0$.\n\npositive definite matrix: A symmetric matrix $A$ such that $\\mathbf{x}^{T} A \\mathbf{x}\u003e0$ for all $\\mathbf{x} \\neq \\mathbf{0}$.\n\npositive definite quadratic form: A quadratic form $Q$ such that $Q(\\mathbf{x})\u003e0$ for all $\\mathbf{x} \\neq \\mathbf{0}$.\n\npositive hull (of a set $S$ ): The set of all positive combinations of points in $S$, denoted by pos $S$.\n\npositive semidefinite matrix: A symmetric matrix $A$ such that $\\mathbf{x}^{T} A \\mathbf{x} \\geq 0$ for all $\\mathbf{x}$.\n\npositive semidefinite quadratic form: A quadratic form $Q$ such that $Q(\\mathbf{x}) \\geq 0$ for all $\\mathbf{x}$.\n\npower method: An algorithm for estimating a strictly dominant eigenvalue of a square matrix.\n\nprincipal axes (of a quadratic form $\\mathbf{x}^{T} A \\mathbf{x}$ ): The orthonormal columns of an orthogonal matrix $P$ such that $P^{-1} A P$ is diagonal. (These columns are unit eigenvectors of $A$.) Usually the columns of $P$ are ordered in such a way that the corresponding eigenvalues of $A$ are arranged in decreasing order of magnitude.\n\nprincipal components (of the data in a matrix $B$ of observations): The unit eigenvectors of a sample covariance matrix $S$ for $B$, with the eigenvectors arranged so that the corresponding eigenvalues of $S$ decrease in magnitude. If $B$ is in mean-deviation form, then the principal components are the right singular vectors in a singular value decomposition of $B^{T}$.\n\nprobability vector: A vector in $\\mathbb{R}^{n}$ whose entries are nonnegative and sum to one.\n\nproduct $A \\mathbf{x}$ : The linear combination of the columns of $A$ using the corresponding entries in $\\mathbf{x}$ as weights.\n\nproduction vector: The vector in the Leontief input-output model that lists the amounts that are to be produced by the various sectors of an economy.\n\nprofile (of a set $S$ in $\\mathbb{R}^{n}$ ): The set of extreme points of $S$.\n\nprojection matrix (or orthogonal projection matrix): A symmetric matrix $B$ such that $B^{2}=B$. A simple example is $B=\\mathbf{v}^{T}$, where $\\mathbf{v}$ is a unit vector.\n\nproper subset of a set $S$ : A subset of $S$ that does not equal $S$ itself.\n\nproper subspace: Any subspace of a vector space $V$ other than $V$ itself.\n\npseudoinverse ( of $A$ ): The matrix $V D^{-1} U^{T}$, when $U D V^{T}$ is a reduced singular value decomposition of $A$.\n\nQR factorization: A factorization of an $m \\times n$ matrix $A$ with linearly independent columns, $A=Q R$, where $Q$ is an $m \\times n$ matrix whose columns form an orthonormal basis for $\\operatorname{Col} A$, and $R$ is an $n \\times n$ upper triangular invertible matrix with positive entries on its diagonal.\n\nquadratic Bézier curve: A curve whose description may be written in the form $\\mathbf{g}(t)=(1-t) \\mathbf{f}_{0}(t)+t \\mathbf{f}_{1}(t)$ for $0 \\leq t \\leq$ 1 , where $\\mathbf{f}_{0}(t)=(1-t) \\mathbf{p}_{0}+t \\mathbf{p}_{1}$ and $\\mathbf{f}_{1}(t)=(1-t) \\mathbf{p}_{1}+$ $t \\mathbf{p}_{2}$. The points $\\mathbf{p}_{0}, \\mathbf{p}_{1}, \\mathbf{p}_{2}$ are called the control points for the curve.\n\nquadratic form: A function $Q$ defined for $\\mathbf{x}$ in $\\mathbb{R}^{n}$ by $Q(\\mathbf{x})=$ $\\mathbf{x}^{T} A \\mathbf{x}$, where $A$ is an $n \\times n$ symmetric matrix (called the matrix of the quadratic form).\n\nrange (of a linear transformation $T$ ): The set of all vectors of the form $T(\\mathbf{x})$ for some $\\mathbf{x}$ in the domain of $T$.\n\nrank (of a matrix $A$ ): The dimension of the column space of $A$, denoted by $\\operatorname{rank} A$.\n\nRayleigh quotient: $R(\\mathbf{x})=\\left(\\mathbf{x}^{T} A \\mathbf{x}\\right) /\\left(\\mathbf{x}^{T} \\mathbf{x}\\right)$. An estimate of an eigenvalue of $A$ (usually a symmetric matrix).\n\nrecurrence relation: See difference equation. \n\nreduced echelon form (or reduced row echelon form): A reduced echelon matrix that is row equivalent to a given matrix.\n\nreduced echelon matrix: A rectangular matrix in echelon form that has these additional properties: The leading entry in each nonzero row is 1 , and each leading 1 is the only nonzero entry in its column.\n\nreduced singular value decomposition: A factorization $A=U D V^{T}$, for an $m \\times n$ matrix $A$ of rank $r$, where $U$ is $m \\times r$ with orthonormal columns, $D$ is an $r \\times r$ diagonal matrix with the $r$ nonzero singular values of $A$ on its diagonal, and $V$ is $n \\times r$ with orthonormal columns.\n\nregression coefficients: The coefficients $\\beta_{0}$ and $\\beta_{1}$ in the leastsquares line $y=\\beta_{0}+\\beta_{1} x$.\n\nregular solid: One of the five possible regular polyhedrons in $\\mathbb{R}^{3}$ : the tetrahedron (4 equal triangular faces), the cube (6 square faces), the octahedron (8 equal triangular faces), the dodecahedron (12 equal pentagonal faces), and the icosahedron (20 equal triangular faces).\n\nregular stochastic matrix: A stochastic matrix $P$ such that some matrix power $P^{k}$ contains only strictly positive entries.\n\nrelative change or relative error (in b): The quantity $\\|\\Delta \\mathbf{b}\\| /\\|\\mathbf{b}\\|$ when $\\mathbf{b}$ is changed to $\\mathbf{b}+\\Delta \\mathbf{b}$.\n\nrepellor (of a dynamical system in $\\mathbb{R}^{2}$ ): The origin when all trajectories except the constant zero sequence or function tend away from $\\mathbf{0}$.\n\nresidual vector: The quantity $\\epsilon$ that appears in the general linear model: $\\mathbf{y}=X \\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}$; that is, $\\boldsymbol{\\epsilon}=\\mathbf{y}-X \\boldsymbol{\\beta}$, the difference between the observed values and the predicted values (of $y$ ).\n\n$\\operatorname{Re} \\mathbf{x}$ : The vector in $\\mathbb{R}^{n}$ formed from the real parts of the entries of a vector $\\mathbf{x}$ in $\\mathbb{C}^{n}$.\n\nright inverse (of $A$ ): Any rectangular matrix $C$ such that $A C=I$\n\nright-multiplication (by $A$ ): Multiplication of a matrix on the right by $A$.\n\nright singular vectors (of $A$ ): The columns of $V$ in the singular value decomposition $A=U \\Sigma V^{T}$.\n\nroundoff error: Error in floating point arithmetic caused when the result of a calculation is rounded (or truncated) to the number of floating point digits stored. Also, the error that results when the decimal representation of a number such as $1 / 3$ is approximated by a floating point number with a finite number of digits.\n\nrow-column rule: The rule for computing a product $A B$ in which the $(i, j)$-entry of $A B$ is the sum of the products of corresponding entries from row $i$ of $A$ and column $j$ of $B$.\n\nrow equivalent (matrices): Two matrices for which there exists a (finite) sequence of row operations that transforms one matrix into the other.\n\nrow reduction algorithm: A systematic method using elementary row operations that reduces a matrix to echelon form or reduced echelon form. row replacement: An elementary row operation that replaces one row of a matrix by the sum of the row and a multiple of another row.\n\nrow space (of a matrix $A$ ): The set Row $A$ of all linear combinations of the vectors formed from the rows of $A$; also denoted by $\\operatorname{Col} A^{T}$.\n\nrow sum: The sum of the entries in a row of a matrix.\n\nrow vector: A matrix with only one row, or a single row of a matrix that has several rows.\n\nrow-vector rule for computing $\\boldsymbol{A} \\mathbf{x}$ : The rule for computing a product $A \\mathbf{x}$ in which the $i$ th entry of $A \\mathbf{x}$ is the sum of the products of corresponding entries from row $i$ of $A$ and from the vector $\\mathbf{x}$.\n\nsaddle point (of a dynamical system in $\\mathbb{R}^{2}$ ): The origin when some trajectories are attracted to $\\mathbf{0}$ and other trajectories are repelled from $\\mathbf{0}$.\n\nsame direction (as a vector $\\mathbf{v}$ ): A vector that is a positive multiple of $\\mathbf{v}$.\n\nsample mean: The average $M$ of a set of vectors, $\\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{N}$, given by $M=(1 / N)\\left(\\mathbf{X}_{1}+\\cdots+\\mathbf{X}_{N}\\right)$.\n\nscalar: A (real) number used to multiply either a vector or a matrix.\n\nscalar multiple of $\\mathbf{u}$ by $\\boldsymbol{c}$ : The vector $c \\mathbf{u}$ obtained by multiplying each entry in $\\mathbf{u}$ by $c$.\n\nscale (a vector): Multiply a vector (or a row or column of a matrix) by a nonzero scalar.\n\nSchur complement: A certain matrix formed from the blocks of a $2 \\times 2$ partitioned matrix $A=\\left[A_{i j}\\right]$. If $A_{11}$ is invertible, its Schur complement is given by $A_{22}-A_{21} A_{11}^{-1} A_{12}$. If $A_{22}$ is invertible, its Schur complement is given by $A_{11}-A_{12} A_{22}^{-1} A_{21}$.\n\nSchur factorization (of $A$, for real scalars): A factorization $A=U R U^{T}$ of an $n \\times n$ matrix $A$ having $n$ real eigenvalues, where $U$ is an $n \\times n$ orthogonal matrix and $R$ is an upper triangular matrix.\n\nset spanned by $\\left\\{\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{\\boldsymbol{p}}\\right\\}$ : The set $\\operatorname{Span}\\left\\{\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{p}\\right\\}$.\n\nsignal (or discrete-time signal): A doubly infinite sequence of numbers, $\\left\\{y_{k}\\right\\}$; a function defined on the integers; belongs to the vector space $\\mathbb{S}$.\n\nsimilar (matrices): Matrices $A$ and $B$ such that $P^{-1} A P=B$, or equivalently, $A=P B P^{-1}$, for some invertible matrix $P$.\n\nsimilarity transformation: A transformation that changes $A$ into $P^{-1} A P$.\n\nsimplex: The convex hull of an affinely independent finite set of vectors in $\\mathbb{R}^{n}$.\n\nsingular (matrix): A square matrix that has no inverse.\n\nsingular value decomposition (of an $m \\times n$ matrix $A$ ): $A=$ $U \\Sigma V^{T}$, where $U$ is an $m \\times m$ orthogonal matrix, $V$ is an $n \\times n$ orthogonal matrix, and $\\Sigma$ is an $m \\times n$ matrix with nonnegative entries on the main diagonal (arranged in decreasing order of magnitude) and zeros elsewhere. If $\\operatorname{rank} A=r$, then $\\Sigma$ has exactly $r$ positive entries (the nonzero singular values of $A$ ) on the diagonal.\n\nsingular values (of $A$ ): The (positive) square roots of the eigenvalues of $A^{T} A$, arranged in decreasing order of magnitude.\n\nsize (of a matrix): Two numbers, written in the form $m \\times n$, that specify the number of rows $(m)$ and columns $(n)$ in the matrix.\n\nsolution (of a linear system involving variables $x_{1}, \\ldots, x_{n}$ ): A list $\\left(s_{1}, s_{2}, \\ldots, s_{n}\\right)$ of numbers that makes each equation in the system a true statement when the values $s_{1}, \\ldots, s_{n}$ are substituted for $x_{1}, \\ldots, x_{n}$, respectively.\n\nsolution set: The set of all possible solutions of a linear system. The solution set is empty when the linear system is inconsistent.\n\n$\\operatorname{Span}\\left\\{\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{\\boldsymbol{p}}\\right\\}$ : The set of all linear combinations of $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{p}$. Also, the subspace spanned (or generated) by $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{p}$.\n\nspanning set (for a subspace $H$ ): Any set $\\left\\{\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{p}\\right\\}$ in $H$ such that $H=\\operatorname{Span}\\left\\{\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{p}\\right\\}$.\n\nspectral decomposition (of $A$ ): A representation\n\n$$\nA=\\lambda_{1} \\mathbf{u}_{1} \\mathbf{u}_{1}^{T}+\\cdots+\\lambda_{n} \\mathbf{u}_{n} \\mathbf{u}_{n}^{T}\n$$\n\nwhere $\\left\\{\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{n}\\right\\}$ is an orthonormal basis of eigenvectors of $A$, and $\\lambda_{1}, \\ldots, \\lambda_{n}$ are the corresponding eigenvalues of $A$.\n\nspiral point (of a dynamical system in $\\mathbb{R}^{2}$ ): The origin when the trajectories spiral about $\\mathbf{0}$.\n\nstage-matrix model: A difference equation $\\mathbf{x}_{k+1}=A \\mathbf{x}_{k}$ where $\\mathbf{x}_{k}$ lists the number of females in a population at time $k$, with the females classified by various stages of development (such as juvenile, subadult, and adult).\n\nstandard basis: The basis $\\mathcal{E}=\\left\\{\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\right\\}$ for $\\mathbb{R}^{n}$ consisting of the columns of the $n \\times n$ identity matrix, or the basis $\\left\\{1, t, \\ldots, t^{n}\\right\\}$ for $\\mathbb{P}_{n}$.\n\nstandard matrix (for a linear transformation $T$ ): The matrix $A$ such that $T(\\mathbf{x})=A \\mathbf{x}$ for all $\\mathbf{x}$ in the domain of $T$.\n\nstandard position: The position of the graph of an equation $\\mathbf{x}^{T} A \\mathbf{x}=c$, when $A$ is a diagonal matrix.\n\nstate vector: A probability vector. In general, a vector that describes the \"state\" of a physical system, often in connection with a difference equation $\\mathbf{x}_{k+1}=A \\mathbf{x}_{k}$.\n\nsteady-state vector (for a stochastic matrix $P$ ): A probability vector $\\mathbf{q}$ such that $P \\mathbf{q}=\\mathbf{q}$.\n\nstiffness matrix: The inverse of a flexibility matrix. The $j$ th column of a stiffness matrix gives the loads that must be applied at specified points on an elastic beam in order to produce a unit deflection at the $j$ th point on the beam.\n\nstochastic matrix: A square matrix whose columns are probability vectors.\n\nstrictly dominant eigenvalue: An eigenvalue $\\lambda_{1}$ of a matrix $A$ with the property that $\\left|\\lambda_{1}\\right|\u003e\\left|\\lambda_{k}\\right|$ for all other eigenvalues $\\lambda_{k}$ of $A$. submatrix (of $A$ ): Any matrix obtained by deleting some rows and/or columns of $A$; also, $A$ itself.\n\nsubspace: A subset $H$ of some vector space $V$ such that $H$ has these properties: (1) the zero vector of $V$ is in $H$; (2) $H$ is closed under vector addition; and (3) $H$ is closed under multiplication by scalars.\n\nsupporting hyperplane (to a compact convex set $S$ in $\\mathbb{R}^{n}$ ): A hyperplane $H=[f: d]$ such that $H \\cap S \\neq \\varnothing$ and either $f(x) \\leq d$ for all $x$ in $S$ or $f(x) \\geq d$ for all $x$ in $S$.\n\nsymmetric matrix: A matrix $A$ such that $A^{T}=A$.\n\nsystem of linear equations (or a linear system): A collection of one or more linear equations involving the same set of variables, say, $x_{1}, \\ldots, x_{n}$.\n\ntetrahedron: A three-dimensional solid object bounded by four equal triangular faces, with three faces meeting at each vertex.\n\ntotal variance: The trace of the covariance matrix $S$ of a matrix of observations.\n\ntrace (of a square matrix $A$ ): The sum of the diagonal entries in $A$, denoted by $\\operatorname{tr} A$.\n\ntrajectory: The graph of a solution $\\left\\{\\mathbf{x}_{0}, \\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots\\right\\}$ of a dynamical system $\\mathbf{x}_{k+1}=A \\mathbf{x}_{k}$, often connected by a thin curve to make the trajectory easier to see. Also, the graph of $\\mathbf{x}(t)$ for $t \\geq 0$, when $\\mathbf{x}(t)$ is a solution of a differential equation $\\mathbf{x}^{\\prime}(t)=A \\mathbf{x}(t)$\n\ntransfer matrix: A matrix $A$ associated with an electrical circuit having input and output terminals, such that the output vector is $A$ times the input vector.\n\ntransformation (or function, or mapping) $T$ from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{\\boldsymbol{m}}:$ A rule that assigns to each vector $\\mathbf{x}$ in $\\mathbb{R}^{n}$ a unique vector $T(\\mathbf{x})$ in $\\mathbb{R}^{m}$. Notation: $T: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$. Also, $T: V \\rightarrow W$ denotes a rule that assigns to each $\\mathbf{x}$ in $V$ a unique vector $T(\\mathbf{x})$ in $W$.\n\ntranslation (by a vector $\\mathbf{p}$ ): The operation of adding $\\mathbf{p}$ to a vector or to each vector in a given set.\n\ntranspose (of $A$ ): An $n \\times m$ matrix $A^{T}$ whose columns are the corresponding rows of the $m \\times n$ matrix $A$.\n\ntrend analysis: The use of orthogonal polynomials to fit data, with the inner product given by evaluation at a finite set of points.\n\ntriangle inequality: $\\|\\mathbf{u}+\\mathbf{v}\\| \\leq\\|\\mathbf{u}\\|+\\|\\mathbf{v}\\|$ for all $\\mathbf{u}, \\mathbf{v}$.\n\ntriangular matrix: A matrix $A$ with either zeros above or zeros below the diagonal entries.\n\ntrigonometric polynomial: A linear combination of the constant function 1 and sine and cosine functions such as $\\cos n t$ and $\\sin n t$.\n\ntrivial solution: The solution $\\mathbf{x}=\\mathbf{0}$ of a homogeneous equation $A \\mathbf{x}=\\mathbf{0}$.\n\nuncorrelated variables: Any two variables $x_{i}$ and $x_{j}$ (with $i \\neq j$ ) that range over the $i$ th and $j$ th coordinates of the observation vectors in an observation matrix, such that the covariance $s_{i j}$ is zero.\n\nunderdetermined system: A system of equations with fewer equations than unknowns.\n\nuniqueness question: Asks, \"If a solution of a system exists, is it unique-that is, is it the only one?\"\n\nunit consumption vector: A column vector in the Leontief input-output model that lists the inputs a sector needs for each unit of its output; a column of the consumption matrix.\n\nunit lower triangular matrix: A square lower triangular matrix with ones on the main diagonal.\n\nunit vector: A vector $\\mathbf{v}$ such that $\\|\\mathbf{v}\\|=1$.\n\nupper triangular matrix: A matrix $U$ (not necessarily square) with zeros below the diagonal entries $u_{11}, u_{22}, \\ldots$\n\nVandermonde matrix: An $n \\times n$ matrix $V$ or its transpose, when $V$ has the form\n\n$$\nV=\\left[\\begin{array}{ccccc}\n1 \u0026 x_{1} \u0026 x_{1}^{2} \u0026 \\cdots \u0026 x_{1}^{n-1} \\\\\n1 \u0026 x_{2} \u0026 x_{2}^{2} \u0026 \\cdots \u0026 x_{2}^{n-1} \\\\\n\\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\\\\n1 \u0026 x_{n} \u0026 x_{n}^{2} \u0026 \\cdots \u0026 x_{n}^{n-1}\n\\end{array}\\right]\n$$\n\nvariance (of a variable $x_{j}$ ): The diagonal entry $s_{j j}$ in the covariance matrix $S$ for a matrix of observations, where $x_{j}$ varies over the $j$ th coordinates of the observation vectors.\n\nvector: A list of numbers; a matrix with only one column. In general, any element of a vector space.\n\nvector addition: Adding vectors by adding corresponding entries.\n\nvector equation: An equation involving a linear combination of vectors with undetermined weights.\n\nvector space: A set of objects, called vectors, on which two operations are defined, called addition and multiplication by scalars. Ten axioms must be satisfied. See the first definition in Section 4.1.\n\nvector subtraction: Computing $\\mathbf{u}+(-1) \\mathbf{v}$ and writing the result as $\\mathbf{u}-\\mathbf{v}$.\n\nweighted least squares: Least-squares problems with a weighted inner product such as\n\n$$\n\\langle\\mathbf{x}, \\mathbf{y}\\rangle=w_{1}^{2} x_{1} y_{1}+\\cdots+w_{n}^{2} x_{n} y_{n} .\n$$\n\nweights: The scalars used in a linear combination.\n\nzero subspace: The subspace $\\{\\boldsymbol{0}\\}$ consisting of only the zero vector.\n\nzero vector: The unique vector, denoted by $\\mathbf{0}$, such that $\\mathbf{u}+\\mathbf{0}=\\mathbf{u}$ for all $\\mathbf{u}$. In $\\mathbb{R}^{n}, \\mathbf{0}$ is the vector whose entries are all zeros.\n","lastmodified":"2022-12-23T17:10:38.478086923Z","tags":null},"/linear-algebra/span":{"title":"Span","content":"\n# Description\n\nThe span of a set of vectors is a [vector\nspace](linear-algebra/vector-space.md).\n\n\n\nlorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim\nlabore culpa sint ad nisi lorem pariatur mollit ex esse exercitation amet. nisi\nanim cupidatat excepteur officia. reprehenderit nostrud nostrud ipsum lorem est\naliquip amet voluptate voluptate dolor minim nulla est proident. nostrud officia\npariatur ut officia. sit irure elit esse ea nulla sunt ex occaecat reprehenderit\ncommodo officia dolor lorem duis laboris cupidatat officia voluptate. culpa\nproident adipisicing id nulla nisi laboris ex in lorem sunt duis officia\neiusmod. aliqua reprehenderit commodo ex non excepteur duis sunt velit enim.\nvoluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa\nduis.\n\n\n\nlorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim\nlabore culpa sint ad nisi lorem pariatur mollit ex esse exercitation amet. nisi\nanim cupidatat excepteur officia. reprehenderit nostrud nostrud ipsum lorem est\naliquip amet voluptate voluptate dolor minim nulla est proident. nostrud officia\npariatur ut officia. sit irure elit esse ea nulla sunt ex occaecat reprehenderit\ncommodo officia dolor lorem duis laboris cupidatat officia voluptate. culpa\nproident adipisicing id nulla nisi laboris ex in lorem sunt duis officia\neiusmod. aliqua reprehenderit commodo ex non excepteur duis sunt velit enim.\nvoluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa\nduis.\n\n\n\n\nlorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim\nlabore culpa sint ad nisi lorem pariatur mollit ex esse exercitation amet. nisi\nanim cupidatat excepteur officia. reprehenderit nostrud nostrud ipsum lorem est\naliquip amet voluptate voluptate dolor minim nulla est proident. nostrud officia\npariatur ut officia. sit irure elit esse ea nulla sunt ex occaecat reprehenderit\ncommodo officia dolor lorem duis laboris cupidatat officia voluptate. culpa\nproident adipisicing id nulla nisi laboris ex in lorem sunt duis officia\neiusmod. aliqua reprehenderit commodo ex non excepteur duis sunt velit enim.\nvoluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa\nduis.\n\n\n\n# AFD\n\nasdflksdfkl\n\n\nlorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim\nlabore culpa sint ad nisi lorem pariatur mollit ex esse exercitation amet. nisi\nanim cupidatat excepteur officia. reprehenderit nostrud nostrud ipsum lorem est\naliquip amet voluptate voluptate dolor minim nulla est proident. nostrud officia\npariatur ut officia. sit irure elit esse ea nulla sunt ex occaecat reprehenderit\ncommodo officia dolor lorem duis laboris cupidatat officia voluptate. culpa\nproident adipisicing id nulla nisi laboris ex in lorem sunt duis officia\neiusmod. aliqua reprehenderit commodo ex non excepteur duis sunt velit enim.\nvoluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa\nduis.\n\n\nlorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim\nlabore culpa sint ad nisi lorem pariatur mollit ex esse exercitation amet. nisi\nanim cupidatat excepteur officia. reprehenderit nostrud nostrud ipsum lorem est\naliquip amet voluptate voluptate dolor minim nulla est proident. nostrud officia\npariatur ut officia. sit irure elit esse ea nulla sunt ex occaecat reprehenderit\ncommodo officia dolor lorem duis laboris cupidatat officia voluptate. culpa\nproident adipisicing id nulla nisi laboris ex in lorem sunt duis officia\neiusmod. aliqua reprehenderit commodo ex non excepteur duis sunt velit enim.\nvoluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa\nduis.\n\n\nlorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim\nlabore culpa sint ad nisi lorem pariatur mollit ex esse exercitation amet. nisi\nanim cupidatat excepteur officia. reprehenderit nostrud nostrud ipsum lorem est\naliquip amet voluptate voluptate dolor minim nulla est proident. nostrud officia\npariatur ut officia. sit irure elit esse ea nulla sunt ex occaecat reprehenderit\ncommodo officia dolor lorem duis laboris cupidatat officia voluptate. culpa\nproident adipisicing id nulla nisi laboris ex in lorem sunt duis officia\neiusmod. aliqua reprehenderit commodo ex non excepteur duis sunt velit enim.\nvoluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa\nduis.\n\n","lastmodified":"2022-12-23T17:10:38.478086923Z","tags":null},"/linear-algebra/vector-space":{"title":"Vector Space","content":"\nHello World!\n\n\n[link here](span.md#AFD)\n\n\nIdeas:\n\nlink to / embed external resources\n- SageMath Cells / Octave Online / MATLAB\n- Relevant 3blue1brown video\n- Other relevant YouTube videos\n- Relevant sections in umd textbook \n- Relevant sections in https://davidaustinm.github.io/ula/frontmatter.html\n","lastmodified":"2022-12-23T17:10:38.478086923Z","tags":null}}