{"/":{"title":"Linear Algebra Concept Map","content":"\nThis is a WIP concept map of the topics covered in linear algebra.\n\nUse the search bar to get started.\n\n[glossary](glossary)\n","lastmodified":"2022-12-25T00:51:11.577820107Z","tags":null},"/glossary":{"title":"","content":"# adjugate (or classical adjoint)\nThe matrix adj $A$ formed from a square matrix $A$ by replacing the $(i, j)$-entry of $A$ by the $(i, j)$-cofactor, for all $i$ and $j$, and then transposing the resulting matrix.\n\n# affine combination\nA linear combination of vectors (points in $\\mathbb{R}^{n}$) in which the sum of the weights involved is 1.\n\n# affine dependence relation\nAn equation of the form $c_{1} \\mathbf{v}\\_{1}+$ $\\cdots+c_{p} \\mathbf{v}\\_{p}=\\mathbf{0}$, where the weights $c_{1}, \\ldots, c_{p}$ are not all zero, and $c_{1}+\\cdots+c_{p}=0$.\n\n# affine hull (or affine span) of a set $S$ \nThe set of all affine combinations of points in $S$, denoted by aff $S$.\n\n# affinely dependent set\nA set $\\left\\{\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{p}\\right\\}$ in $\\mathbb{R}^{n}$ such that there are real numbers $c_{1}, \\ldots, c_{p}$, not all zero, such that $c_{1}+\\cdots+$ $c_{p}=0$ and $c_{1} \\mathbf{v}\\_{1}+\\cdots+c_{p} \\mathbf{v}\\_{p}=\\mathbf{0}$.\n\n# affinely independent set\nA set $\\left\\{\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{p}\\right\\}$ in $\\mathbb{R}^{n}$ that is not affinely dependent.\n\n# affine set (or affine subset)\nA set $S$ of points such that if $\\mathbf{p}$ and $\\mathbf{q}$ are in $S$, then $(1-t) \\mathbf{p}+t \\mathbf{q} \\in S$ for each real number $t$.\n\n# affine transformation\nA mapping $T: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ of the form $T(\\mathbf{x})=A \\mathbf{x}+\\mathbf{b}$, with $A$ an $m \\times n$ matrix and $\\mathbf{b}$ in $\\mathbb{R}^{m}$.\n\n# algebraic multiplicity\nThe multiplicity of an eigenvalue as a root of the characteristic equation.\n\n# angle (between nonzero vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^{2}$ or $\\mathbb{R}^{3}$)\nThe angle $\\vartheta$ between the two directed line segments from the origin to the points $\\mathbf{u}$ and $\\mathbf{v}$. Related to the scalar product by\n\n$$\n\\mathbf{u} \\cdot \\mathbf{v}=\\|\\mathbf{u}\\|\\|\\mathbf{v}\\| \\cos \\vartheta\n$$\n\n# associative law of multiplication\n$\\quad A(B C)=(A B) C$, for all $A$, $B, C$.\n\n# attractor (of a dynamical system in $\\mathbb{R}^{2}$)\nThe origin when all trajectories tend toward $\\mathbf{0}$.\n\n# augmented matrix\nA matrix made up of a coefficient matrix for a linear system and one or more columns to the right. Each extra column contains the constants from the right side of a system with the given coefficient matrix. \n\n# auxiliary equation\nA polynomial equation in a variable $r$, created from the coefficients of a homogeneous difference equation.\n\n# back-substitution (with matrix notation)\nThe backward phase of row reduction of an augmented matrix that transforms an echelon matrix into a reduced echelon matrix; used to find the solution(s) of a system of linear equations.\n\n# backward phase (of row reduction)\nThe last part of the algorithm that reduces a matrix in echelon form to a reduced echelon form.\n\n# band matrix\nA matrix whose nonzero entries lie within a band along the main diagonal.\n\n# barycentric coordinates (of a point $\\mathbf{p}$ with respect to an affinely independent set $S=\\left\\{\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{k}\\right\\}$)\nThe (unique) set of weights $c_{1}, \\ldots, c_{k}$ such that $\\mathbf{p}=c_{1} \\mathbf{v}\\_{1}+\\cdots+c_{k} \\mathbf{v}\\_{k}$ and $c_{1}+$ $\\cdots+c_{k}=1$. (Sometimes also called the affine coordinates of $\\mathbf{p}$ with respect to $S$.)\n\n# basic variable\nA variable in a linear system that corresponds to a pivot column in the coefficient matrix.\n\n# basis (for a nontrivial subspace $H$ of a vector space $V$)\nAn indexed set $\\mathcal{B}=\\left\\{\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{p}\\right\\}$ in $V$ such that: (i) $\\mathcal{B}$ is a linearly independent set and (ii) the subspace spanned by $\\mathcal{B}$ coincides with $H$, that is, $H=\\operatorname{Span}\\left\\{\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{p}\\right\\}$.\n\n# $\\mathcal{B}$-coordinates of $\\mathbf{x}$ \nSee coordinates of $\\mathbf{x}$ relative to the basis $\\mathcal{B}$.\n\n# best approximation\nThe closest point in a given subspace to a given vector.\n\n# bidiagonal matrix\nA matrix whose nonzero entries lie on the main diagonal and on one diagonal adjacent to the main diagonal.\n\n# block diagonal (matrix)\nA partitioned matrix $A=\\left[A_{i j}\\right]$ such that each block $A_{i j}$ is a zero matrix for $i \\neq j$.\n\n# block matrix\nSee partitioned matrix.\n\n# block matrix multiplication\nThe row-column multiplication of partitioned matrices as if the block entries were scalars. block upper triangular (matrix): A partitioned matrix $A=\\left[A_{i j}\\right]$ such that each block $A_{i j}$ is a zero matrix for $i\u003ej$.\n\n# boundary point of a set $S$ in $\\mathbb{R}^{n}$ \nA point $\\mathbf{p}$ such that every open ball in $\\mathbb{R}^{n}$ centered at $\\mathbf{p}$ intersects both $S$ and the complement of $S$.\n\n# bounded set in $\\mathbb{R}^{n}$ \nA set that is contained in an open ball $B(\\mathbf{0}, \\delta)$ for some $\\delta\u003e0$.\n\n# $\\mathcal{B}$-matrix (for $T$)\nA matrix $[T]\\_{\\mathcal{B}}$ for a linear transformation $T: V \\rightarrow V$ relative to a basis $\\mathcal{B}$ for $V$, with the property that $[T(\\mathbf{x})]\\_{\\mathcal{B}}=[T]\\_{\\mathcal{B}}[\\mathbf{x}]\\_{\\mathcal{B}}$ for all $\\mathbf{x}$ in $V$.\n\n# Cauchy-Schwarz inequality\n$|\\langle\\mathbf{u}, \\mathbf{v}\\rangle| \\leq\\|u\\| \\cdot\\|v\\|$ for all u, $\\mathbf{v}$. change of basis: See change-of-coordinates matrix.\n\n# change-of-coordinates matrix (from a basis $\\mathcal{B}$ to a basis $\\mathcal{C}$)\nA matrix $\\underset{\\mathcal{C} \\leftarrow \\mathcal{B}}{P}$ that transforms $\\mathcal{B}$-coordinate vectors into $\\mathcal{C}$ coordinate vectors: $[\\mathbf{x}]\\_{\\mathcal{C}}={ }\\_{\\mathcal{C} \\leftarrow \\mathcal{B}}^{P}[\\mathbf{x}]\\_{\\mathcal{B}}$. If $\\mathcal{C}$ is the standard basis for $\\mathbb{R}^{n}$, then ${ }\\_{\\mathcal{C} \\leftarrow \\mathcal{B}}$ is sometimes written as $P_{\\mathcal{B}}$.\n\n# characteristic equation (of $A$)\n$\\quad \\operatorname{det}(A-\\lambda I)=0$.\n\n# characteristic polynomial (of $A$)\n$\\operatorname{det}(A-\\lambda I)$ or, in some texts, $\\operatorname{det}(\\lambda I-A)$.\n\n# Cholesky factorization\nA factorization $A=R^{T} R$, where $R$ is an invertible upper triangular matrix whose diagonal entries are all positive.\n\n# closed ball (in $\\mathbb{R}^{n}$)\nA set $\\{\\mathbf{x}:\\|\\mathbf{x}-\\mathbf{p}\\|\u003c\\delta\\}$ in $\\mathbb{R}^{n}$, where $\\mathbf{p}$ is in $\\mathbb{R}^{n}$ and $\\delta\u003e0$.\n\n# closed set (in $\\mathbb{R}^{n}$)\nA set that contains all of its boundary points. codomain (of a transformation $T: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$): The set $\\mathbb{R}^{m}$ that contains the range of $T$. In general, if $T$ maps a vector space $V$ into a vector space $W$, then $W$ is called the codomain of $T$.\n\n# coefficient matrix\nA matrix whose entries are the coefficients of a system of linear equations.\n\n# cofactor\nA number $C_{i j}=(-1)^{i+j} \\operatorname{det} A_{i j}$, called the $(i, j)$ cofactor of $A$, where $A_{i j}$ is the submatrix formed by deleting the $i$ th row and the $j$ th column of $A$.\n\n# cofactor expansion\nA formula for $\\operatorname{det} A$ using cofactors associated with one row or one column, such as for row 1:\n\n$$\n\\operatorname{det} A=a_{11} C_{11}+\\cdots+a_{1 n} C_{1 n}\n$$\n\n# column-row expansion\nThe expression of a product $A B$ as a sum of outer products: $\\operatorname{col}\\_{1}(A) \\operatorname{row}\\_{1}(B)+\\cdots+$ $\\operatorname{col}\\_{n}(A) \\operatorname{row}\\_{n}(B)$, where $n$ is the number of columns of $A$.\n\n# column space (of an $m \\times n$ matrix $A$)\nThe set $\\operatorname{Col} A$ of all linear combinations of the columns of $A$. If $A=\\left[\\mathbf{a}\\_{1} \\cdots \\mathbf{a}\\_{n}\\right]$, then $\\operatorname{Col} A=\\operatorname{Span}\\left\\{\\mathbf{a}\\_{1}, \\ldots, \\mathbf{a}\\_{n}\\right\\}$. Equivalently,\n\n$$\n\\operatorname{Col} A=\\left\\{\\mathbf{y}: \\mathbf{y}=A \\mathbf{x} \\text { for some } \\mathbf{x} \\text { in } \\mathbb{R}^{n}\\right\\}\n$$\n\n# column sum\nThe sum of the entries in a column of a matrix. column vector: A matrix with only one column, or a single column of a matrix that has several columns.\n\n# commuting matrices\nTwo matrices $A$ and $B$ such that $A B=B A$.\n\n# compact set (in $\\mathbb{R}^{n}$)\nA set in $\\mathbb{R}^{n}$ that is both closed and bounded.\n\n# companion matrix\nA special form of matrix whose characteristic polynomial is $(-1)^{n} p(\\lambda)$ when $p(\\lambda)$ is a specified polynomial whose leading term is $\\lambda^{n}$.\n\n# complex eigenvalue\nA nonreal root of the characteristic equation of an $n \\times n$ matrix.\n\n# complex eigenvector\nA nonzero vector $\\mathbf{x}$ in $\\mathbb{C}^{n}$ such that $A \\mathbf{x}=\\lambda \\mathbf{x}$, where $A$ is an $n \\times n$ matrix and $\\lambda$ is a complex eigenvalue.\n\n# component of $\\mathbf{y}$ orthogonal to $\\mathbf{u}$ (for $\\mathbf{u} \\neq \\mathbf{0}$)\nThe vector $\\mathbf{y}-\\frac{\\mathbf{y} \\cdot \\mathbf{u}}{\\mathbf{u} \\cdot \\mathbf{u}} \\mathbf{u}$.\n\n# composition of linear transformations\nA mapping produced by applying two or more linear transformations in succession. If the transformations are matrix transformations, say left-multiplication by $B$ followed by left-multiplication by $A$, then the composition is the mapping $\\mathbf{x} \\mapsto A(B \\mathbf{x})$.\n\n# condition number (of $A$)\nThe quotient $\\sigma_{1} / \\sigma_{n}$, where $\\sigma_{1}$ is the largest singular value of $A$ and $\\sigma_{n}$ is the smallest singular value. The condition number is $+\\infty$ when $\\sigma_{n}$ is zero.\n\n# conformable for block multiplication\nTwo partitioned matrices $A$ and $B$ such that the block product $A B$ is defined: The column partition of $A$ must match the row partition of $B$.\n\n# consistent linear system\nA linear system with at least one solution.\n\n# constrained optimization\nThe problem of maximizing a quantity such as $\\mathbf{x}^{T} A \\mathbf{x}$ or $\\|A \\mathbf{x}\\|$ when $\\mathbf{x}$ is subject to one or more constraints, such as $\\mathbf{x}^{T} \\mathbf{x}=1$ or $\\mathbf{x}^{T} \\mathbf{v}=0$.\n\n# consumption matrix\nA matrix in the Leontief input-output model whose columns are the unit consumption vectors for the various sectors of an economy.\n\n# contraction\nA mapping $\\mathbf{x} \\mapsto r \\mathbf{x}$ for some scalar $r$, with $0 \\leq r \\leq 1$\n\n# controllable (pair of matrices)\nA matrix pair $(A, B)$ where $A$ is $n \\times n, B$ has $n$ rows, and\n\n$$\n\\operatorname{rank}\\left[\\begin{array}{lllll}\nB \u0026 A B \u0026 A^{2} B \u0026 \\cdots \u0026 A^{n-1} B\n\\end{array}\\right]=n\n$$\n\nRelated to a state-space model of a control system and the difference equation $\\mathbf{x}\\_{k+1}=A \\mathbf{x}\\_{k}+B \\mathbf{u}\\_{k}(k=0,1, \\ldots)$.\n\n# convergent (sequence of vectors)\nA sequence $\\left\\{\\mathbf{x}\\_{k}\\right\\}$ such that the entries in $\\mathbf{x}\\_{k}$ can be made as close as desired to the entries in some fixed vector for all $k$ sufficiently large.\n\n# convex combination (of points $\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{k}$ in $\\mathbb{R}^{n}$)\nA linear combination of vectors (points) in which the weights in the combination are nonnegative and the sum of the weights is 1.\n\n# convex hull (of a set $S$)\nThe set of all convex combinations of points in $S$, denoted by: conv $S$. convex set: A set $S$ with the property that for each $\\mathbf{p}$ and $\\mathbf{q}$ in $S$, the line segment $\\overline{\\mathbf{p q}}$ is contained in $S$.\n\n# coordinate mapping (determined by an ordered basis $\\mathcal{B}$ in a vector space $V$)\nA mapping that associates to each $\\mathbf{x}$ in $V$ its coordinate vector $[\\mathbf{x}]\\_{\\mathcal{B}}$.\n\n# coordinates of $x$ relative to the basis $\\mathcal{B}=\\left\\{\\mathbf{b}\\_{1}, \\ldots, \\mathbf{b}\\_{\\boldsymbol{n}}\\right\\}$ \nThe weights $c_{1}, \\ldots, c_{n}$ in the equation $\\mathbf{x}=c_{1} \\mathbf{b}\\_{1}+\\cdots+c_{n} \\mathbf{b}\\_{n}$.\n\n# coordinate vector of $\\mathbf{x}$ relative to $\\mathcal{B}$ \nThe vector $[\\mathbf{x}]\\_{\\mathcal{B}}$ whose entries are the coordinates of $\\mathbf{x}$ relative to the basis $\\mathcal{B}$.\n\n# covariance (of variables $x_{i}$ and $x_{j}$, for $i \\neq j$)\nThe entry $s_{i j}$ in the covariance matrix $S$ for a matrix of observations, where $x_{i}$ and $x_{j}$ vary over the $i$ th and $j$ th coordinates, respectively, of the observation vectors.\n\n# covariance matrix (or sample covariance matrix)\nThe $p \\times p$ matrix $S$ defined by $S=(N-1)^{-1} B B^{T}$, where $B$ is a $p \\times N$ matrix of observations in mean-deviation form.\n\n# Cramer's rule\nA formula for each entry in the solution $\\mathbf{x}$ of the equation $A \\mathbf{x}=\\mathbf{b}$ when $A$ is an invertible matrix.\n\n# cross-product term\nA term $c x_{i} x_{j}$ in a quadratic form, with $i \\neq j$.\n\n# cube\nA three-dimensional solid object bounded by six square faces, with three faces meeting at each vertex.\n\n# decoupled system\nA difference equation $\\mathbf{y}\\_{k+1}=A \\mathbf{y}\\_{k}$, or a differential equation $\\mathbf{y}^{\\prime}(t)=A \\mathbf{y}(t)$, in which $A$ is a diagonal matrix. The discrete evolution of each entry in $\\mathbf{y}\\_{k}$ (as a function of $k$), or the continuous evolution of each entry in the vector-valued function $\\mathbf{y}(t)$, is unaffected by what happens to the other entries as $k \\rightarrow \\infty$ or $t \\rightarrow \\infty$.\n\n# design matrix\nThe matrix $X$ in the linear model $\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}$, where the columns of $X$ are determined in some way by the observed values of some independent variables.\n\n# determinant (of a square matrix $A$)\nThe number $\\operatorname{det} A$ defined inductively by a cofactor expansion along the first row of $A$. Also, $(-1)^{r}$ times the product of the diagonal entries in any echelon form $U$ obtained from $A$ by row replacements and $r$ row interchanges (but no scaling operations).\n\n# diagonal entries (in a matrix)\nEntries having equal row and column indices.\n\n# diagonalizable (matrix)\nA matrix that can be written in factored form as $P D P^{-1}$, where $D$ is a diagonal matrix and $P$ is an invertible matrix.\n\n# diagonal matrix\nA square matrix whose entries not on the main diagonal are all zero.\n\n# difference equation (or linear recurrence relation)\nAn equation of the form $\\mathbf{x}\\_{k+1}=A \\mathbf{x}\\_{k}(k=0,1,2, \\ldots)$ whose solution is a sequence of vectors, $\\mathbf{x}\\_{0}, \\mathbf{x}\\_{1}, \\ldots$\n\n# dilation\nA mapping $\\mathbf{x} \\mapsto r \\mathbf{x}$ for some scalar $r$, with $1 \u003c r$.\n \n# dimension\n\n\nof a flat $S$ : The dimension of the corresponding parallel subspace.\n\nof a set $S$ : The dimension of the smallest flat containing $S$.\n\nof a subspace $S$ : The number of vectors in a basis for $S$, written as $\\operatorname{dim} S$.\n\nof a vector space $V$ : The number of vectors in a basis for $V$, written as $\\operatorname{dim} V$. The dimension of the zero space is 0.\n\n# discrete linear dynamical system\nA difference equation of the form $\\mathbf{x}\\_{k+1}=A \\mathbf{x}\\_{k}$ that describes the changes in a system (usually a physical system) as time passes. The physical system is measured at discrete times, when $k=0,1,2, \\ldots$, and the state of the system at time $k$ is a vector $\\mathbf{x}\\_{k}$ whose entries provide certain facts of interest about the system.\n\n# distance between $\\mathbf{u}$ and $\\mathbf{v}$ \nThe length of the vector $\\mathbf{u}-\\mathbf{v}$, denoted by dist $(\\mathbf{u}, \\mathbf{v})$.\n\n# distance to a subspace\nThe distance from a given point (vector) $\\mathbf{v}$ to the nearest point in the subspace.\n\n# distributive laws\n(left) $A(B+C)=A B+A C$, and (right) $(B+C) A=B A+C A$, for all $A, B, C$.\n\n# domain (of a transformation $T$)\nThe set of all vectors $\\mathbf{x}$ for which $T(\\mathbf{x})$ is defined.\n\n# dot product\nSee inner product.\n\n# dynamical system\nSee discrete linear dynamical system.\n\n# echelon form (or row echelon form, of a matrix)\nAn echelon matrix that is row equivalent to the given matrix.\n\n# echelon matrix (or row echelon matrix)\nA rectangular matrix that has three properties: (1) All nonzero rows are above any row of all zeros. (2) Each leading entry of a row is in a column to the right of the leading entry of the row above it. (3) All entries in a column below a leading entry are zero.\n\n# eigenfunctions (of a differential equation $\\mathbf{x}^{\\prime}(t)=A \\mathbf{x}(t)$)\n$\\quad \\mathrm{A}$ function $\\mathbf{x}(t)=\\mathbf{v} e^{\\lambda t}$, where $\\mathbf{v}$ is an eigenvector of $A$ and $\\lambda$ is the corresponding eigenvalue.\n\n# eigenspace (of $A$ corresponding to $\\lambda$)\nThe set of all solutions of $A \\mathbf{x}=\\lambda \\mathbf{x}$, where $\\lambda$ is an eigenvalue of $A$. Consists of the zero vector and all eigenvectors corresponding to $\\lambda$.\n\n# eigenvalue (of $A$)\nA scalar $\\lambda$ such that the equation $A \\mathbf{x}=\\lambda \\mathbf{x}$ has a solution for some nonzero vector $\\mathbf{x}$.\n \n# eigenvector (of $A$)\nA nonzero vector $\\mathbf{x}$ such that $A \\mathbf{x}=\\lambda \\mathbf{x}$ for some scalar $\\lambda$.\n\n# eigenvector basis\nA basis consisting entirely of eigenvectors of a given matrix.\n\n# eigenvector decomposition (of $\\mathbf{x}$)\nAn equation, $\\mathbf{x}=c_{1} \\mathbf{v}\\_{1}+$ $\\cdots+c_{n} \\mathbf{v}\\_{n}$, expressing $\\mathbf{x}$ as a linear combination of eigenvectors of a matrix.\n\n# elementary matrix\nAn invertible matrix that results by performing one elementary row operation on an identity matrix.\n\n# elementary row operations\n(1) (Replacement) Replace one row by the sum of itself and a multiple of another row. (2) Interchange two rows. (3) (Scaling) Multiply all entries in a row by a nonzero constant.\n\n# equal vectors\nVectors in $\\mathbb{R}^{n}$ whose corresponding entries are the same. equilibrium prices: A set of prices for the total output of the various sectors in an economy, such that the income of each sector exactly balances its expenses.\n\n# equilibrium vector\nSee steady-state vector.\n\n# equivalent (linear) systems\nLinear systems with the same solution set.\n\n# exchange model\nSee Leontief exchange model.\n\n# existence question\nAsks, \"Does a solution to the system exist?\" That is, \"Is the system consistent?\" Also, \"Does a solution of $A \\mathbf{x}=\\mathbf{b}$ exist for all possible $\\mathbf{b}$ ?\"\n\n# expansion by cofactors\nSee cofactor expansion.\n\n# explicit description (of a subspace $W$ of $\\mathbb{R}^{n}$)\nA parametric representation of $W$ as the set of all linear combinations of a set of specified vectors.\n\n# extreme point (of a convex set $S$)\nA point $\\mathbf{p}$ in $S$ such that $\\mathbf{p}$ is not in the interior of any line segment that lies in $S$. (That is, if $\\mathbf{x}, \\mathbf{y}$ are in $S$ and $\\mathbf{p}$ is on the line segment $\\overline{\\mathbf{x y}}$, then $\\mathbf{p}=\\mathbf{x}$ or $\\mathbf{p}=\\mathbf{y}$.\n\n# factorization (of $A$)\nAn equation that expresses $A$ as a product of two or more matrices.\n\n# final demand vector (or bill of final demands)\nThe vector d in the Leontief input-output model that lists the dollar values of the goods and services demanded from the various sectors by the nonproductive part of the economy. The vector $\\mathbf{d}$ can represent consumer demand, government consumption, surplus production, exports, or other external demand.\n\n# finite-dimensional (vector space)\nA vector space that is spanned by a finite set of vectors.\n\n# flat $\\left(\\right.$ in $\\mathbb{R}^{n}$)\nA translate of a subspace of $\\mathbb{R}^{n}$.\n\n# flexibility matrix\nA matrix whose $j$ th column gives the deflections of an elastic beam at specified points when a unit force is applied at the $j$ th point on the beam.\n\n# floating point arithmetic\nArithmetic with numbers represented as decimals $\\pm . d_{1} \\cdots d_{p} \\times 10^{r}$, where $r$ is an integer and the number $p$ of digits to the right of the decimal point is usually between 8 and 16 .\n\n# flop\nOne arithmetic operation $(+,-, *, /)$ on two real floating point numbers.\n\n# forward phase (of row reduction)\nThe first part of the algorithm that reduces a matrix to echelon form.\n\n# Fourier approximation (of order $n$)\nThe closest point in the subspace of $n$ th-order trigonometric polynomials to a given function in $C[0,2 \\pi]$.\n\n# Fourier coefficients\nThe weights used to make a trigonometric polynomial as a Fourier approximation to a function.\n \n# Fourier series\nAn infinite series that converges to a function in the inner product space $C[0,2 \\pi]$, with the inner product given by a definite integral.\n\n# free variable\nAny variable in a linear system that is not a basic variable. full rank (matrix): An $m \\times n$ matrix whose rank is the smaller of $m$ and $n$.\n\n# fundamental set of solutions\nA basis for the set of all solutions of a homogeneous linear difference or differential equation.\n\n# fundamental subspaces (determined by $A$)\nThe null space and column space of $A$, and the null space and column space of $A^{T}$, with $\\operatorname{Col} A^{T}$ commonly called the row space of $A$.\n\n# Gaussian elimination\nSee row reduction algorithm.\n\n# general least-squares problem\nGiven an $m \\times n$ matrix $A$ and a vector $\\mathbf{b}$ in $\\mathbb{R}^{m}$, find $\\hat{\\mathbf{x}}$ in $\\mathbb{R}^{n}$ such that $\\|\\mathbf{b}-A \\hat{\\mathbf{x}}\\| \\leq\\|\\mathbf{b}-A \\mathbf{x}\\|$ for all $\\mathbf{x}$ in $\\mathbb{R}^{n}$.\n\n# general solution (of a linear system)\nA parametric description of a solution set that expresses the basic variables in terms of the free variables (the parameters), if any. After Section 1.5, the parametric description is written in vector form.\n\n# Givens rotation\nA linear transformation from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{n}$ used in computer programs to create zero entries in a vector (usually a column of a matrix).\n\n# Gram matrix (of $A$)\nThe matrix $A^{T} A$.\n\n# Gram-Schmidt process\nAn algorithm for producing an orthogonal or orthonormal basis for a subspace that is spanned by a given set of vectors.\n\n# homogeneous coordinates\nIn $\\mathbb{R}^{3}$, the representation of $(x, y, z)$ as $(X, Y, Z, H)$ for any $H \\neq 0$, where $x=X / H$, $y=Y / H$, and $z=Z / H$. In $\\mathbb{R}^{2}, H$ is usually taken as 1 , and the homogeneous coordinates of $(x, y)$ are written as $(x, y, 1)$.\n\n# homogeneous equation\nAn equation of the form $A \\mathbf{x}=\\mathbf{0}$, possibly written as a vector equation or as a system of linear equations.\n\n# homogeneous form of (a vector) $\\mathbf{v}$ in $\\mathbb{R}^{n}\n$ The point $\\tilde{\\mathbf{v}}=\\left[\\begin{array}{l}\\mathbf{v} \\\\ 1\\end{array}\\right]$ in $\\mathbb{R}^{n+1}$.\n\n# Householder reflection\nA transformation $\\mathbf{x} \\mapsto Q \\mathbf{x}$, where $Q=I-2 \\mathbf{u u}^{T}$ and $\\mathbf{u}$ is a unit vector $\\left(\\mathbf{u}^{T} \\mathbf{u}=1\\right)$.\n\n# hyperplane (in $\\mathbb{R}^{n}$)\nA flat in $\\mathbb{R}^{n}$ of dimension $n-1$. Also: a translate of a subspace of dimension $n-1$.\n\n# identity matrix (denoted by $I$ or $I_{n}$)\nA square matrix with ones on the diagonal and zeros elsewhere.\n\n# ill-conditioned matrix\nA square matrix with a large (or possibly infinite) condition number; a matrix that is singular or can become singular if some of its entries are changed ever so slightly.\n\n# image (of a vector $\\mathbf{x}$ under a transformation $T$)\nThe vector $T(\\mathbf{x})$ assigned to $\\mathbf{x}$ by $T$. implicit description (of a subspace $W$ of $\\mathbb{R}^{n}$): A set of one or more homogeneous equations that characterize the points of $W$.\n\n# Im $\\mathbf{x}$ \nThe vector in $\\mathbb{R}^{n}$ formed from the imaginary parts of the entries of a vector $\\mathbf{x}$ in $\\mathbb{C}^{n}$.\n\n# inconsistent linear system\nA linear system with no solution.\n\n# indefinite matrix\nA symmetric matrix $A$ such that $\\mathbf{x}^{T} A \\mathbf{x}$ assumes both positive and negative values.\n\n# indefinite quadratic form\nA quadratic form $Q$ such that $Q(\\mathbf{x})$ assumes both positive and negative values.\n\n# infinite-dimensional (vector space)\nA nonzero vector space $V$ that has no finite basis.\n\n# inner product\nThe scalar $\\mathbf{u}^{T} \\mathbf{v}$, usually written as $\\mathbf{u} \\cdot \\mathbf{v}$, where $\\mathbf{u}$ and $\\mathbf{v}$ are vectors in $\\mathbb{R}^{n}$ viewed as $n \\times 1$ matrices. Also called the dot product of $\\mathbf{u}$ and $\\mathbf{v}$. In general, a function on a vector space that assigns to each pair of vectors $\\mathbf{u}$ and $\\mathbf{v}$ a number $\\langle\\mathbf{u}, \\mathbf{v}\\rangle$, subject to certain axioms. See Section 6.7.\n\n# inner product space\nA vector space on which is defined an inner product.\n\n# input-output matrix\nSee consumption matrix.\n\n# input-output model\nSee Leontief input-output model.\n\n# interior point (of a set $S$ in $\\mathbb{R}^{n}$)\nA point $\\mathbf{p}$ in $S$ such that for some $\\delta\u003e0$, the open ball $\\mathbf{B}(\\mathbf{p}, \\delta)$ centered at $\\mathbf{p}$ is contained in $S$.\n\n# intermediate demands\nDemands for goods or services that will be consumed in the process of producing other goods and services for consumers. If $\\mathbf{x}$ is the production level and $C$ is the consumption matrix, then $C \\mathbf{x}$ lists the intermediate demands.\n\n# interpolating polynomial\nA polynomial whose graph passes through every point in a set of data points in $\\mathbb{R}^{2}$.\n\n# invariant subspace (for $A$)\nA subspace $H$ such that $A \\mathbf{x}$ is in $H$ whenever $\\mathbf{x}$ is in $H$.\n\n# inverse (of an $n \\times n$ matrix $A$)\nAn $n \\times n$ matrix $A^{-1}$ such that $A A^{-1}=A^{-1} A=I_{n}$.\n\n# inverse power method\nAn algorithm for estimating an eigenvalue $\\lambda$ of a square matrix, when a good initial estimate of $\\lambda$ is available.\n\n# invertible linear transformation\nA linear transformation $T: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ such that there exists a function $S: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ satisfying both $T(S(\\mathbf{x}))=\\mathbf{x}$ and $S(T(\\mathbf{x}))=\\mathbf{x}$ for all $\\mathbf{x}$ in $\\mathbb{R}^{n}$.\n\n# invertible matrix\nA square matrix that possesses an inverse.\n\n# isomorphic vector spaces\nTwo vector spaces $V$ and $W$ for which there is a one-to-one linear transformation $T$ that maps $V$ onto $W$\n\n# isomorphism\nA one-to-one linear mapping from one vector space onto another.\n\n# kernel (of a linear transformation $T\nV \\rightarrow W$): The set of $\\mathbf{x}$ in $V$ such that $T(\\mathbf{x})=\\mathbf{0}$. \n\n# Kirchhoff's laws\n(1) (voltage law) The algebraic sum of the $R I$ voltage drops in one direction around a loop equals the algebraic sum of the voltage sources in the same direction around the loop. (2) (current law) The current in a branch is the algebraic sum of the loop currents flowing through that branch.\n\n# ladder network\nAn electrical network assembled by connecting in series two or more electrical circuits.\n\n# leading entry\nThe leftmost nonzero entry in a row of a matrix. least-squares error: The distance $\\|\\mathbf{b}-A \\hat{\\mathbf{x}}\\|$ from $\\mathbf{b}$ to $A \\hat{\\mathbf{x}}$, when $\\hat{\\mathbf{x}}$ is a least-squares solution of $A \\mathbf{x}=\\mathbf{b}$.\n\n# least-squares line\nThe line $y=\\hat{\\beta}\\_{0}+\\hat{\\beta}\\_{1} x$ that minimizes the least-squares error in the equation $\\mathbf{y}=X \\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}$.\n\n# least-squares solution (of $A \\mathbf{x}=\\mathbf{b}$)\nA vector $\\hat{\\mathbf{x}}$ such that $\\|\\mathbf{b}-A \\hat{\\mathbf{x}}\\| \\leq\\|\\mathbf{b}-A \\mathbf{x}\\|$ for all $\\mathbf{x}$ in $\\mathbb{R}^{n}$\n\n# left inverse (of $A$)\nAny rectangular matrix $C$ such that $C A=I$.\n\n# left-multiplication (by $A$)\nMultiplication of a vector or matrix on the left by $A$.\n\n# left singular vectors (of $A$)\nThe columns of $U$ in the singular value decomposition $A=U \\Sigma V^{T}$.\n\n# length (or norm, of $\\mathbf{v}$)\nThe scalar $\\|\\mathbf{v}\\|=\\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}=\\sqrt{\\langle\\mathbf{v}, \\mathbf{v}\\rangle}$.\n\n# Leontief exchange (or closed) model\nA model of an economy where inputs and outputs are fixed, and where a set of prices for the outputs of the sectors is sought such that the income of each sector equals its expenditures. This \"equilibrium\" condition is expressed as a system of linear equations, with the prices as the unknowns.\n\n# Leontief input-output model (or Leontief production equation)\nThe equation $\\mathbf{x}=C \\mathbf{x}+\\mathbf{d}$, where $\\mathbf{x}$ is production, $\\mathbf{d}$ is final demand, and $C$ is the consumption (or input-output) matrix. The $j$ th column of $C$ lists the inputs that sector $j$ consumes per unit of output.\n\n# level set (or gradient) of a linear functional $f$ on $\\mathbb{R}^{n}$ \nA set $[f: d]=\\left\\{\\mathbf{x} \\in \\mathbb{R}^{n}: f(\\mathbf{x})=d\\right\\}$\n\n# linear combination\nA sum of scalar multiples of vectors. The scalars are called the weights.\n\n# linear dependence relation\nA homogeneous vector equation where the weights are all specified and at least one weight is nonzero.\n\n# linear equation (in the variables $x_{1}, \\ldots, x_{n}$)\nAn equation that can be written in the form $a_{1} x_{1}+a_{2} x_{2}+\\cdots+a_{n} x_{n}=b$, where $b$ and the coefficients $a_{1}, \\ldots, a_{n}$ are real or complex numbers.\n\n# linear filter\nA linear difference equation used to transform discrete-time signals.\n\n# linear functional (on $\\mathbb{R}^{n}$)\nA linear transformation $f$ from $\\mathbb{R}^{n}$ into $\\mathbb{R}$.\n\n# linearly dependent (vectors)\nAn indexed set $\\left\\{\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{p}\\right\\}$ with the property that there exist weights $c_{1}, \\ldots, c_{p}$, not all zero, such that $c_{1} \\mathbf{v}\\_{1}+\\cdots+c_{p} \\mathbf{v}\\_{p}=\\mathbf{0}$. That is, the vector equation $c_{1} \\mathbf{v}\\_{1}+c_{2} \\mathbf{v}\\_{2}+\\cdots+c_{p} \\mathbf{v}\\_{p}=\\mathbf{0}$ has a nontrivial solution.\n\n# linearly independent (vectors)\nAn indexed set $\\left\\{\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{p}\\right\\}$ with the property that the vector equation $c_{1} \\mathbf{v}\\_{1}+$ $c_{2} \\mathbf{v}\\_{2}+\\cdots+c_{p} \\mathbf{v}\\_{p}=\\mathbf{0}$ has only the trivial solution, $c_{1}=\\cdots=c_{p}=0$.\n\n# linear model (in statistics)\nAny equation of the form $\\mathbf{y}=X \\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}$, where $X$ and $\\mathbf{y}$ are known and $\\boldsymbol{\\beta}$ is to be chosen to minimize the length of the residual vector, $\\epsilon$.\n\n# linear system\nA collection of one or more linear equations involving the same variables, say, $x_{1}, \\ldots, x_{n}$.\n\n# linear transformation $\\boldsymbol{T}$ (from a vector space $V$ into a vector space $W$)\nA rule $T$ that assigns to each vector $\\mathbf{x}$ in $V$ a unique vector $T(\\mathbf{x})$ in $W$, such that (i) $T(\\mathbf{u}+\\mathbf{v})=T(\\mathbf{u})+T(\\mathbf{v})$ for all $\\mathbf{u}, \\mathbf{v}$ in $V$, and (ii) $T(c \\mathbf{u})=c T(\\mathbf{u})$ for all $\\mathbf{u}$ in $V$ and all scalars $c$. Notation: $T: V \\rightarrow W ;$ also, $\\mathbf{x} \\mapsto A \\mathbf{x}$ when $T: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ and $A$ is the standard matrix for $T$.\n\n# line through p parallel to $\\mathbf{v}$ \nThe set $\\{\\mathbf{p}+t \\mathbf{v}: t$ in $\\mathbb{R}\\}$.\n\n# loop current\nThe amount of electric current flowing through a loop that makes the algebraic sum of the $R I$ voltage drops around the loop equal to the algebraic sum of the voltage sources in the loop.\n\n# lower triangular matrix\nA matrix with zeros above the main diagonal.\n\n# lower triangular part (of $A$)\nA lower triangular matrix whose entries on the main diagonal and below agree with those in $A$.\n\n# LU factorization\nThe representation of a matrix $A$ in the form $A=L U$ where $L$ is a square lower triangular matrix with ones on the diagonal (a unit lower triangular matrix) and $U$ is an echelon form of $A$.\n\n# magnitude (of a vector)\nSee norm.\n\n# main diagonal (of a matrix)\nThe entries with equal row and column indices.\n\n# mapping\nSee transformation.\n\n# Markov chain\nA sequence of probability vectors $\\mathbf{x}\\_{0}, \\mathbf{x}\\_{1}$, $\\mathbf{x}\\_{2}, \\ldots$, together with a stochastic matrix $P$ such that $\\mathbf{x}\\_{k+1}=P \\mathbf{x}\\_{k}$ for $k=0,1,2, \\ldots$\n\n# matrix\nA rectangular array of numbers.\n\n# matrix equation\nAn equation that involves at least one matrix; for instance, $A \\mathbf{x}=\\mathbf{b}$.\n\n# matrix for $T$ relative to bases $\\mathcal{B}$ and $\\mathcal{C}$ \nA matrix $M$ for a linear transformation $T: V \\rightarrow W$ with the property that $[T(\\mathbf{x})]\\_{\\mathcal{C}}=M[\\mathbf{x}]\\_{\\mathcal{B}}$ for all $\\mathbf{x}$ in $V$, where $\\mathcal{B}$ is a basis for $V$ and $\\mathcal{C}$ is a basis for $W$. When $W=V$ and $\\mathcal{C}=\\mathcal{B}$, the matrix $M$ is called the $\\mathcal{B}$-matrix for $T$ and is denoted by $[T]\\_{\\mathcal{B}}$.\n\n# matrix of observations\nA $p \\times N$ matrix whose columns are observation vectors, each column listing $p$ measurements made on an individual or object in a specified population or set. matrix transformation: A mapping $\\mathbf{x} \\mapsto A \\mathbf{x}$, where $A$ is an $m \\times n$ matrix and $\\mathbf{x}$ represents any vector in $\\mathbb{R}^{n}$.\n\n# maximal linearly independent set (in $V$)\nA linearly independent set $\\mathcal{B}$ in $V$ such that if a vector $\\mathbf{v}$ in $V$ but not in $\\mathcal{B}$ is added to $\\mathcal{B}$, then the new set is linearly dependent.\n\n# mean-deviation form (of a matrix of observations)\nA matrix whose row vectors are in mean-deviation form. For each row, the entries sum to zero.\n\n# mean-deviation form (of a vector)\nA vector whose entries sum to zero.\n\n# mean square error\nThe error of an approximation in an inner product space, where the inner product is defined by a definite integral.\n\n# migration matrix\nA matrix that gives the percentage movement between different locations, from one period to the next.\n\n# minimal spanning set (for a subspace $H$)\nA set $\\mathcal{B}$ that spans $H$ and has the property that if one of the elements of $\\mathcal{B}$ is removed from $\\mathcal{B}$, then the new set does not span $H$.\n\n# $m \\times n$ matrix\nA matrix with $m$ rows and $n$ columns.\n\n# Moore-Penrose inverse\nSee pseudoinverse.\n\n# multiple regression\nA linear model involving several independent variables and one dependent variable.\n\n# nearly singular matrix\nAn ill-conditioned matrix.\n\n# negative definite matrix\nA symmetric matrix $A$ such that $\\mathbf{x}^{T} A \\mathbf{x}\u003c0$ for all $\\mathbf{x} \\neq \\mathbf{0}$.\n\n# negative definite quadratic form\nA quadratic form $Q$ such that $Q(\\mathbf{x})\u003c0$ for all $\\mathbf{x} \\neq \\mathbf{0}$.\n\n# negative semidefinite matrix\nA symmetric matrix $A$ such that $\\mathbf{x}^{T} A \\mathbf{x} \\leq 0$ for all $\\mathbf{x}$.\n\n# negative semidefinite quadratic form\nA quadratic form $Q$ such that $Q(\\mathbf{x}) \\leq 0$ for all $\\mathbf{x}$.\n\n# nonhomogeneous equation\nAn equation of the form $A \\mathbf{x}=\\mathbf{b}$ with $\\mathbf{b} \\neq \\mathbf{0}$, possibly written as a vector equation or as a system of linear equations.\n\n# nonsingular (matrix)\nAn invertible matrix.\n\n# nontrivial solution\nA nonzero solution of a homogeneous equation or system of homogeneous equations.\n\n# nonzero (matrix or vector)\nA matrix (with possibly only one row or column) that contains at least one nonzero entry.\n\n# norm (or length, of $\\mathbf{v}$)\nThe scalar $\\|\\mathbf{v}\\|=\\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}=\\sqrt{\\langle\\mathbf{v}, \\mathbf{v}\\rangle}$.\n\n# normal equations\nThe system of equations represented by $A^{T} A \\mathbf{x}=A^{T} \\mathbf{b}$, whose solution yields all least-squares solutions of $A \\mathbf{x}=\\mathbf{b}$. In statistics, a common notation is $X^{T} X \\boldsymbol{\\beta}=X^{T} \\mathbf{y}$\n\n# normalizing (a nonzero vector $\\mathbf{v}$)\nThe process of creating a unit vector $\\mathbf{u}$ that is a positive multiple of $\\mathbf{v}$.\n\n# normal vector (to a subspace $V$ of $\\mathbb{R}^{n}$)\nA vector $\\mathbf{n}$ in $\\mathbb{R}^{n}$ such that $\\mathbf{n} \\cdot \\mathbf{x}=0$ for all $\\mathbf{x}$ in $V$. \n\n# null space ( of an $m \\times n$ matrix $A$)\nThe set $\\operatorname{Nul} A$ of all solutions to the homogeneous equation $A \\mathbf{x}=\\mathbf{0}$. Nul $A=\\{\\mathbf{x}: \\mathbf{x}$ is in $\\mathbb{R}^{n}$ and $\\left.A \\mathbf{x}=\\mathbf{0}\\right\\}$\n\n# observation vector\nThe vector $\\mathbf{y}$ in the linear model $\\mathbf{y}=X \\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}$, where the entries in $\\mathbf{y}$ are the observed values of a dependent variable.\n\n# one-to-one (mapping)\nA mapping $T: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ such that each $\\mathbf{b}$ in $R^{m}$ is the image of at most one $\\mathbf{x}$ in $\\mathbb{R}^{n}$.\n\n# onto (mapping)\nA mapping $T: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ such that each $\\mathbf{b}$ in $R^{m}$ is the image of at least one $\\mathbf{x}$ in $\\mathbb{R}^{n}$.\n\n# open ball $\\mathbf{B}(\\mathbf{p}, \\delta)$ in $\\mathbb{R}^{n}$ \nThe set $\\{\\mathbf{x}:\\|\\mathbf{x}-\\mathbf{p}\\|\u003c\\delta\\}$ in $\\mathbb{R}^{n}$, where $\\delta\u003e0$\n\n# open set $S$ in $\\mathbb{R}^{n}$ \nA set that contains none of its boundary points. (Equivalently, $S$ is open if every point of $S$ is an interior point.)\n\n# origin\nThe zero vector.\n\n# orthogonal basis\nA basis that is also an orthogonal set.\n\n# orthogonal complement ( of $W$)\nThe set $W^{\\perp}$ of all vectors orthogonal to $W$.\n\n# orthogonal decomposition\nThe representation of a vector $\\mathbf{y}$ as the sum of two vectors, one in a specified subspace $W$ and the other in $W^{\\perp}$. In general, a decomposition $\\mathbf{y}=c_{1} \\mathbf{u}\\_{1}+\\cdots+c_{p} \\mathbf{u}\\_{p}$, where $\\left\\{\\mathbf{u}\\_{1}, \\ldots, \\mathbf{u}\\_{p}\\right\\}$ is an orthogonal basis for a subspace that contains $\\mathbf{y}$.\n\n# orthogonally diagonalizable (matrix)\nA matrix $A$ that admits a factorization, $A=P D P^{-1}$, with $P$ an orthogonal matrix $\\left(P^{-1}=P^{T}\\right)$ and $D$ diagonal.\n\n# orthogonal matrix\nA square invertible matrix $U$ such that $U^{-1}=U^{T}$\n\n# orthogonal projection of $\\mathbf{y}$ onto $\\mathbf{u}$ (or onto the line through $\\mathbf{u}$ and the origin, for $\\mathbf{u} \\neq \\mathbf{0}$)\nThe vector $\\hat{\\mathbf{y}}$ defined by $\\hat{\\mathbf{y}}=\\frac{\\mathbf{y} \\cdot \\mathbf{u}}{\\mathbf{u} \\cdot \\mathbf{u}} \\mathbf{u}$. orthogonal projection of y onto $W$ : The unique vector $\\hat{\\mathbf{y}}$ in $W$ such that $\\mathbf{y}-\\hat{\\mathbf{y}}$ is orthogonal to $W$. Notation: $\\hat{\\mathbf{y}}=\\operatorname{proj}\\_{W} \\mathbf{y}$.\n\n# orthogonal set\nA set $S$ of vectors such that $\\mathbf{u} \\cdot \\mathbf{v}=0$ for each distinct pair $\\mathbf{u}, \\mathbf{v}$ in $S$.\n\n# orthogonal to $\\boldsymbol{W}$ \nOrthogonal to every vector in $W$.\n\n# orthonormal basis\nA basis that is an orthogonal set of unit vectors.\n\n# orthonormal set\nAn orthogonal set of unit vectors.\n\n# outer product\nA matrix product $\\mathbf{u v}^{T}$ where $\\mathbf{u}$ and $\\mathbf{v}$ are vectors in $\\mathbb{R}^{n}$ viewed as $n \\times 1$ matrices. (The transpose symbol is on the \"outside\" of the symbols $\\mathbf{u}$ and $\\mathbf{v}$.)\n\n# overdetermined system\nA system of equations with more equations than unknowns.\n\n# parallel flats\nTwo or more flats such that each flat is a translate of the other flats. parallelogram rule for addition: A geometric interpretation of the sum of two vectors $\\mathbf{u}, \\mathbf{v}$ as the diagonal of the parallelogram determined by $\\mathbf{u}, \\mathbf{v}$, and $\\mathbf{0}$.\n\n# parameter vector\nThe unknown vector $\\boldsymbol{\\beta}$ in the linear model $\\mathbf{y}=X \\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}$\n\n# parametric equation of a line\nAn equation of the form $\\mathbf{x}=\\mathbf{p}+t \\mathbf{v}(t$ in $\\mathbb{R})$.\n\n# parametric equation of a plane\nAn equation of the form $\\mathbf{x}=\\mathbf{p}+s \\mathbf{u}+t \\mathbf{v} \\quad(s, t$ in $\\mathbb{R})$, with $\\mathbf{u}$ and $\\mathbf{v}$ linearly independent.\n\n# partitioned matrix (or block matrix)\nA matrix whose entries are themselves matrices of appropriate sizes.\n\n# permuted lower triangular matrix\nA matrix such that a permutation of its rows will form a lower triangular matrix.\n\n# permuted LU factorization\nThe representation of a matrix $A$ in the form $A=L U$ where $L$ is a square matrix such that a permutation of its rows will form a unit lower triangular matrix, and $U$ is an echelon form of $A$.\n\n# pivot\nA nonzero number that either is used in a pivot position to create zeros through row operations or is changed into a leading 1 , which in turn is used to create zeros.\n\n# pivot column\nA column that contains a pivot position.\n\n# pivot position\nA position in a matrix $A$ that corresponds to a leading entry in an echelon form of $A$.\n\n# plane through $\\mathbf{u}, \\mathbf{v}$, and the origin\nA set whose parametric equation is $\\mathbf{x}=s \\mathbf{u}+t \\mathbf{v}(s, t$ in $\\mathbb{R})$, with $\\mathbf{u}$ and $\\mathbf{v}$ linearly independent.\n\n# polar decomposition (of $A$)\nA factorization $A=P Q$, where $P$ is an $n \\times n$ positive semidefinite matrix with the same rank as $A$, and $Q$ is an $n \\times n$ orthogonal matrix.\n\n# polygon\nA polytope in $\\mathbb{R}^{2}$.\n\n# polyhedron\nA polytope in $\\mathbb{R}^{3}$.\n\n# polytope\nThe convex hull of a finite set of points in $\\mathbb{R}^{n}$ (a special type of compact convex set).\n\n# positive combination (of points $\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{m}$ in $\\mathbb{R}^{n}$)\nA linear combination $c_{1} \\mathbf{v}\\_{1}+\\cdots+c_{m} \\mathbf{v}\\_{m}$, where all $c_{i} \\geq 0$.\n\n# positive definite matrix\nA symmetric matrix $A$ such that $\\mathbf{x}^{T} A \\mathbf{x}\u003e0$ for all $\\mathbf{x} \\neq \\mathbf{0}$.\n\n# positive definite quadratic form\nA quadratic form $Q$ such that $Q(\\mathbf{x})\u003e0$ for all $\\mathbf{x} \\neq \\mathbf{0}$.\n\n# positive hull (of a set $S$)\nThe set of all positive combinations of points in $S$, denoted by pos $S$.\n\n# positive semidefinite matrix\nA symmetric matrix $A$ such that $\\mathbf{x}^{T} A \\mathbf{x} \\geq 0$ for all $\\mathbf{x}$.\n\n# positive semidefinite quadratic form\nA quadratic form $Q$ such that $Q(\\mathbf{x}) \\geq 0$ for all $\\mathbf{x}$.\n\n# power method\nAn algorithm for estimating a strictly dominant eigenvalue of a square matrix.\n\n# principal axes (of a quadratic form $\\mathbf{x}^{T} A \\mathbf{x}$)\nThe orthonormal columns of an orthogonal matrix $P$ such that $P^{-1} A P$ is diagonal. (These columns are unit eigenvectors of $A$.) Usually the columns of $P$ are ordered in such a way that the corresponding eigenvalues of $A$ are arranged in decreasing order of magnitude.\n\n# principal components (of the data in a matrix $B$ of observations)\nThe unit eigenvectors of a sample covariance matrix $S$ for $B$, with the eigenvectors arranged so that the corresponding eigenvalues of $S$ decrease in magnitude. If $B$ is in mean-deviation form, then the principal components are the right singular vectors in a singular value decomposition of $B^{T}$.\n\n# probability vector\nA vector in $\\mathbb{R}^{n}$ whose entries are nonnegative and sum to one.\n\n# product $A \\mathbf{x}$ \nThe linear combination of the columns of $A$ using the corresponding entries in $\\mathbf{x}$ as weights.\n\n# production vector\nThe vector in the Leontief input-output model that lists the amounts that are to be produced by the various sectors of an economy.\n\n# profile (of a set $S$ in $\\mathbb{R}^{n}$)\nThe set of extreme points of $S$.\n\n# projection matrix (or orthogonal projection matrix)\nA symmetric matrix $B$ such that $B^{2}=B$. A simple example is $B=\\mathbf{v}^{T}$, where $\\mathbf{v}$ is a unit vector.\n\n# proper subset of a set $S$ \nA subset of $S$ that does not equal $S$ itself.\n\n# proper subspace\nAny subspace of a vector space $V$ other than $V$ itself.\n\n# pseudoinverse (of $A$)\nThe matrix $V D^{-1} U^{T}$, when $U D V^{T}$ is a reduced singular value decomposition of $A$.\n\n# QR factorization\nA factorization of an $m \\times n$ matrix $A$ with linearly independent columns, $A=Q R$, where $Q$ is an $m \\times n$ matrix whose columns form an orthonormal basis for $\\operatorname{Col} A$, and $R$ is an $n \\times n$ upper triangular invertible matrix with positive entries on its diagonal.\n\n# quadratic BÃ©zier curve\nA curve whose description may be written in the form $\\mathbf{g}(t)=(1-t) \\mathbf{f}\\_{0}(t)+t \\mathbf{f}\\_{1}(t)$ for $0 \\leq t \\leq$ 1 , where $\\mathbf{f}\\_{0}(t)=(1-t) \\mathbf{p}\\_{0}+t \\mathbf{p}\\_{1}$ and $\\mathbf{f}\\_{1}(t)=(1-t) \\mathbf{p}\\_{1}+$ $t \\mathbf{p}\\_{2}$. The points $\\mathbf{p}\\_{0}, \\mathbf{p}\\_{1}, \\mathbf{p}\\_{2}$ are called the control points for the curve.\n\n# quadratic form\nA function $Q$ defined for $\\mathbf{x}$ in $\\mathbb{R}^{n}$ by $Q(\\mathbf{x})=$ $\\mathbf{x}^{T} A \\mathbf{x}$, where $A$ is an $n \\times n$ symmetric matrix (called the matrix of the quadratic form).\n\n# range (of a linear transformation $T$)\nThe set of all vectors of the form $T(\\mathbf{x})$ for some $\\mathbf{x}$ in the domain of $T$.\n\n# rank (of a matrix $A$)\nThe dimension of the column space of $A$, denoted by $\\operatorname{rank} A$.\n\n# Rayleigh quotient\n$R(\\mathbf{x})=\\left(\\mathbf{x}^{T} A \\mathbf{x}\\right) /\\left(\\mathbf{x}^{T} \\mathbf{x}\\right)$. An estimate of an eigenvalue of $A$ (usually a symmetric matrix).\n\n# recurrence relation\nSee difference equation. \n\n# reduced echelon form (or reduced row echelon form)\nA reduced echelon matrix that is row equivalent to a given matrix.\n\n# reduced echelon matrix\nA rectangular matrix in echelon form that has these additional properties: The leading entry in each nonzero row is 1 , and each leading 1 is the only nonzero entry in its column.\n\n# reduced singular value decomposition\nA factorization $A=U D V^{T}$, for an $m \\times n$ matrix $A$ of rank $r$, where $U$ is $m \\times r$ with orthonormal columns, $D$ is an $r \\times r$ diagonal matrix with the $r$ nonzero singular values of $A$ on its diagonal, and $V$ is $n \\times r$ with orthonormal columns.\n\n# regression coefficients\nThe coefficients $\\beta_{0}$ and $\\beta_{1}$ in the leastsquares line $y=\\beta_{0}+\\beta_{1} x$.\n\n# regular solid\nOne of the five possible regular polyhedrons in $\\mathbb{R}^{3}$ : the tetrahedron (4 equal triangular faces), the cube (6 square faces), the octahedron (8 equal triangular faces), the dodecahedron (12 equal pentagonal faces), and the icosahedron (20 equal triangular faces).\n\n# regular stochastic matrix\nA stochastic matrix $P$ such that some matrix power $P^{k}$ contains only strictly positive entries.\n\n# relative change or relative error (in b)\nThe quantity $\\|\\Delta \\mathbf{b}\\| /\\|\\mathbf{b}\\|$ when $\\mathbf{b}$ is changed to $\\mathbf{b}+\\Delta \\mathbf{b}$.\n\n# repellor (of a dynamical system in $\\mathbb{R}^{2}$)\nThe origin when all trajectories except the constant zero sequence or function tend away from $\\mathbf{0}$.\n\n# residual vector\nThe quantity $\\epsilon$ that appears in the general linear model: $\\mathbf{y}=X \\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}$; that is, $\\boldsymbol{\\epsilon}=\\mathbf{y}-X \\boldsymbol{\\beta}$, the difference between the observed values and the predicted values (of $y$).\n\n# $\\operatorname{Re} \\mathbf{x}$ \nThe vector in $\\mathbb{R}^{n}$ formed from the real parts of the entries of a vector $\\mathbf{x}$ in $\\mathbb{C}^{n}$.\n\n# right inverse (of $A$)\nAny rectangular matrix $C$ such that $A C=I$\n\n# right-multiplication (by $A$)\nMultiplication of a matrix on the right by $A$.\n\n# right singular vectors (of $A$)\nThe columns of $V$ in the singular value decomposition $A=U \\Sigma V^{T}$.\n\n# roundoff error\nError in floating point arithmetic caused when the result of a calculation is rounded (or truncated) to the number of floating point digits stored. Also, the error that results when the decimal representation of a number such as $1 / 3$ is approximated by a floating point number with a finite number of digits.\n\n# row-column rule\nThe rule for computing a product $A B$ in which the $(i, j)$-entry of $A B$ is the sum of the products of corresponding entries from row $i$ of $A$ and column $j$ of $B$.\n\n# row equivalent (matrices)\nTwo matrices for which there exists a (finite) sequence of row operations that transforms one matrix into the other.\n\n# row reduction algorithm\nA systematic method using elementary row operations that reduces a matrix to echelon form or reduced echelon form. row replacement: An elementary row operation that replaces one row of a matrix by the sum of the row and a multiple of another row.\n\n# row space (of a matrix $A$)\nThe set Row $A$ of all linear combinations of the vectors formed from the rows of $A$; also denoted by $\\operatorname{Col} A^{T}$.\n\n# row sum\nThe sum of the entries in a row of a matrix.\n\n# row vector\nA matrix with only one row, or a single row of a matrix that has several rows.\n\n# row-vector rule for computing $\\boldsymbol{A} \\mathbf{x}$ \nThe rule for computing a product $A \\mathbf{x}$ in which the $i$ th entry of $A \\mathbf{x}$ is the sum of the products of corresponding entries from row $i$ of $A$ and from the vector $\\mathbf{x}$.\n\n# saddle point (of a dynamical system in $\\mathbb{R}^{2}$)\nThe origin when some trajectories are attracted to $\\mathbf{0}$ and other trajectories are repelled from $\\mathbf{0}$.\n\n# same direction (as a vector $\\mathbf{v}$)\nA vector that is a positive multiple of $\\mathbf{v}$.\n\n# sample mean\nThe average $M$ of a set of vectors, $\\mathbf{X}\\_{1}, \\ldots, \\mathbf{X}\\_{N}$, given by $M=(1 / N)\\left(\\mathbf{X}\\_{1}+\\cdots+\\mathbf{X}\\_{N}\\right)$.\n\n# scalar\nA (real) number used to multiply either a vector or a matrix.\n\n# scalar multiple of $\\mathbf{u}$ by $\\boldsymbol{c}$ \nThe vector $c \\mathbf{u}$ obtained by multiplying each entry in $\\mathbf{u}$ by $c$.\n\n# scale (a vector)\nMultiply a vector (or a row or column of a matrix) by a nonzero scalar.\n\n# Schur complement\nA certain matrix formed from the blocks of a $2 \\times 2$ partitioned matrix $A=\\left[A_{i j}\\right]$. If $A_{11}$ is invertible, its Schur complement is given by $A_{22}-A_{21} A_{11}^{-1} A_{12}$. If $A_{22}$ is invertible, its Schur complement is given by $A_{11}-A_{12} A_{22}^{-1} A_{21}$.\n\n# Schur factorization (of $A$, for real scalars)\nA factorization $A=U R U^{T}$ of an $n \\times n$ matrix $A$ having $n$ real eigenvalues, where $U$ is an $n \\times n$ orthogonal matrix and $R$ is an upper triangular matrix.\n\n# set spanned by $\\left\\{\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{\\boldsymbol{p}}\\right\\}$ \nThe set $\\operatorname{Span}\\left\\{\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{p}\\right\\}$.\n\n# signal (or discrete-time signal)\nA doubly infinite sequence of numbers, $\\left\\{y_{k}\\right\\}$; a function defined on the integers; belongs to the vector space $\\mathbb{S}$.\n\n# similar (matrices)\nMatrices $A$ and $B$ such that $P^{-1} A P=B$, or equivalently, $A=P B P^{-1}$, for some invertible matrix $P$.\n\n# similarity transformation\nA transformation that changes $A$ into $P^{-1} A P$.\n\n# simplex\nThe convex hull of an affinely independent finite set of vectors in $\\mathbb{R}^{n}$.\n\n# singular (matrix)\nA square matrix that has no inverse.\n\n# singular value decomposition (of an $m \\times n$ matrix $A$)\n$A=$ $U \\Sigma V^{T}$, where $U$ is an $m \\times m$ orthogonal matrix, $V$ is an $n \\times n$ orthogonal matrix, and $\\Sigma$ is an $m \\times n$ matrix with nonnegative entries on the main diagonal (arranged in decreasing order of magnitude) and zeros elsewhere. If $\\operatorname{rank} A=r$, then $\\Sigma$ has exactly $r$ positive entries (the nonzero singular values of $A$) on the diagonal.\n\n# singular values (of $A$)\nThe (positive) square roots of the eigenvalues of $A^{T} A$, arranged in decreasing order of magnitude.\n\n# size (of a matrix)\nTwo numbers, written in the form $m \\times n$, that specify the number of rows $(m)$ and columns $(n)$ in the matrix.\n\n# solution (of a linear system involving variables $x_{1}, \\ldots, x_{n}$)\nA list $\\left(s_{1}, s_{2}, \\ldots, s_{n}\\right)$ of numbers that makes each equation in the system a true statement when the values $s_{1}, \\ldots, s_{n}$ are substituted for $x_{1}, \\ldots, x_{n}$, respectively.\n\n# solution set\nThe set of all possible solutions of a linear system. The solution set is empty when the linear system is inconsistent.\n\n# $\\operatorname{Span}\\left\\{\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{\\boldsymbol{p}}\\right\\}$ \nThe set of all linear combinations of $\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{p}$. Also, the subspace spanned (or generated) by $\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{p}$.\n\n# spanning set (for a subspace $H$)\nAny set $\\left\\{\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{p}\\right\\}$ in $H$ such that $H=\\operatorname{Span}\\left\\{\\mathbf{v}\\_{1}, \\ldots, \\mathbf{v}\\_{p}\\right\\}$.\n\n# spectral decomposition (of $A$)\nA representation\n\n$$\nA=\\lambda_{1} \\mathbf{u}\\_{1} \\mathbf{u}\\_{1}^{T}+\\cdots+\\lambda_{n} \\mathbf{u}\\_{n} \\mathbf{u}\\_{n}^{T}\n$$\n\nwhere $\\left\\{\\mathbf{u}\\_{1}, \\ldots, \\mathbf{u}\\_{n}\\right\\}$ is an orthonormal basis of eigenvectors of $A$, and $\\lambda_{1}, \\ldots, \\lambda_{n}$ are the corresponding eigenvalues of $A$.\n\n# spiral point (of a dynamical system in $\\mathbb{R}^{2}$)\nThe origin when the trajectories spiral about $\\mathbf{0}$.\n\n# stage-matrix model\nA difference equation $\\mathbf{x}\\_{k+1}=A \\mathbf{x}\\_{k}$ where $\\mathbf{x}\\_{k}$ lists the number of females in a population at time $k$, with the females classified by various stages of development (such as juvenile, subadult, and adult).\n\n# standard basis\nThe basis $\\mathcal{E}=\\left\\{\\mathbf{e}\\_{1}, \\ldots, \\mathbf{e}\\_{n}\\right\\}$ for $\\mathbb{R}^{n}$ consisting of the columns of the $n \\times n$ identity matrix, or the basis $\\left\\{1, t, \\ldots, t^{n}\\right\\}$ for $\\mathbb{P}\\_{n}$.\n\n# standard matrix (for a linear transformation $T$)\nThe matrix $A$ such that $T(\\mathbf{x})=A \\mathbf{x}$ for all $\\mathbf{x}$ in the domain of $T$.\n\n# standard position\nThe position of the graph of an equation $\\mathbf{x}^{T} A \\mathbf{x}=c$, when $A$ is a diagonal matrix.\n\n# state vector\nA probability vector. In general, a vector that describes the \"state\" of a physical system, often in connection with a difference equation $\\mathbf{x}\\_{k+1}=A \\mathbf{x}\\_{k}$.\n\n# steady-state vector (for a stochastic matrix $P$)\nA probability vector $\\mathbf{q}$ such that $P \\mathbf{q}=\\mathbf{q}$.\n\n# stiffness matrix\nThe inverse of a flexibility matrix. The $j$ th column of a stiffness matrix gives the loads that must be applied at specified points on an elastic beam in order to produce a unit deflection at the $j$ th point on the beam.\n\n# stochastic matrix\nA square matrix whose columns are probability vectors.\n\nstrictly dominant eigenvalue: An eigenvalue $\\lambda_{1}$ of a matrix $A$ with the property that $\\left|\\lambda_{1}\\right|\u003e\\left|\\lambda_{k}\\right|$ for all other eigenvalues $\\lambda_{k}$ of $A$. submatrix (of $A$): Any matrix obtained by deleting some rows and/or columns of $A$; also, $A$ itself.\n\n# subspace\nA subset $H$ of some vector space $V$ such that $H$ has these properties: (1) the zero vector of $V$ is in $H$; (2) $H$ is closed under vector addition; and (3) $H$ is closed under multiplication by scalars.\n\n# supporting hyperplane (to a compact convex set $S$ in $\\mathbb{R}^{n}$)\nA hyperplane $H=[f: d]$ such that $H \\cap S \\neq \\varnothing$ and either $f(x) \\leq d$ for all $x$ in $S$ or $f(x) \\geq d$ for all $x$ in $S$.\n\n# symmetric matrix\nA matrix $A$ such that $A^{T}=A$.\n\n# system of linear equations (or a linear system)\nA collection of one or more linear equations involving the same set of variables, say, $x_{1}, \\ldots, x_{n}$.\n\n# tetrahedron\nA three-dimensional solid object bounded by four equal triangular faces, with three faces meeting at each vertex.\n\n# total variance\nThe trace of the covariance matrix $S$ of a matrix of observations.\n\n# trace (of a square matrix $A$)\nThe sum of the diagonal entries in $A$, denoted by $\\operatorname{tr} A$.\n\n# trajectory\nThe graph of a solution $\\left\\{\\mathbf{x}\\_{0}, \\mathbf{x}\\_{1}, \\mathbf{x}\\_{2}, \\ldots\\right\\}$ of a dynamical system $\\mathbf{x}\\_{k+1}=A \\mathbf{x}\\_{k}$, often connected by a thin curve to make the trajectory easier to see. Also, the graph of $\\mathbf{x}(t)$ for $t \\geq 0$, when $\\mathbf{x}(t)$ is a solution of a differential equation $\\mathbf{x}^{\\prime}(t)=A \\mathbf{x}(t)$\n\n# transfer matrix\nA matrix $A$ associated with an electrical circuit having input and output terminals, such that the output vector is $A$ times the input vector.\n\n# transformation (or function, or mapping) $T$ from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{\\boldsymbol{m}}\n$ A rule that assigns to each vector $\\mathbf{x}$ in $\\mathbb{R}^{n}$ a unique vector $T(\\mathbf{x})$ in $\\mathbb{R}^{m}$. Notation: $T: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$. Also, $T: V \\rightarrow W$ denotes a rule that assigns to each $\\mathbf{x}$ in $V$ a unique vector $T(\\mathbf{x})$ in $W$.\n\n# translation (by a vector $\\mathbf{p}$)\nThe operation of adding $\\mathbf{p}$ to a vector or to each vector in a given set.\n\n# transpose (of $A$)\nAn $n \\times m$ matrix $A^{T}$ whose columns are the corresponding rows of the $m \\times n$ matrix $A$.\n\n# trend analysis\nThe use of orthogonal polynomials to fit data, with the inner product given by evaluation at a finite set of points.\n\n# triangle inequality\n$\\|\\mathbf{u}+\\mathbf{v}\\| \\leq\\|\\mathbf{u}\\|+\\|\\mathbf{v}\\|$ for all $\\mathbf{u}, \\mathbf{v}$.\n\n# triangular matrix\nA matrix $A$ with either zeros above or zeros below the diagonal entries.\n\n# trigonometric polynomial\nA linear combination of the constant function 1 and sine and cosine functions such as $\\cos n t$ and $\\sin n t$.\n\n# trivial solution\nThe solution $\\mathbf{x}=\\mathbf{0}$ of a homogeneous equation $A \\mathbf{x}=\\mathbf{0}$.\n\n# uncorrelated variables\nAny two variables $x_{i}$ and $x_{j}$ (with $i \\neq j$) that range over the $i$ th and $j$ th coordinates of the observation vectors in an observation matrix, such that the covariance $s_{i j}$ is zero.\n\n# underdetermined system\nA system of equations with fewer equations than unknowns.\n\n# uniqueness question\nAsks, \"If a solution of a system exists, is it unique-that is, is it the only one?\"\n\n# unit consumption vector\nA column vector in the Leontief input-output model that lists the inputs a sector needs for each unit of its output; a column of the consumption matrix.\n\n# unit lower triangular matrix\nA square lower triangular matrix with ones on the main diagonal.\n\n# unit vector\nA vector $\\mathbf{v}$ such that $\\|\\mathbf{v}\\|=1$.\n\n# upper triangular matrix\nA matrix $U$ (not necessarily square) with zeros below the diagonal entries $u_{11}, u_{22}, \\ldots$\n\n# Vandermonde matrix\nAn $n \\times n$ matrix $V$ or its transpose, when $V$ has the form\n\n$$\nV=\\left[\\begin{array}{ccccc}\n1 \u0026 x_{1} \u0026 x_{1}^{2} \u0026 \\cdots \u0026 x_{1}^{n-1} \\\\\n1 \u0026 x_{2} \u0026 x_{2}^{2} \u0026 \\cdots \u0026 x_{2}^{n-1} \\\\\n\\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\\\\n1 \u0026 x_{n} \u0026 x_{n}^{2} \u0026 \\cdots \u0026 x_{n}^{n-1}\n\\end{array}\\right]\n$$\n\n# variance (of a variable $x_{j}$)\nThe diagonal entry $s_{j j}$ in the covariance matrix $S$ for a matrix of observations, where $x_{j}$ varies over the $j$ th coordinates of the observation vectors.\n\n# vector\nA list of numbers; a matrix with only one column. In general, any element of a vector space.\n\n# vector addition\nAdding vectors by adding corresponding entries.\n\n# vector equation\nAn equation involving a linear combination of vectors with undetermined weights.\n\n# vector space\nA set of objects, called vectors, on which two operations are defined, called addition and multiplication by scalars. Ten axioms must be satisfied. See the first definition in Section 4.1.\n\n# vector subtraction\nComputing $\\mathbf{u}+(-1) \\mathbf{v}$ and writing the result as $\\mathbf{u}-\\mathbf{v}$.\n\n# weighted least squares\nLeast-squares problems with a weighted inner product such as\n\n$$\n\\langle\\mathbf{x}, \\mathbf{y}\\rangle=w_{1}^{2} x_{1} y_{1}+\\cdots+w_{n}^{2} x_{n} y_{n} .\n$$\n\n# weights\nThe scalars used in a linear combination.\n\n# zero subspace\nThe subspace $\\{\\boldsymbol{0}\\}$ consisting of only the zero vector.\n\n# zero vector\nThe unique vector, denoted by $\\mathbf{0}$, such that $\\mathbf{u}+\\mathbf{0}=\\mathbf{u}$ for all $\\mathbf{u}$. In $\\mathbb{R}^{n}, \\mathbf{0}$ is the vector whose entries are all zeros.\n\n","lastmodified":"2022-12-25T00:51:11.577820107Z","tags":null},"/linear-algebra/span":{"title":"Span","content":"\n# Description\n\nThe span of a set of vectors is a [vector\nspace](linear-algebra/vector-space.md).\n\n\n\nlorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim\nlabore culpa sint ad nisi lorem pariatur mollit ex esse exercitation amet. nisi\nanim cupidatat excepteur officia. reprehenderit nostrud nostrud ipsum lorem est\naliquip amet voluptate voluptate dolor minim nulla est proident. nostrud officia\npariatur ut officia. sit irure elit esse ea nulla sunt ex occaecat reprehenderit\ncommodo officia dolor lorem duis laboris cupidatat officia voluptate. culpa\nproident adipisicing id nulla nisi laboris ex in lorem sunt duis officia\neiusmod. aliqua reprehenderit commodo ex non excepteur duis sunt velit enim.\nvoluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa\nduis.\n\n\n\nlorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim\nlabore culpa sint ad nisi lorem pariatur mollit ex esse exercitation amet. nisi\nanim cupidatat excepteur officia. reprehenderit nostrud nostrud ipsum lorem est\naliquip amet voluptate voluptate dolor minim nulla est proident. nostrud officia\npariatur ut officia. sit irure elit esse ea nulla sunt ex occaecat reprehenderit\ncommodo officia dolor lorem duis laboris cupidatat officia voluptate. culpa\nproident adipisicing id nulla nisi laboris ex in lorem sunt duis officia\neiusmod. aliqua reprehenderit commodo ex non excepteur duis sunt velit enim.\nvoluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa\nduis.\n\n\n\n\nlorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim\nlabore culpa sint ad nisi lorem pariatur mollit ex esse exercitation amet. nisi\nanim cupidatat excepteur officia. reprehenderit nostrud nostrud ipsum lorem est\naliquip amet voluptate voluptate dolor minim nulla est proident. nostrud officia\npariatur ut officia. sit irure elit esse ea nulla sunt ex occaecat reprehenderit\ncommodo officia dolor lorem duis laboris cupidatat officia voluptate. culpa\nproident adipisicing id nulla nisi laboris ex in lorem sunt duis officia\neiusmod. aliqua reprehenderit commodo ex non excepteur duis sunt velit enim.\nvoluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa\nduis.\n\n\n\n# AFD\n\nasdflksdfkl\n\n\nlorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim\nlabore culpa sint ad nisi lorem pariatur mollit ex esse exercitation amet. nisi\nanim cupidatat excepteur officia. reprehenderit nostrud nostrud ipsum lorem est\naliquip amet voluptate voluptate dolor minim nulla est proident. nostrud officia\npariatur ut officia. sit irure elit esse ea nulla sunt ex occaecat reprehenderit\ncommodo officia dolor lorem duis laboris cupidatat officia voluptate. culpa\nproident adipisicing id nulla nisi laboris ex in lorem sunt duis officia\neiusmod. aliqua reprehenderit commodo ex non excepteur duis sunt velit enim.\nvoluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa\nduis.\n\n\nlorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim\nlabore culpa sint ad nisi lorem pariatur mollit ex esse exercitation amet. nisi\nanim cupidatat excepteur officia. reprehenderit nostrud nostrud ipsum lorem est\naliquip amet voluptate voluptate dolor minim nulla est proident. nostrud officia\npariatur ut officia. sit irure elit esse ea nulla sunt ex occaecat reprehenderit\ncommodo officia dolor lorem duis laboris cupidatat officia voluptate. culpa\nproident adipisicing id nulla nisi laboris ex in lorem sunt duis officia\neiusmod. aliqua reprehenderit commodo ex non excepteur duis sunt velit enim.\nvoluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa\nduis.\n\n\nlorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim\nlabore culpa sint ad nisi lorem pariatur mollit ex esse exercitation amet. nisi\nanim cupidatat excepteur officia. reprehenderit nostrud nostrud ipsum lorem est\naliquip amet voluptate voluptate dolor minim nulla est proident. nostrud officia\npariatur ut officia. sit irure elit esse ea nulla sunt ex occaecat reprehenderit\ncommodo officia dolor lorem duis laboris cupidatat officia voluptate. culpa\nproident adipisicing id nulla nisi laboris ex in lorem sunt duis officia\neiusmod. aliqua reprehenderit commodo ex non excepteur duis sunt velit enim.\nvoluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa\nduis.\n\n","lastmodified":"2022-12-25T00:51:11.577820107Z","tags":null},"/linear-algebra/vector-space":{"title":"Vector Space","content":"\nHello World!\n\n\n[link here](span.md#AFD)\n\n\nIdeas:\n\nlink to / embed external resources\n- SageMath Cells / Octave Online / MATLAB\n- Relevant 3blue1brown video\n- Other relevant YouTube videos\n- Relevant sections in umd textbook \n- Relevant sections in https://davidaustinm.github.io/ula/frontmatter.html\n","lastmodified":"2022-12-25T00:51:11.577820107Z","tags":null}}